[
  {
    "id": "001",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n    ports:\n    - containerPort: 80\n",
    "errors": []
  },
  {
    "id": "002",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "003",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "004",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "005",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "006",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n      readOnlyRootFilesystem: true\n    command:\n    - sleep\n    - '3600'\n",
    "errors": []
  },
  {
    "id": "007",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n    command:\n    - sleep\n    - '3600'\n",
    "errors": []
  },
  {
    "id": "008",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n    command:\n    - sleep\n    - '3600'\n",
    "errors": []
  },
  {
    "id": "009",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n      runAsNonRoot: true\n    command:\n    - sleep\n    - '3600'\n",
    "errors": []
  },
  {
    "id": "010",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n    command:\n    - sleep\n    - '3600'\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "011",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: false\n    command:\n    - sleep\n    - '3600'\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "012",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add: []\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "013",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add: []\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "014",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:stable\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "015",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "016",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add: []\n        drop:\n        - NET_RAW\n        - NET_ADMIN\n        - SYS_ADMIN\n        - SYS_MODULE\n        - SYS_PTRACE\n        - SYS_CHROOT\n      privileged: false\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "017",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "018",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "019",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "errors": []
  },
  {
    "id": "020",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-postgresql-hl.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "021",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-postgresql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "022",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-api-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "023",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-redis.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "024",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9102'\nspec:\n  type: ExternalName\n  externalName: release-name-statsd.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "025",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-triggerer.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "026",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-worker.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "027",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "028",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "029",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: default\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "030",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "031",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "032",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "033",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "034",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "035",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "036",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "037",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "038",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "039",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "040",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "041",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "042",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "043",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "044",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "045",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "046",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "047",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "048",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "049",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "050",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "051",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "052",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "053",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "054",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "errors": []
  },
  {
    "id": "055",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "errors": []
  },
  {
    "id": "056",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "errors": []
  },
  {
    "id": "057",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "errors": []
  },
  {
    "id": "058",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits: {}\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "059",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "060",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "061",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "062",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "063",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        command:\n        - /bin/sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "064",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        command:\n        - /bin/sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "065",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "066",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "067",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "068",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "069",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "070",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "071",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "072",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "073",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "074",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "075",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "076",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "077",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "078",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "079",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "080",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "081",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "082",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "083",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "084",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "errors": []
  },
  {
    "id": "085",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "086",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: default\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "087",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "088",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "089",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "090",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: default\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "091",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "092",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "errors": []
  },
  {
    "id": "093",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ExternalName\n  externalName: release-name-argocd-applicationset-controller.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "094",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\n  name: release-name-argocd-repo-server\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-argocd-repo-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "095",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-argocd-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "096",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ExternalName\n  externalName: release-name-argocd-dex-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "097",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ExternalName\n  externalName: release-name-argocd-redis.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "098",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "099",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "100",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "101",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "102",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "103",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "104",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "105",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "106",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "107",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "108",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "109",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "110",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "111",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "112",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "113",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "114",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "115",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "116",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "117",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "118",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "119",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "120",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "121",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "122",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "123",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "errors": []
  },
  {
    "id": "124",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "errors": []
  },
  {
    "id": "125",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  sessionAffinity: None\n  type: ExternalName\n  externalName: release-name-argo-workflows-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "126",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "127",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "128",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "129",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "130",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "131",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          privileged: false\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "132",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          privileged: false\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "133",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-headless\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-postgresql-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "134",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-postgresql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "135",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: hub.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "136",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: trivy.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "137",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:stable\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "138",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "139",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "140",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "141",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "142",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "143",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "144",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "145",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "146",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "147",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "148",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "149",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "150",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "errors": []
  },
  {
    "id": "151",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "152",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "153",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "154",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "155",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:stable\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "156",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "157",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "158",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "159",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n          privileged: false\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "160",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n          privileged: false\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "161",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: 3600\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "162",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:stable\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "163",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "164",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "165",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "166",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "167",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "168",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "169",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "170",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "errors": []
  },
  {
    "id": "171",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:stable\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "172",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "173",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "174",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "175",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "176",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "177",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "178",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "179",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "errors": []
  },
  {
    "id": "180",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:stable\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "181",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "182",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "183",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "184",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "185",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "186",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "187",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "188",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "errors": []
  },
  {
    "id": "189",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\nspec:\n  type: ExternalName\n  externalName: release-name-sealed-secrets.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "190",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n    app.kubernetes.io/component: metrics\nspec:\n  type: ExternalName\n  externalName: release-name-sealed-secrets-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "191",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: default\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "192",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n          privileged: false\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "193",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n          privileged: false\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "194",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "195",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-external-dns.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "196",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  revisionHistoryLimit: 10\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/version: 0.18.0\n        helm.sh/chart: external-dns-9.0.3\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: external-dns\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      containers:\n      - name: external-dns\n        image: docker.io/bitnami/external-dns:0.18.0-debian-12-r4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --metrics-address=:7979\n        - --log-level=info\n        - --log-format=text\n        - --policy=upsert-only\n        - --provider=aws\n        - --registry=txt\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --aws-api-retries=3\n        - --aws-zone-type=\n        - --aws-batch-change-size=1000\n        env:\n        - name: AWS_DEFAULT_REGION\n          value: us-east-1\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 7979\n        livenessProbe:\n          tcpSocket:\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 2\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "197",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-broker\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: broker\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: broker\n      app.kubernetes.io/part-of: kafka\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "198",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "199",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka-controller-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-kafka-controller-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "200",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: kafka\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-kafka.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "201",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  podManagementPolicy: Parallel\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n  serviceName: release-name-kafka-controller-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: kafka\n        app.kubernetes.io/version: 4.0.0\n        helm.sh/chart: kafka-32.4.3\n        app.kubernetes.io/component: controller-eligible\n        app.kubernetes.io/part-of: kafka\n      annotations:\n        checksum/configuration: d96601aff02b0e88a9bc5c8593a6d0446462e05650b1eb84af185e551160d1c8\n        checksum/secret: dba2cdb043e84e63d768aad3292379237050a24915b1b3e70c1f2408e8cd76e8\n    spec:\n      automountServiceAccountToken: false\n      hostNetwork: false\n      hostIPC: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: kafka\n                  app.kubernetes.io/component: controller-eligible\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        seccompProfile:\n          type: RuntimeDefault\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-config\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add: []\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \". /opt/bitnami/scripts/libkafka.sh\\nconfigure_kafka_sasl() {\\n    # Replace\\\n          \\ placeholders with passwords\\n    replace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"interbroker-password-placeholder\\\" \\\"$KAFKA_INTER_BROKER_PASSWORD\\\"\\n\\\n          \\    replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"controller-password-placeholder\\\"\\\n          \\ \\\"$KAFKA_CONTROLLER_PASSWORD\\\"\\n    read -r -a passwords <<< \\\"$(tr ',;'\\\n          \\ ' ' <<<\\\"${KAFKA_CLIENT_PASSWORDS:-}\\\")\\\"\\n    for ((i = 0; i < ${#passwords[@]};\\\n          \\ i++)); do\\n        replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"password-placeholder-${i}\\\\\\\n          \\\"\\\" \\\"${passwords[i]}\\\\\\\"\\\"\\n    done\\n}\\n\\ncp /configmaps/server.properties\\\n          \\ $KAFKA_CONF_FILE\\n\\n# Get pod ID and role, last and second last fields\\\n          \\ in the pod name respectively\\nPOD_ID=\\\"${MY_POD_NAME##*-}\\\"\\nPOD_ROLE=\\\"\\\n          ${MY_POD_NAME%-*}\\\"; POD_ROLE=\\\"${POD_ROLE##*-}\\\"\\n\\n# Configure node.id\\n\\\n          ID=$((POD_ID + KAFKA_MIN_ID))\\n[[ -f \\\"/bitnami/kafka/data/meta.properties\\\"\\\n          \\ ]] && ID=\\\"$(grep \\\"node.id\\\" /bitnami/kafka/data/meta.properties | awk\\\n          \\ -F '=' '{print $2}')\\\"\\nkafka_server_conf_set \\\"node.id\\\" \\\"$ID\\\"\\n# Configure\\\n          \\ initial controllers\\nif [[ \\\"controller\\\" =~ \\\"$POD_ROLE\\\" ]]; then\\n\\\n          \\    INITIAL_CONTROLLERS=()\\n    for ((i = 0; i < 3; i++)); do\\n       \\\n          \\ var=\\\"KAFKA_CONTROLLER_${i}_DIR_ID\\\"; DIR_ID=\\\"${!var}\\\"\\n        [[ $i\\\n          \\ -eq $POD_ID ]] && [[ -f \\\"/bitnami/kafka/data/meta.properties\\\" ]] &&\\\n          \\ DIR_ID=\\\"$(grep \\\"directory.id\\\" /bitnami/kafka/data/meta.properties |\\\n          \\ awk -F '=' '{print $2}')\\\"\\n        INITIAL_CONTROLLERS+=(\\\"${i}@${KAFKA_FULLNAME}-${POD_ROLE}-${i}.${KAFKA_CONTROLLER_SVC_NAME}.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}:${KAFKA_CONTROLLER_PORT}:${DIR_ID}\\\"\\\n          )\\n    done\\n    echo \\\"${INITIAL_CONTROLLERS[*]}\\\" | awk -v OFS=',' '{$1=$1}1'\\\n          \\ > /shared/initial-controllers.txt\\nfi\\nreplace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"advertised-address-placeholder\\\" \\\"${MY_POD_NAME}.${KAFKA_FULLNAME}-${POD_ROLE}-headless.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}\\\"\\\n          \\nsasl_env_vars=(\\n  KAFKA_CLIENT_PASSWORDS\\n  KAFKA_INTER_BROKER_PASSWORD\\n\\\n          \\  KAFKA_INTER_BROKER_CLIENT_SECRET\\n  KAFKA_CONTROLLER_PASSWORD\\n  KAFKA_CONTROLLER_CLIENT_SECRET\\n\\\n          )\\nfor env_var in \\\"${sasl_env_vars[@]}\\\"; do\\n    file_env_var=\\\"${env_var}_FILE\\\"\\\n          \\n    if [[ -n \\\"${!file_env_var:-}\\\" ]]; then\\n        if [[ -r \\\"${!file_env_var:-}\\\"\\\n          \\ ]]; then\\n            export \\\"${env_var}=$(< \\\"${!file_env_var}\\\")\\\"\\n\\\n          \\            unset \\\"${file_env_var}\\\"\\n        else\\n            warn \\\"\\\n          Skipping export of '${env_var}'. '${!file_env_var:-}' is not readable.\\\"\\\n          \\n        fi\\n    fi\\ndone\\nconfigure_kafka_sasl\\nif [[ -f /secret-config/server-secret.properties\\\n          \\ ]]; then\\n    cat /secret-config/server-secret.properties >> $KAFKA_CONF_FILE\\n\\\n          fi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_FULLNAME\n          value: release-name-kafka\n        - name: CLUSTER_DOMAIN\n          value: cluster.local\n        - name: KAFKA_VOLUME_DIR\n          value: /bitnami/kafka\n        - name: KAFKA_CONF_FILE\n          value: /config/server.properties\n        - name: KAFKA_MIN_ID\n          value: '0'\n        - name: KAFKA_CONTROLLER_SVC_NAME\n          value: release-name-kafka-controller-headless\n        - name: KAFKA_CONTROLLER_PORT\n          value: '9093'\n        - name: KAFKA_CONTROLLER_0_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-0-id\n        - name: KAFKA_CONTROLLER_1_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-1-id\n        - name: KAFKA_CONTROLLER_2_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-2-id\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: kafka-config\n          mountPath: /config\n        - name: kafka-configmaps\n          mountPath: /configmaps\n        - name: kafka-secret-config\n          mountPath: /secret-config\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n        env:\n        - name: KAFKA_HEAP_OPTS\n          value: -XX:InitialRAMPercentage=75 -XX:MaxRAMPercentage=75\n        - name: KAFKA_CFG_PROCESS_ROLES\n          value: controller,broker\n        - name: KAFKA_INITIAL_CONTROLLERS_FILE\n          value: /shared/initial-controllers.txt\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: KAFKA_KRAFT_CLUSTER_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: cluster-id\n        - name: KAFKA_KRAFT_BOOTSTRAP_SCRAM_USERS\n          value: 'true'\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        ports:\n        - name: controller\n          containerPort: 9093\n        - name: client\n          containerPort: 9092\n        - name: interbroker\n          containerPort: 9094\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - pgrep\n            - -f\n            - kafka\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: controller\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: logs\n          mountPath: /opt/bitnami/kafka/logs\n        - name: kafka-config\n          mountPath: /opt/bitnami/kafka/config/server.properties\n          subPath: server.properties\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      volumes:\n      - name: kafka-configmaps\n        configMap:\n          name: release-name-kafka-controller-configuration\n      - name: kafka-secret-config\n        emptyDir: {}\n      - name: kafka-config\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: init-shared\n        emptyDir: {}\n      - name: kafka-sasl\n        projected:\n          sources:\n          - secret:\n              name: release-name-kafka-user-passwords\n      - name: logs\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "202",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "203",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "204",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-postgresql-hl.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "205",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-postgresql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "206",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-keycloak-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "207",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-keycloak.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "208",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-16.7.26\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:17.6.0-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_USER\n          value: bn_keycloak\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_DATABASE\n          value: bitnami_keycloak\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "209",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  podManagementPolicy: Parallel\n  serviceName: release-name-keycloak-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n  template:\n    metadata:\n      annotations:\n        checksum/configmap-env-vars: 32b97b2f95a4b4c37d1e7ba71916ca4c1f73d024fd8a3e077f2c86fba821b469\n        checksum/secrets: 04a6ccee11f98c05bad75a87ca3f46ce99e265b4b5d35d83a1a541641c64d855\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: keycloak\n        app.kubernetes.io/version: 26.3.3\n        helm.sh/chart: keycloak-25.2.0\n        app.kubernetes.io/component: keycloak\n        app.kubernetes.io/part-of: keycloak\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: keycloak\n                  app.kubernetes.io/component: keycloak\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-write-dirs\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '. /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying writable dirs to empty dir\"\n\n          # In order to not break the application functionality we need to make some\n\n          # directories writable, so we need to copy it to an empty dir volume\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir\n\n          info \"Copy operation completed\"\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: keycloak\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        envFrom:\n        - configMapRef:\n            name: release-name-keycloak-env-vars\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: discovery\n          containerPort: 7800\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 1\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /realms/master\n            port: http\n            scheme: HTTP\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /bitnami/keycloak\n          subPath: app-volume-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/lib/quarkus\n          subPath: app-quarkus-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/data\n          subPath: app-data-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/providers\n          subPath: app-providers-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/themes\n          subPath: app-themes-dir\n        - name: keycloak-secrets\n          mountPath: /opt/bitnami/keycloak/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: keycloak-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-keycloak\n          - secret:\n              name: release-name-postgresql\n              items:\n              - key: password\n                path: db-password\n",
    "errors": []
  },
  {
    "id": "210",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "211",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-mariadb-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "212",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-mariadb.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "213",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "214",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "215",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "216",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "217",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "218",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ExternalName\n  externalName: release-name-minio-console.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "219",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ExternalName\n  externalName: release-name-minio.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "220",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: minio\n        app.kubernetes.io/part-of: minio\n      annotations:\n        checksum/credentials-secret: d7f9b363039fe1a5911ad90a3de83dbcfe0eba441592f6815c91be1861016459\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: minio\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: OnRootMismatch\n        supplementalGroups: []\n        sysctls: []\n      initContainers: null\n      containers:\n      - name: minio\n        image: docker.io/bitnami/minio:2025.7.23-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MINIO_DISTRIBUTED_MODE_ENABLED\n          value: 'no'\n        - name: MINIO_SCHEME\n          value: http\n        - name: MINIO_FORCE_NEW_KEYS\n          value: 'no'\n        - name: MINIO_ROOT_USER_FILE\n          value: /opt/bitnami/minio/secrets/root-user\n        - name: MINIO_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/minio/secrets/root-password\n        - name: MINIO_SKIP_CLIENT\n          value: 'yes'\n        - name: MINIO_API_PORT_NUMBER\n          value: '9000'\n        - name: MINIO_BROWSER\n          value: 'off'\n        - name: MINIO_PROMETHEUS_AUTH_TYPE\n          value: public\n        - name: MINIO_DATA_DIR\n          value: /bitnami/minio/data\n        ports:\n        - name: api\n          containerPort: 9000\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: api\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/minio/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /.mc\n          subPath: app-mc-dir\n        - name: minio-credentials\n          mountPath: /opt/bitnami/minio/secrets/\n        - name: data\n          mountPath: /bitnami/minio/data\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: minio-credentials\n        secret:\n          secretName: release-name-minio\n      - name: data\n        persistentVolumeClaim:\n          claimName: release-name-minio\n",
    "errors": []
  },
  {
    "id": "221",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: console\n        app.kubernetes.io/part-of: minio\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: console\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: console\n        image: docker.io/bitnami/minio-object-browser:2.0.2-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - server\n        - --host\n        - 0.0.0.0\n        - --port\n        - '9090'\n        env:\n        - name: CONSOLE_MINIO_SERVER\n          value: http://release-name-minio:9000\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        ports:\n        - name: http\n          containerPort: 9090\n        livenessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          httpGet:\n            path: /minio\n            port: http\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /.console\n          subPath: app-console-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "222",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "223",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  publishNotReadyAddresses: false\n  externalName: release-name-mongodb.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "224",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "errors": []
  },
  {
    "id": "225",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "errors": []
  },
  {
    "id": "226",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "errors": []
  },
  {
    "id": "227",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "228",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-mysql-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "229",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-mysql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "230",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  podManagementPolicy: ''\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mysql-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 0aa4c7bb029f4871ca0cdece35adf5a5caaf6f2e016ef25c8cae18901c047e48\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/version: 9.4.0\n        helm.sh/chart: mysql-14.0.3\n        app.kubernetes.io/part-of: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mysql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mysql/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mysql/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mysql\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MYSQL_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mysql/secrets/mysql-root-password\n        - name: MYSQL_ENABLE_SSL\n          value: 'no'\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_DATABASE\n          value: my_database\n        envFrom: null\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        startupProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mysql\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/logs\n          subPath: app-logs-dir\n        - name: config\n          mountPath: /opt/bitnami/mysql/conf/my.cnf\n          subPath: my.cnf\n        - name: mysql-credentials\n          mountPath: /opt/bitnami/mysql/secrets/\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-mysql\n      - name: mysql-credentials\n        secret:\n          secretName: release-name-mysql\n          items:\n          - key: mysql-root-password\n            path: mysql-root-password\n          - key: mysql-password\n            path: mysql-password\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "231",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "232",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\n  annotations: null\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalTrafficPolicy: Cluster\n  externalName: release-name-nginx.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "233",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "errors": []
  },
  {
    "id": "234",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "errors": []
  },
  {
    "id": "235",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "errors": []
  },
  {
    "id": "236",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "237",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-postgresql-hl.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "238",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-postgresql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "239",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "240",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "241",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "242",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  publishNotReadyAddresses: true\n  trafficDistribution: PreferClose\n  type: ExternalName\n  externalName: release-name-rabbitmq-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "243",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-rabbitmq.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "244",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  serviceName: release-name-rabbitmq-headless\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: rabbitmq\n        app.kubernetes.io/version: 4.1.3\n        helm.sh/chart: rabbitmq-16.0.14\n      annotations:\n        checksum/config: 81f24711d28981f706e1ae5b2e3ae075d7014b462120496ce6f50d5053194f5e\n        checksum/secret: 0db9412a28617166460cac5333a5cf8ebbc34cfe87087e0f09b593395c5fa678\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: rabbitmq\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      terminationGracePeriodSeconds: 120\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-plugins-dir\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '#!/bin/bash\n\n\n          . /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying plugins dir to empty dir\"\n\n          # In order to not break the possibility of installing custom plugins, we\n          need\n\n          # to make the plugins directory writable, so we need to copy it to an empty\n          dir volume\n\n          cp -r --preserve=mode /opt/bitnami/rabbitmq/plugins/ /emptydir/app-plugins-dir\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: rabbitmq\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/bash\n              - -ec\n              - \"if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then\\n\\\n                \\    /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t \\\"120\\\" -d \\\"\\\n                false\\\"\\nelse\\n    rabbitmqctl stop_app\\nfi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_FORCE_BOOT\n          value: 'no'\n        - name: RABBITMQ_NODE_NAME\n          value: rabbit@$(MY_POD_NAME).release-name-rabbitmq-headless.$(MY_POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_UPDATE_PASSWORD\n          value: 'no'\n        - name: RABBITMQ_MNESIA_DIR\n          value: /opt/bitnami/rabbitmq/.rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)\n        - name: RABBITMQ_LDAP_ENABLE\n          value: 'no'\n        - name: RABBITMQ_LOGS\n          value: '-'\n        - name: RABBITMQ_ULIMIT_NOFILES\n          value: '65535'\n        - name: RABBITMQ_USE_LONGNAME\n          value: 'true'\n        - name: RABBITMQ_ERL_COOKIE_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-erlang-cookie\n        - name: RABBITMQ_LOAD_DEFINITIONS\n          value: 'no'\n        - name: RABBITMQ_DEFINITIONS_FILE\n          value: /app/load_definition.json\n        - name: RABBITMQ_SECURE_PASSWORD\n          value: 'yes'\n        - name: RABBITMQ_USERNAME\n          value: user\n        - name: RABBITMQ_PASSWORD_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-password\n        - name: RABBITMQ_PLUGINS\n          value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap\n        envFrom: null\n        ports:\n        - name: amqp\n          containerPort: 5672\n        - name: dist\n          containerPort: 25672\n        - name: stats\n          containerPort: 15672\n        - name: epmd\n          containerPort: 4369\n        - name: metrics\n          containerPort: 9419\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/virtual-hosts\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/local-alarms\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: configuration\n          mountPath: /bitnami/rabbitmq/conf\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/etc/rabbitmq\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/lib/rabbitmq\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/\n          subPath: app-erlang-cookie\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/log/rabbitmq\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/plugins\n          subPath: app-plugins-dir\n        - name: data\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/mnesia\n        - name: rabbitmq-secrets\n          mountPath: /opt/bitnami/rabbitmq/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: configuration\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq-config\n      - name: rabbitmq-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq\n          - secret:\n              name: release-name-rabbitmq\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: rabbitmq\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "245",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "246",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "247",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\nspec:\n  type: ExternalName\n  externalName: release-name-redis-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "248",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  type: ExternalName\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  externalName: release-name-redis-master.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "249",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  type: ExternalName\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  externalName: release-name-redis-replicas.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "250",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "251",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "252",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "253",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "254",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "255",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "256",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  type: ExternalName\n  externalName: release-name-thanos-query-frontend.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "257",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-grpc\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ExternalName\n  externalName: release-name-thanos-query-grpc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "258",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ExternalName\n  externalName: release-name-thanos-query.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "259",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query-frontend\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query-frontend\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query-frontend\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query-frontend\n        - --log.level=info\n        - --log.format=logfmt\n        - --http-address=0.0.0.0:9090\n        - --query-frontend.downstream-url=http://release-name-thanos-query:9090\n        ports:\n        - name: http\n          containerPort: 9090\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "errors": []
  },
  {
    "id": "260",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query\n        - --log.level=info\n        - --log.format=logfmt\n        - --grpc-address=0.0.0.0:10901\n        - --http-address=0.0.0.0:10902\n        - --query.replica-label=replica\n        - --alert.query-url=http://release-name-thanos-query.default.svc.cluster.local:9090\n        ports:\n        - name: http\n          containerPort: 10902\n          protocol: TCP\n        - name: grpc\n          containerPort: 10901\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "errors": []
  },
  {
    "id": "261",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "262",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "263",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-mariadb-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "264",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-mariadb.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "265",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  type: ExternalName\n  externalTrafficPolicy: Cluster\n  sessionAffinity: None\n  externalName: release-name-wordpress.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "266",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "errors": []
  },
  {
    "id": "267",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "errors": []
  },
  {
    "id": "268",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "errors": []
  },
  {
    "id": "269",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "270",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:stable\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "271",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "272",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ExternalName\n  externalName: release-name-cert-manager-cainjector.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "273",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ExternalName\n  externalName: release-name-cert-manager.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "274",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ExternalName\n  externalName: release-name-cert-manager-webhook.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "275",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: default\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "276",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "277",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "278",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: default\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "279",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "280",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "281",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: default\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "282",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "283",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "284",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "285",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: default\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "286",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "287",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "288",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9964'\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  type: ExternalName\n  externalName: cilium-envoy.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "289",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hubble-peer\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: hubble-peer\nspec:\n  internalTrafficPolicy: Local\n  type: ExternalName\n  externalName: hubble-peer.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "290",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "291",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "293",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "294",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "295",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "296",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "297",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "298",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "299",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "300",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "301",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "302",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "303",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "304",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "305",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - IPC_LOCK\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "306",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "308",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "309",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "310",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "311",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "312",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "313",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "314",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "318",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "319",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "320",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "321",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "322",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "323",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "324",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "325",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "326",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "327",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "328",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "329",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "330",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "errors": []
  },
  {
    "id": "331",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "333",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "334",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "335",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add: []\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "337",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "339",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "340",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "341",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "343",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "errors": []
  },
  {
    "id": "344",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "errors": []
  },
  {
    "id": "346",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "errors": []
  },
  {
    "id": "347",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "errors": []
  },
  {
    "id": "348",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        terminationMessagePolicy: FallbackToLogsOnError\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "errors": []
  },
  {
    "id": "349",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: aws-cluster-autoscaler\n  maxUnavailable: 1\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "350",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-aws-cluster-autoscaler.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "351",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  type: ExternalName\n  externalName: release-name-datadog-cluster-agent.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "352",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent-admission-controller\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  type: ExternalName\n  externalName: release-name-datadog-cluster-agent-admission-controller.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "353",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  internalTrafficPolicy: Local\n  type: ExternalName\n  externalName: release-name-datadog.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "355",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "356",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "358",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "359",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "361",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "362",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "363",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "364",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "365",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: elasticsearch-master-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "366",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations: {}\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: false\n  externalName: elasticsearch-master.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "367",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master-headless\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  publishNotReadyAddresses: true\n  type: ExternalName\n  externalName: elasticsearch-master-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "368",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "369",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "370",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n          runAsNonRoot: true\n          runAsUser: 1000\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "371",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "372",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n          runAsNonRoot: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "373",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "374",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n          privileged: false\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "errors": []
  },
  {
    "id": "375",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "errors": []
  },
  {
    "id": "376",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "errors": []
  },
  {
    "id": "377",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "errors": []
  },
  {
    "id": "378",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-external-dns.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "379",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "errors": []
  },
  {
    "id": "380",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "381",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "382",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\n    external-secrets.io/component: webhook\nspec:\n  type: ExternalName\n  externalName: release-name-external-secrets-webhook.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "383",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "errors": []
  },
  {
    "id": "385",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "386",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "387",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "388",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "389",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      dnsPolicy: ClusterFirst\n",
    "errors": []
  },
  {
    "id": "390",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "errors": []
  },
  {
    "id": "392",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "errors": []
  },
  {
    "id": "393",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "errors": []
  },
  {
    "id": "394",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-fluent-bit.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "395",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "errors": []
  },
  {
    "id": "396",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: default\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "errors": []
  },
  {
    "id": "397",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "errors": []
  },
  {
    "id": "398",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "errors": []
  },
  {
    "id": "399",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "errors": []
  },
  {
    "id": "400",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:stable\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "401",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "402",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "403",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "404",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "405",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "406",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "407",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql-witness\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: witness\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: witness\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "408",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n    matchExpressions:\n    - key: job-name\n      operator: NotIn\n      values:\n      - release-name-valkey-cluster-cluster-update\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "409",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-postgresql-ha-pgpool.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "410",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: false\n  externalName: release-name-postgresql-ha-postgresql-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "411",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  type: ExternalName\n  externalName: release-name-postgresql-ha-postgresql.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "412",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-valkey-cluster-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "413",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ExternalName\n  sessionAffinity: None\n  externalName: release-name-valkey-cluster.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "414",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-http\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ExternalName\n  externalName: release-name-gitea-http.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "415",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-ssh\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ExternalName\n  externalName: release-name-gitea-ssh.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "416",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 4.6.3\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: pgpool\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: pgpool\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      containers:\n      - name: pgpool\n        image: docker.io/bitnamilegacy/pgpool:4.6.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PGPOOL_BACKEND_NODES\n          value: 0:release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,1:release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,2:release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,\n        - name: PGPOOL_SR_CHECK_USER\n          value: sr_check_user\n        - name: PGPOOL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/sr-check-password\n        - name: PGPOOL_SR_CHECK_DATABASE\n          value: postgres\n        - name: PGPOOL_ENABLE_LDAP\n          value: 'no'\n        - name: PGPOOL_POSTGRES_USERNAME\n          value: gitea\n        - name: PGPOOL_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: PGPOOL_ADMIN_USERNAME\n          value: admin\n        - name: PGPOOL_ADMIN_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/admin-password\n        - name: PGPOOL_AUTHENTICATION_METHOD\n          value: scram-sha-256\n        - name: PGPOOL_ENABLE_LOAD_BALANCING\n          value: 'yes'\n        - name: PGPOOL_ENABLE_CONNECTION_CACHE\n          value: 'yes'\n        - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE\n          value: transaction\n        - name: PGPOOL_ENABLE_LOG_CONNECTIONS\n          value: 'no'\n        - name: PGPOOL_ENABLE_LOG_HOSTNAME\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PCP_PROCESSES\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT\n          value: 'no'\n        - name: PGPOOL_RESERVED_CONNECTIONS\n          value: '1'\n        - name: PGPOOL_CHILD_LIFE_TIME\n          value: ''\n        - name: PGPOOL_ENABLE_TLS\n          value: 'no'\n        - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT\n          value: '6'\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /opt/bitnami/scripts/pgpool/healthcheck.sh\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - bash\n            - -ec\n            - PGPASSWORD=$(< $PGPOOL_POSTGRES_PASSWORD_FILE) psql -U \"gitea\" -d \"gitea\"\n              -h /opt/bitnami/pgpool/tmp -tA -c \"SELECT 1\" >/dev/null\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/etc\n          subPath: app-etc-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/logs\n          subPath: app-logs-dir\n        - name: postgresql-creds\n          subPath: pgpool-password\n          mountPath: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: pgpool-creds\n          subPath: admin-password\n          mountPath: /opt/bitnami/pgpool/secrets/admin-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/pgpool/secrets/sr-check-password\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n          items:\n          - key: password\n            path: pgpool-password\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n",
    "errors": []
  },
  {
    "id": "417",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "418",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "419",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "420",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "421",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "422",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "423",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          runAsNonRoot: true\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "424",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "425",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "426",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "427",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "428",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext:\n          privileged: false\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n          privileged: false\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "errors": []
  },
  {
    "id": "429",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: data\nspec:\n  replicas: 3\n  podManagementPolicy: Parallel\n  serviceName: release-name-postgresql-ha-postgresql-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: data\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: postgresql\n        role: data\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: postgresql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnamilegacy/postgresql-repmgr:17.6.0-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /pre-stop.sh\n              - '25'\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_USER\n          value: gitea\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_DB\n          value: gitea\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'true'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit, repmgr\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PRIMARY_PORT\n          value: '5432'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REPMGR_UPGRADE_EXTENSION\n          value: 'no'\n        - name: REPMGR_PGHBA_TRUST_ALL\n          value: 'no'\n        - name: REPMGR_MOUNTED_CONF_DIR\n          value: /bitnami/repmgr/conf\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: REPMGR_PARTNER_NODES\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,\n        - name: REPMGR_PRIMARY_HOST\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_NAME\n          value: $(MY_POD_NAME)\n        - name: REPMGR_NODE_NETWORK_NAME\n          value: $(MY_POD_NAME).release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_TYPE\n          value: data\n        - name: REPMGR_LOG_LEVEL\n          value: NOTICE\n        - name: REPMGR_CONNECT_TIMEOUT\n          value: '5'\n        - name: REPMGR_RECONNECT_ATTEMPTS\n          value: '2'\n        - name: REPMGR_RECONNECT_INTERVAL\n          value: '3'\n        - name: REPMGR_USERNAME\n          value: repmgr\n        - name: REPMGR_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: REPMGR_USE_PASSFILE\n          value: 'true'\n        - name: REPMGR_PASSFILE_PATH\n          value: /opt/bitnami/repmgr/conf/.pgpass\n        - name: REPMGR_DATABASE\n          value: repmgr\n        - name: REPMGR_FENCE_OLD_PRIMARY\n          value: 'no'\n        - name: REPMGR_CHILD_NODES_CHECK_INTERVAL\n          value: '5'\n        - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT\n          value: '1'\n        - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT\n          value: '30'\n        - name: POSTGRESQL_SR_CHECK\n          value: 'yes'\n        - name: POSTGRESQL_SR_CHECK_USERNAME\n          value: sr_check_user\n        - name: POSTGRESQL_SR_CHECK_DATABASE\n          value: postgres\n        - name: POSTGRESQL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/sr-check-password\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /liveness-probe.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /readiness-probe.sh\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/conf\n          subPath: repmgr-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/tmp\n          subPath: repmgr-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/logs\n          subPath: repmgr-logs-dir\n        - name: postgresql-creds\n          mountPath: /opt/bitnami/postgresql/secrets/postgres-password\n          subPath: postgres-password\n        - name: postgresql-creds\n          subPath: password\n          mountPath: /opt/bitnami/postgresql/secrets/password\n        - name: postgresql-creds\n          subPath: repmgr-password\n          mountPath: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/postgresql/secrets/sr-check-password\n        - name: data\n          mountPath: /bitnami/postgresql\n        - name: scripts\n          mountPath: /pre-stop.sh\n          subPath: pre-stop.sh\n        - name: scripts\n          mountPath: /liveness-probe.sh\n          subPath: liveness-probe.sh\n        - name: scripts\n          mountPath: /readiness-probe.sh\n          subPath: readiness-probe.sh\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: scripts\n        configMap:\n          name: release-name-postgresql-ha-postgresql-scripts\n          defaultMode: 493\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "430",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n  replicas: 3\n  serviceName: release-name-valkey-cluster-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: valkey-cluster\n        app.kubernetes.io/version: 8.1.3\n        helm.sh/chart: valkey-cluster-3.0.24\n      annotations:\n        checksum/scripts: b416f9c23ba0f844168db1fda55802c08f6e6158998b5dc543fb3e2827aee988\n        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/config: d260a75cea2840d303318eeb5409b6f6f190e3e221ea66f3e98857402f575fa0\n    spec:\n      hostNetwork: false\n      enableServiceLinks: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: valkey-cluster\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: release-name-valkey-cluster\n        image: docker.io/bitnamilegacy/valkey-cluster:8.1.3-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - \"# Backwards compatibility change\\nif ! [[ -f /opt/bitnami/valkey/etc/valkey.conf\\\n          \\ ]]; then\\n    echo COPYING FILE\\n    cp  /opt/bitnami/valkey/etc/valkey-default.conf\\\n          \\ /opt/bitnami/valkey/etc/valkey.conf\\nfi\\npod_index=($(echo \\\"$POD_NAME\\\"\\\n          \\ | tr \\\"-\\\" \\\"\\\\n\\\"))\\npod_index=\\\"${pod_index[-1]}\\\"\\nif [[ \\\"$pod_index\\\"\\\n          \\ == \\\"0\\\" ]]; then\\n  export VALKEY_CLUSTER_CREATOR=\\\"yes\\\"\\n  export VALKEY_CLUSTER_REPLICAS=\\\"\\\n          0\\\"\\nfi\\n/opt/bitnami/scripts/valkey-cluster/entrypoint.sh /opt/bitnami/scripts/valkey-cluster/run.sh\\n\"\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VALKEY_NODES\n          value: 'release-name-valkey-cluster-0.release-name-valkey-cluster-headless\n            release-name-valkey-cluster-1.release-name-valkey-cluster-headless release-name-valkey-cluster-2.release-name-valkey-cluster-headless '\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: VALKEY_AOF_ENABLED\n          value: 'yes'\n        - name: VALKEY_TLS_ENABLED\n          value: 'no'\n        - name: VALKEY_PORT_NUMBER\n          value: '6379'\n        ports:\n        - name: tcp-redis\n          containerPort: 6379\n        - name: tcp-redis-bus\n          containerPort: 16379\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: scripts\n          mountPath: /scripts\n        - name: valkey-data\n          mountPath: /bitnami/valkey/data\n          subPath: null\n        - name: default-config\n          mountPath: /opt/bitnami/valkey/etc/valkey-default.conf\n          subPath: valkey-default.conf\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: scripts\n        configMap:\n          name: release-name-valkey-cluster-scripts\n          defaultMode: 493\n      - name: default-config\n        configMap:\n          name: release-name-valkey-cluster-default\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: valkey-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: valkey-cluster\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "errors": []
  },
  {
    "id": "431",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:stable\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "432",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "433",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "434",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "435",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "436",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "errors": []
  },
  {
    "id": "437",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "errors": []
  },
  {
    "id": "438",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "errors": []
  },
  {
    "id": "439",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ExternalName\n  externalName: release-name-grafana.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "440",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "441",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "442",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "443",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "444",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "445",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: default\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "446",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "447",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "448",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "449",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-headless\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n    variant: headless\nspec:\n  type: ExternalName\n  externalName: release-name-loki-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "450",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-memberlist\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\nspec:\n  type: ExternalName\n  publishNotReadyAddresses: true\n  externalName: release-name-loki-memberlist.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "451",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  type: ExternalName\n  externalName: release-name-loki.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "452",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: default\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "453",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "454",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "455",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "456",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "457",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "458",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "459",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "errors": []
  },
  {
    "id": "460",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "errors": []
  },
  {
    "id": "461",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "errors": []
  },
  {
    "id": "462",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "errors": []
  },
  {
    "id": "463",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "464",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "465",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "466",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "errors": []
  },
  {
    "id": "467",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-core.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "468",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-database.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "469",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-jobservice.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "470",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-portal.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "471",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-redis.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "472",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-registry.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "473",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  type: ExternalName\n  externalName: release-name-harbor-trivy.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "474",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "475",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "476",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "477",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "errors": []
  },
  {
    "id": "478",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "errors": []
  },
  {
    "id": "479",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "errors": []
  },
  {
    "id": "480",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "errors": []
  },
  {
    "id": "481",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "errors": []
  },
  {
    "id": "482",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "errors": []
  },
  {
    "id": "483",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "484",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "485",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "486",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "487",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "488",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "errors": []
  },
  {
    "id": "489",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "490",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "491",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "492",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "493",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "494",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "496",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "498",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "499",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "500",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: trivy\n    app.kubernetes.io/component: trivy\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-trivy\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: trivy\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: trivy\n        app.kubernetes.io/component: trivy\n      annotations:\n        checksum/secret: 6fbc9db4d37c6bc53260a46776f92e1e0390c1d50cd884e19a8d0a3c73deed8f\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: trivy\n        image: goharbor/trivy-adapter-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n        env:\n        - name: HTTP_PROXY\n          value: ''\n        - name: HTTPS_PROXY\n          value: ''\n        - name: NO_PROXY\n          value: release-name-harbor-core,release-name-harbor-jobservice,release-name-harbor-database,release-name-harbor-registry,release-name-harbor-portal,release-name-harbor-trivy,release-name-harbor-exporter,127.0.0.1,localhost,.local,.internal\n        - name: SCANNER_LOG_LEVEL\n          value: info\n        - name: SCANNER_TRIVY_CACHE_DIR\n          value: /home/scanner/.cache/trivy\n        - name: SCANNER_TRIVY_REPORTS_DIR\n          value: /home/scanner/.cache/reports\n        - name: SCANNER_TRIVY_DEBUG_MODE\n          value: 'false'\n        - name: SCANNER_TRIVY_VULN_TYPE\n          value: os,library\n        - name: SCANNER_TRIVY_TIMEOUT\n          value: 5m0s\n        - name: SCANNER_TRIVY_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: gitHubToken\n        - name: SCANNER_TRIVY_SEVERITY\n          value: UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL\n        - name: SCANNER_TRIVY_IGNORE_UNFIXED\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_JAVA_DB_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-db,ghcr.io/aquasecurity/trivy-db\n        - name: SCANNER_TRIVY_JAVA_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-java-db,ghcr.io/aquasecurity/trivy-java-db\n        - name: SCANNER_TRIVY_OFFLINE_SCAN\n          value: 'false'\n        - name: SCANNER_TRIVY_SECURITY_CHECKS\n          value: vuln\n        - name: SCANNER_TRIVY_INSECURE\n          value: 'false'\n        - name: SCANNER_API_SERVER_ADDR\n          value: :8080\n        - name: SCANNER_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_STORE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_JOB_QUEUE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        ports:\n        - name: api-server\n          containerPort: 8080\n        volumeMounts:\n        - name: data\n          mountPath: /home/scanner/.cache\n          subPath: null\n          readOnly: false\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/healthy\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/ready\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 1\n            memory: 1Gi\n          requests:\n            cpu: 200m\n            memory: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n",
    "errors": []
  },
  {
    "id": "501",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-agent-injector-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-vault-agent-injector-svc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "502",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-internal\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    vault-internal: 'true'\n  annotations: null\nspec:\n  publishNotReadyAddresses: true\n  type: ExternalName\n  externalName: release-name-vault-internal.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "503",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  publishNotReadyAddresses: true\n  type: ExternalName\n  externalName: release-name-vault.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "505",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "errors": []
  },
  {
    "id": "506",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: default\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "errors": []
  },
  {
    "id": "509",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "510",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n",
    "errors": []
  },
  {
    "id": "511",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "512",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: default\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "513",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "514",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          privileged: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "515",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "516",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "517",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "518",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "519",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller-admission\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-ingress-nginx-controller-admission.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "520",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations: null\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  type: ExternalName\n  ipFamilyPolicy: SingleStack\n  ipFamilies:\n  - IPv4\n  externalName: release-name-ingress-nginx-controller.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "522",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "errors": []
  },
  {
    "id": "523",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "errors": []
  },
  {
    "id": "525",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "errors": []
  },
  {
    "id": "526",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "527",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "528",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "529",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "530",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "531",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "errors": []
  },
  {
    "id": "532",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    release: release-name\n    istio: pilot\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: istiod\n      istio: pilot\n  unhealthyPodEvictionPolicy: AlwaysAllow\n",
    "errors": []
  },
  {
    "id": "533",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    app: istiod\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  type: ExternalName\n  externalName: istiod.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "534",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: default\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "errors": []
  },
  {
    "id": "535",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "errors": []
  },
  {
    "id": "536",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins-agent\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  type: ExternalName\n  externalName: release-name-jenkins-agent.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "537",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  type: ExternalName\n  externalName: release-name-jenkins.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "539",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "540",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "541",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "542",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "543",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n          privileged: false\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "544",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "545",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "546",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "547",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "548",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "549",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "550",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "551",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "552",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kong-proxy\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    enable-metrics: 'true'\nspec:\n  type: ExternalName\n  externalName: release-name-kong-proxy.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "553",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  type: ExternalName\n  externalName: release-name-kubernetes-dashboard-api.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "554",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-auth\n    app.kubernetes.io/version: 1.3.0\n    app.kubernetes.io/component: auth\n  annotations: null\n  name: release-name-kubernetes-dashboard-auth\nspec:\n  type: ExternalName\n  externalName: release-name-kubernetes-dashboard-auth.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "555",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  type: ExternalName\n  externalName: release-name-kubernetes-dashboard-metrics-scraper.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "556",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  type: ExternalName\n  externalName: release-name-kubernetes-dashboard-web.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "557",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "errors": []
  },
  {
    "id": "558",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "errors": []
  },
  {
    "id": "559",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "errors": []
  },
  {
    "id": "560",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "errors": []
  },
  {
    "id": "561",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "errors": []
  },
  {
    "id": "562",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "563",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n        app.kubernetes.io/version: 1.2.2\n        app.kubernetes.io/component: metrics-scraper\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-metrics-scraper\n        image: docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "564",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-web\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-web\n        app.kubernetes.io/version: 1.7.0\n        app.kubernetes.io/component: web\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-web\n        image: docker.io/kubernetesui/dashboard-web:1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --settings-config-map-name=release-name-kubernetes-dashboard-web-settings\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: web\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "565",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: release-name-kyverno-svc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "566",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: release-name-kyverno-svc-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "567",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-background-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: kyverno-background-controller-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "568",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: kyverno-cleanup-controller.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "569",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: kyverno-cleanup-controller-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "570",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-reports-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  type: ExternalName\n  externalName: kyverno-reports-controller-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "571",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-admission-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: admission-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - admission-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kyverno-pre\n        image: reg.kyverno.io/kyverno/kyvernopre:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --loggingFormat=text\n        - --v=2\n        - --openreportsEnabled=false\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n      containers:\n      - name: kyverno\n        image: reg.kyverno.io/kyverno/kyverno:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --caSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-ca\n        - --tlsSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-pair\n        - --backgroundServiceAccountName=system:serviceaccount:default:kyverno-background-controller\n        - --reportsServiceAccountName=system:serviceaccount:default:kyverno-reports-controller\n        - --servicePort=443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --crdWatcher=false\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --admissionReports=true\n        - --maxAdmissionReports=1000\n        - --autoUpdateWebhooks=true\n        - --enableConfigMapCaching=true\n        - --controllerRuntimeMetricsAddress=:8080\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --forceFailurePolicyIgnore=false\n        - --generateValidatingAdmissionPolicy=true\n        - --generateMutatingAdmissionPolicy=false\n        - --dumpPatches=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --protectManagedResources=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        resources:\n          limits:\n            memory: 384Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "572",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-background-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: background-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: background-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - background-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/background-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-background-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-background-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "573",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cleanup-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cleanup-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - cleanup-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/cleanup-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --caSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-ca\n        - --tlsSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-pair\n        - --servicePort=443\n        - --cleanupServerPort=9443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --protectManagedResources=false\n        - --ttlReconciliationInterval=1m\n        env:\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-cleanup-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-cleanup-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:cleanup-controller\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-cleanup-controller\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "errors": []
  },
  {
    "id": "574",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-reports-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: reports-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: reports-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - reports-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/reports-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --openreportsEnabled=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --admissionReports=true\n        - --aggregateReports=true\n        - --policyReports=true\n        - --validatingAdmissionPolicyReports=true\n        - --mutatingAdmissionPolicyReports=false\n        - --backgroundScan=true\n        - --backgroundScanWorkers=2\n        - --backgroundScanInterval=1h\n        - --skipResourceFilters=true\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-reports-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-reports-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: TUF_ROOT\n          value: /.sigstore\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "575",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "576",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "577",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "578",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "579",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "580",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "581",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "582",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "583",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-backend\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: longhorn-backend.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "584",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-frontend\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: longhorn-frontend.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "585",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-admission-webhook\n  name: longhorn-admission-webhook\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: longhorn-admission-webhook.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "586",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-recovery-backend\n  name: longhorn-recovery-backend\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: longhorn-recovery-backend.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "587",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "588",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "589",
    "policy_id": "non_existent_service_account",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "privileged container detected"
    ]
  },
  {
    "id": "590",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities not defined"
    ]
  },
  {
    "id": "591",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        securityContext:\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "592",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "593",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "594",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "595",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "596",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "597",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "errors": []
  },
  {
    "id": "598",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "599",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "600",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: default\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "601",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "602",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "603",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "604",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "605",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "606",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "errors": []
  },
  {
    "id": "607",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "608",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "609",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "610",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "611",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "errors": []
  },
  {
    "id": "612",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "613",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "614",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "615",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "616",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "617",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "618",
    "policy_id": "job_ttl_after_finished",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "privileged container detected"
    ]
  },
  {
    "id": "619",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          readOnlyRootFilesystem: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "620",
    "policy_id": "non_existent_service_account",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "privileged container detected"
    ]
  },
  {
    "id": "621",
    "policy_id": "drop_capabilities",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n            - NET_RAW\n            - NET_ADMIN\n            - SYS_ADMIN\n            - SYS_MODULE\n            - SYS_PTRACE\n            - SYS_CHROOT\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "622",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "623",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n          runAsNonRoot: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "624",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "625",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "626",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "627",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "628",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "629",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "630",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "631",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "errors": []
  },
  {
    "id": "632",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metallb-webhook-service\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: metallb-webhook-service.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "633",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains NET_ADMIN, NET_RAW, SYS_ADMIN",
      "capabilities not defined",
      "capabilities not defined"
    ]
  },
  {
    "id": "634",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains NET_ADMIN, NET_RAW, SYS_ADMIN",
      "capabilities not defined",
      "capabilities not defined"
    ]
  },
  {
    "id": "635",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains NET_ADMIN, NET_RAW, SYS_ADMIN",
      "capabilities not defined",
      "capabilities not defined"
    ]
  },
  {
    "id": "637",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "639",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "640",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "641",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "642",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "643",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "644",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          readOnlyRootFilesystem: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "645",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "646",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains NET_ADMIN, NET_RAW, SYS_ADMIN",
      "capabilities not defined",
      "capabilities not defined"
    ]
  },
  {
    "id": "647",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "648",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "649",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "650",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "651",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "652",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          runAsNonRoot: true\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          runAsNonRoot: true\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "654",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "655",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "656",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "657",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "658",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "659",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "660",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "661",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "662",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "663",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "664",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "665",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "666",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "667",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n          privileged: false\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n          privileged: false\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n          privileged: false\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "errors": []
  },
  {
    "id": "669",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "errors": []
  },
  {
    "id": "670",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "errors": []
  },
  {
    "id": "671",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "errors": []
  },
  {
    "id": "672",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ExternalName\n  externalName: release-name-metrics-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "673",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: default\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "674",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-metrics-server\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "675",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  type: ExternalName\n  externalName: release-name-nextcloud.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "676",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "errors": []
  },
  {
    "id": "677",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "errors": []
  },
  {
    "id": "678",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          privileged: false\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "errors": []
  },
  {
    "id": "679",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          privileged: false\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "errors": []
  },
  {
    "id": "680",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "errors": []
  },
  {
    "id": "681",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: default\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "errors": []
  },
  {
    "id": "682",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "errors": []
  },
  {
    "id": "683",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "errors": []
  },
  {
    "id": "684",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "errors": []
  },
  {
    "id": "685",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  externalTrafficPolicy: Local\n  type: ExternalName\n  externalName: release-name-nginx-ingress-controller.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "686",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          privileged: false\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "errors": []
  },
  {
    "id": "687",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "errors": []
  },
  {
    "id": "688",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          privileged: false\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "errors": []
  },
  {
    "id": "689",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-oauth2-proxy.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "690",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: default\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "errors": []
  },
  {
    "id": "691",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "errors": []
  },
  {
    "id": "692",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "errors": []
  },
  {
    "id": "693",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ExternalName\n  externalName: release-name-grafana.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "694",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\n  annotations: null\nspec:\n  type: ExternalName\n  externalName: release-name-kube-state-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "695",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\n    jobLabel: node-exporter\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ExternalName\n  externalName: release-name-prometheus-node-exporter.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "696",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-alertmanager\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-alertmanager\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  sessionAffinity: None\n  type: ExternalName\n  externalName: release-name-kube-promethe-alertmanager.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "697",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-coredns\n  labels:\n    app: kube-prometheus-stack-coredns\n    jobLabel: coredns\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-coredns.kube-system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "698",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-controller-manager\n  labels:\n    app: kube-prometheus-stack-kube-controller-manager\n    jobLabel: kube-controller-manager\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-kube-controller-manager.kube-system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "699",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-etcd\n  labels:\n    app: kube-prometheus-stack-kube-etcd\n    jobLabel: kube-etcd\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-kube-etcd.kube-system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "700",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-proxy\n  labels:\n    app: kube-prometheus-stack-kube-proxy\n    jobLabel: kube-proxy\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-kube-proxy.kube-system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "701",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-scheduler\n  labels:\n    app: kube-prometheus-stack-kube-scheduler\n    jobLabel: kube-scheduler\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-kube-scheduler.kube-system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "702",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  type: ExternalName\n  externalName: release-name-kube-promethe-operator.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "703",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-prometheus\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-prometheus\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  publishNotReadyAddresses: false\n  sessionAffinity: None\n  type: ExternalName\n  externalName: release-name-kube-promethe-prometheus.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "704",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "706",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: default\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "710",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "711",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "712",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "713",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "714",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "715",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "716",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "717",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "718",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "719",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "720",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "721",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n          privileged: false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "722",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "errors": []
  },
  {
    "id": "724",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "725",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "726",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "errors": []
  },
  {
    "id": "727",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "errors": []
  },
  {
    "id": "728",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "errors": []
  },
  {
    "id": "729",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "730",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: default\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "731",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "732",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "733",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "734",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: default\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "735",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "736",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "737",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: default\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "738",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "739",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "740",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-alertmanager.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "741",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager-headless\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-alertmanager-headless.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "742",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ExternalName\n  externalName: release-name-kube-state-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "743",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ExternalName\n  externalName: release-name-prometheus-node-exporter.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "744",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/probe: pushgateway\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  type: ExternalName\n  externalName: release-name-prometheus-pushgateway.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "745",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  sessionAffinity: None\n  type: ExternalName\n  externalName: release-name-prometheus-server.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "746",
    "policy_id": "no_host_network",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n  hostNetwork: false\n",
    "errors": []
  },
  {
    "id": "748",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: default\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "752",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "753",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          privileged: false\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "errors": []
  },
  {
    "id": "754",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "errors": []
  },
  {
    "id": "756",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "757",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "758",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "759",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "760",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "761",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "762",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "763",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "764",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: default\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "765",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "766",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "767",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "768",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "errors": []
  },
  {
    "id": "769",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          readOnlyRootFilesystem: true\n          privileged: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "770",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "771",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          privileged: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "772",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          privileged: false\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "errors": []
  },
  {
    "id": "773",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  type: ExternalName\n  externalName: release-name-pgadmin4.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "774",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "775",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "776",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "errors": []
  },
  {
    "id": "777",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:stable\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "778",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "779",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "780",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  type: ExternalName\n  externalName: release-name-traefik.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "781",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "782",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "783",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          privileged: false\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "errors": []
  },
  {
    "id": "784",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  type: ExternalName\n  externalName: release-name-velero.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "785",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "786",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "787",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "788",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "789",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "790",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "791",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "792",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "793",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "794",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "795",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "796",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "797",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "798",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "799",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "800",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:stable\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "errors": []
  },
  {
    "id": "801",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "802",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "803",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "804",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "805",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open-api-doc\nspec:\n  type: ExternalName\n  externalName: open-api-doc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "806",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-metrics\nspec:\n  type: ExternalName\n  externalName: argocd-metrics.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "807",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||:stable'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "errors": []
  },
  {
    "id": "808",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "errors": []
  },
  {
    "id": "809",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "errors": []
  },
  {
    "id": "810",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "errors": []
  },
  {
    "id": "811",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "errors": []
  },
  {
    "id": "812",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "errors": []
  },
  {
    "id": "813",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "errors": []
  },
  {
    "id": "814",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "errors": []
  },
  {
    "id": "815",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "errors": []
  },
  {
    "id": "816",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "817",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "818",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "819",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "820",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "821",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubwz0un1-cffc\n  labels:\n    app: kubwz0un1-cffc\nspec:\n  type: ExternalName\n  externalName: kubwz0un1-cffc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "822",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    component: elasticsearch\n    role: data\n  annotations:\n    cloud.google.com/load-balancer-type: Internal\nspec:\n  type: ExternalName\n  externalName: elasticsearch.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "823",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    getambassador.io/config: \"---\\napiVersion: ambassador/v0\\nkind:  Mapping\\nname:\\\n      \\ webapp_mapping\\nprefix: /jupyter/\\nservice: jupyter-web-app-service.kubeflow\\n\\\n      add_request_headers:\\n  x-forwarded-prefix: /jupyter\"\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n    run: jupyter-web-app\n  name: jupyter-web-app-service\n  namespace: kubeflow\nspec:\n  type: ExternalName\n  externalName: jupyter-web-app-service.kubeflow.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "824",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "825",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "errors": []
  },
  {
    "id": "826",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "errors": []
  },
  {
    "id": "827",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:stable\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "errors": []
  },
  {
    "id": "828",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "829",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "830",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "831",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "832",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "833",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "834",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "835",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "836",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "837",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "838",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "839",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "840",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "841",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "842",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "843",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "844",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "845",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "errors": []
  },
  {
    "id": "846",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "errors": []
  },
  {
    "id": "847",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "errors": []
  },
  {
    "id": "848",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "errors": []
  },
  {
    "id": "849",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "errors": []
  },
  {
    "id": "850",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "851",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "852",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "853",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "854",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "857",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "858",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "860",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "861",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "862",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "863",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver:stable\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "errors": []
  },
  {
    "id": "864",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "errors": []
  },
  {
    "id": "865",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "errors": []
  },
  {
    "id": "866",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "errors": []
  },
  {
    "id": "867",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "errors": []
  },
  {
    "id": "868",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "errors": []
  },
  {
    "id": "869",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: default\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "errors": []
  },
  {
    "id": "870",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "errors": []
  },
  {
    "id": "871",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "errors": []
  },
  {
    "id": "872",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "errors": []
  },
  {
    "id": "873",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  labels:\n    app: nginx\nspec:\n  type: ExternalName\n  loadBalancerSourceRanges:\n  - 10.30.88.0/24\n  externalName: nginx-service-lb.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "874",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "875",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "876",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "877",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "878",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "879",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ws-app01\n  name: vote\n  namespace: attin-studio\nspec:\n  sessionAffinity: None\n  type: ExternalName\n  externalName: vote.attin-studio.svc.cluster.local\nstatus:\n  loadBalancer: {}\n",
    "errors": []
  },
  {
    "id": "881",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "errors": []
  },
  {
    "id": "882",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "errors": []
  },
  {
    "id": "883",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: false\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "errors": []
  },
  {
    "id": "886",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains SYS_ADMIN",
      "privileged container detected"
    ]
  },
  {
    "id": "887",
    "policy_id": "no_host_network",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "privileged container detected"
    ]
  },
  {
    "id": "888",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n          readOnlyRootFilesystem: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "889",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n          readOnlyRootFilesystem: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "890",
    "policy_id": "non_existent_service_account",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "privileged container detected"
    ]
  },
  {
    "id": "891",
    "policy_id": "drop_capabilities",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": false,
    "patched_yaml": null,
    "errors": [
      "capabilities.drop missing",
      "capabilities.add still contains SYS_ADMIN",
      "privileged container detected"
    ]
  },
  {
    "id": "892",
    "policy_id": "no_privileged",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        securityContext:\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "893",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n          runAsNonRoot: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "894",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n          runAsNonRoot: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "896",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "897",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "898",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "899",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      - name: csi-bosplugin\n        securityContext:\n          privileged: false\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "errors": []
  },
  {
    "id": "900",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb:stable\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "errors": []
  },
  {
    "id": "901",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "errors": []
  },
  {
    "id": "902",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "errors": []
  },
  {
    "id": "903",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "errors": []
  },
  {
    "id": "904",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "errors": []
  },
  {
    "id": "905",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard-test\n  labels:\n    addon: kubernetes-dashboard.addons.k8s.io\n    app: kubernetes-dashboard-test\n    kubernetes.io/cluster-service: 'true'\n    facing: external\nspec:\n  type: ExternalName\n  externalName: kubernetes-dashboard-test.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "906",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c:stable\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "errors": []
  },
  {
    "id": "908",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "errors": []
  },
  {
    "id": "909",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "errors": []
  },
  {
    "id": "910",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 500m\n          requests:\n            memory: 100Mi\n            cpu: 100m\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        securityContext:\n          privileged: false\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "errors": []
  },
  {
    "id": "911",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "912",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "913",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "914",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Never\n",
    "errors": []
  },
  {
    "id": "915",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n  labels:\n    app: nginx-test\n  name: nginx-test\nspec:\n  type: ExternalName\n  externalName: nginx-test.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "916",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "917",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: default\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "errors": []
  },
  {
    "id": "918",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "919",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "920",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "922",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "errors": []
  },
  {
    "id": "923",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: default\n",
    "errors": []
  },
  {
    "id": "924",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "errors": []
  },
  {
    "id": "927",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "928",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "929",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "930",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "931",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "932",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "933",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: default\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "934",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "935",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "936",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "937",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  type: ExternalName\n  externalName: app.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "938",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "errors": []
  },
  {
    "id": "939",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "errors": []
  },
  {
    "id": "940",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "errors": []
  },
  {
    "id": "941",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "errors": []
  },
  {
    "id": "942",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "errors": []
  },
  {
    "id": "943",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "errors": []
  },
  {
    "id": "944",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n        securityContext:\n          privileged: false\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "errors": []
  },
  {
    "id": "945",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "946",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "947",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "948",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "949",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "950",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "951",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "952",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "953",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "954",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "955",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis:stable\n        ports:\n        - containerPort: 6379\n",
    "errors": []
  },
  {
    "id": "956",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "957",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "958",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "959",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "960",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "961",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:stable\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "errors": []
  },
  {
    "id": "962",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "errors": []
  },
  {
    "id": "963",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "errors": []
  },
  {
    "id": "964",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "errors": []
  },
  {
    "id": "965",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "errors": []
  },
  {
    "id": "966",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "errors": []
  },
  {
    "id": "967",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "errors": []
  },
  {
    "id": "968",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "969",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "970",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "971",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "972",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause:stable\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "973",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause:stable\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "974",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "975",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "976",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "977",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "978",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "979",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n",
    "errors": []
  },
  {
    "id": "980",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev:stable\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "errors": []
  },
  {
    "id": "981",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "982",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "983",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "984",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "985",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "986",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "987",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "988",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "989",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "errors": []
  },
  {
    "id": "990",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "991",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "992",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "errors": []
  },
  {
    "id": "1018",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1019",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1020",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1021",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1022",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open5gs-pgw-svc-pool\n  namespace: open5gs\n  labels:\n    epc-mode: pgw\nspec:\n  type: ExternalName\n  externalName: open5gs-pgw-svc-pool.open5gs.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1023",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo:stable\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "1025",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "1026",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "1027",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "1028",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "errors": []
  },
  {
    "id": "1029",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  type: ExternalName\n  externalName: node-exporter.monitoring.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1030",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: payment\n  labels:\n    name: payment\nspec:\n  type: ExternalName\n  externalName: payment.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1031",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: qod-api\n  labels:\n    app: qod-api\n    tier: api\nspec:\n  type: ExternalName\n  externalName: qod-api.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1032",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "1033",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox:stable\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1034",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1035",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1036",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1037",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1038",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1039",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1040",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1041",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1042",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1043",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: observability\n    app.kubernetes.io/instance: parca\n    app.kubernetes.io/name: parca\n    app.kubernetes.io/version: v0.7.1\n  name: parca\n  namespace: parca\nspec:\n  type: ExternalName\n  externalName: parca.parca.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1044",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1045",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1046",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1047",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1048",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1049",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "errors": []
  },
  {
    "id": "1050",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1051",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1052",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1053",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server:stable\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "errors": []
  },
  {
    "id": "1054",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1055",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1056",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1057",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1058",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1059",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "errors": []
  },
  {
    "id": "1060",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1061",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: radarr\nspec:\n  type: ExternalName\n  externalName: radarr.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1062",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1063",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1064",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1065",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1067",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "errors": []
  },
  {
    "id": "1068",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "errors": []
  },
  {
    "id": "1069",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "errors": []
  },
  {
    "id": "1070",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "errors": []
  },
  {
    "id": "1071",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:stable\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1072",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1073",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1074",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1075",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: OnFailure\n",
    "errors": []
  },
  {
    "id": "1076",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "errors": []
  },
  {
    "id": "1077",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "errors": []
  },
  {
    "id": "1078",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "errors": []
  },
  {
    "id": "1079",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "errors": []
  },
  {
    "id": "1080",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyuubi-example-service\nspec:\n  type: ExternalName\n  externalName: kyuubi-example-service.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1082",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1083",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1084",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1085",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1087",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "errors": []
  },
  {
    "id": "1088",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "errors": []
  },
  {
    "id": "1089",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "errors": []
  },
  {
    "id": "1090",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "errors": []
  },
  {
    "id": "1091",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1092",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "errors": []
  },
  {
    "id": "1093",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1094",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1095",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1096",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1097",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1098",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n      requests:\n        cpu: 100m\n        memory: 128Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1099",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "errors": []
  },
  {
    "id": "1100",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "errors": []
  },
  {
    "id": "1101",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "errors": []
  },
  {
    "id": "1102",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "errors": []
  },
  {
    "id": "1103",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1104",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1105",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1106",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1107",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1108",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1109",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n                cpu: 500m\n                memory: 256Mi\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1110",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n                cpu: 500m\n                memory: 256Mi\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1111",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n                cpu: 500m\n                memory: 256Mi\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1112",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n                cpu: 500m\n                memory: 256Mi\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1113",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n                cpu: 500m\n                memory: 256Mi\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n            securityContext:\n              privileged: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "errors": []
  },
  {
    "id": "1115",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1116",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1117",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1118",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1119",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  type: ExternalName\n  externalName: minio.minio.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1120",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1121",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1122",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1123",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1124",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1125",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1126",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1127",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1128",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1129",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1131",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1132",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1134",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1135",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1136",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1137",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: homeautomation-backend\n  labels:\n    app: homeautomation\n    tier: backend\nspec:\n  type: ExternalName\n  externalName: homeautomation-backend.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1139",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1140",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1141",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1142",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1143",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr:stable\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "errors": []
  },
  {
    "id": "1145",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1146",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1147",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1148",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1149",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest:stable\n        ports:\n        - containerPort: 8123\n",
    "errors": []
  },
  {
    "id": "1151",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1152",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1153",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1154",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1155",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: music-server\n  namespace: mytunes\n  labels:\n    name: music-db\nspec:\n  type: ExternalName\n  externalName: music-server.mytunes.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1157",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1158",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1159",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: phpmyadmin-service\nspec:\n  type: ExternalName\n  externalName: phpmyadmin-service.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1160",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      terminationGracePeriodSeconds: 180\n",
    "errors": []
  },
  {
    "id": "1161",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n",
    "errors": []
  },
  {
    "id": "1162",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      terminationGracePeriodSeconds: 180\n",
    "errors": []
  },
  {
    "id": "1164",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      terminationGracePeriodSeconds: 10\n",
    "errors": []
  },
  {
    "id": "1165",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      terminationGracePeriodSeconds: 10\n",
    "errors": []
  },
  {
    "id": "1166",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n        securityContext:\n          privileged: false\n      terminationGracePeriodSeconds: 10\n",
    "errors": []
  },
  {
    "id": "1167",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sugampipelinesjavascriptdocker\nspec:\n  type: ExternalName\n  externalName: sugampipelinesjavascriptdocker.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1168",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: bookinfo-uat-ratings\n  namespace: student063-bookinfo-uat\nspec:\n  type: ExternalName\n  externalName: bookinfo-uat-ratings.student063-bookinfo-uat.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1169",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:stable\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1170",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:stable\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1171",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1172",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1173",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1174",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1175",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1176",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1177",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1178",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "errors": []
  },
  {
    "id": "1179",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "errors": []
  },
  {
    "id": "1180",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "errors": []
  },
  {
    "id": "1181",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "errors": []
  },
  {
    "id": "1182",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1183",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1184",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1185",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1186",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1187",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nacos-svc\n  namespace: default\n  labels:\n    app: nacos-svc\nspec:\n  type: ExternalName\n  externalName: nacos-svc.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1188",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1189",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: default\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1190",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1191",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1192",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "errors": []
  },
  {
    "id": "1193",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: openldap\n  labels:\n    app: openldap\nspec:\n  type: ExternalName\n  externalName: openldap.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1194",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: metrics-service\n  namespace: system\nspec:\n  type: ExternalName\n  externalName: metrics-service.system.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1195",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1196",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1197",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1198",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1199",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox:stable\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "errors": []
  },
  {
    "id": "1200",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  restartPolicy: Always\n",
    "errors": []
  },
  {
    "id": "1201",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  restartPolicy: Always\n",
    "errors": []
  },
  {
    "id": "1202",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Always\n",
    "errors": []
  },
  {
    "id": "1203",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  restartPolicy: Always\n",
    "errors": []
  },
  {
    "id": "1204",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "1205",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "1206",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "1207",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "1208",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "errors": []
  },
  {
    "id": "1209",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1210",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1211",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1212",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1213",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1214",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "errors": []
  },
  {
    "id": "1215",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "errors": []
  },
  {
    "id": "1216",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "errors": []
  },
  {
    "id": "1217",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "errors": []
  },
  {
    "id": "1219",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1220",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1221",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1222",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1224",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:stable\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1225",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1226",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1227",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1228",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1229",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1230",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1231",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1232",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "errors": []
  },
  {
    "id": "1233",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1234",
    "policy_id": "non_existent_service_account",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: default\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "errors": []
  },
  {
    "id": "1235",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1237",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1238",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1239",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1240",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1241",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine:stable\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "errors": []
  },
  {
    "id": "1242",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n            securityContext:\n              readOnlyRootFilesystem: true\n              privileged: false\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "errors": []
  },
  {
    "id": "1243",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n            securityContext:\n              runAsNonRoot: true\n              privileged: false\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "errors": []
  },
  {
    "id": "1244",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "errors": []
  },
  {
    "id": "1245",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n            resources:\n              requests:\n                cpu: 100m\n                memory: 128Mi\n              limits:\n                cpu: 500m\n                memory: 256Mi\n            securityContext:\n              privileged: false\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "errors": []
  },
  {
    "id": "1246",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:stable\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "errors": []
  },
  {
    "id": "1248",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1249",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1250",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1251",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n",
    "errors": []
  },
  {
    "id": "1252",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:stable\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "1253",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "1254",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "1255",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "1256",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "errors": []
  },
  {
    "id": "1259",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1260",
    "policy_id": "read_only_root_fs",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1261",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1262",
    "policy_id": "run_as_non_root",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1263",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1264",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1265",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1266",
    "policy_id": "set_requests_limits",
    "accepted": false,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": null,
    "errors": [
      "container image missing or empty"
    ]
  },
  {
    "id": "1267",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo:stable\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "errors": []
  },
  {
    "id": "1268",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1269",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1270",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1271",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1272",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1273",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1274",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1275",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1276",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1277",
    "policy_id": "job_ttl_after_finished",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n  ttlSecondsAfterFinished: 3600\n",
    "errors": []
  },
  {
    "id": "1278",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn:stable\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "errors": []
  },
  {
    "id": "1279",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "errors": []
  },
  {
    "id": "1280",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "errors": []
  },
  {
    "id": "1281",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n        securityContext:\n          privileged: false\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "errors": []
  },
  {
    "id": "1282",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n            cpu: 500m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n        securityContext:\n          privileged: false\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "errors": []
  },
  {
    "id": "1283",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable\n",
    "errors": []
  },
  {
    "id": "1284",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      readOnlyRootFilesystem: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1285",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsNonRoot: true\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1286",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1287",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n    securityContext:\n      privileged: false\n",
    "errors": []
  },
  {
    "id": "1288",
    "policy_id": "no_host_ipc",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1289",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause:stable\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1290",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause:stable\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1291",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      readOnlyRootFilesystem: true\n      privileged: false\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      readOnlyRootFilesystem: true\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1292",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      readOnlyRootFilesystem: true\n      privileged: false\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      readOnlyRootFilesystem: true\n      privileged: false\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1293",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1294",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1295",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1296",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n      privileged: false\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 256Mi\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "errors": []
  },
  {
    "id": "1297",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\nspec:\n  type: ExternalName\n  externalName: grafana.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1298",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    xposer.stakater.com/annotations: 'kubernetes.io/ingress.class: external-ingress\n\n      ingress.kubernetes.io/force-ssl-redirect: true\n\n      exposeIngressUrl: globally'\n  labels:\n    app: hello-kubernetes\n    expose: 'true'\n  name: hello-kubernetes\nspec:\n  type: ExternalName\n  externalName: hello-kubernetes.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1299",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\nspec:\n  type: ExternalName\n  externalName: redis.default.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1300",
    "policy_id": "dangling_service",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: azconf-app\n  namespace: azconf\nspec:\n  type: ExternalName\n  externalName: azconf-app.azconf.svc.cluster.local\n",
    "errors": []
  },
  {
    "id": "1301",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "1302",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "1303",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "1304",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "errors": []
  },
  {
    "id": "1305",
    "policy_id": "no_latest_tag",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base:stable\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1306",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1307",
    "policy_id": "read_only_root_fs",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        securityContext:\n          readOnlyRootFilesystem: true\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1308",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1309",
    "policy_id": "run_as_non_root",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        securityContext:\n          runAsNonRoot: true\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1310",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1311",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1312",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  },
  {
    "id": "1313",
    "policy_id": "set_requests_limits",
    "accepted": true,
    "ok_schema": true,
    "ok_policy": true,
    "patched_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          privileged: false\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "errors": []
  }
]