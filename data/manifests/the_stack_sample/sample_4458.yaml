apiVersion: v1
items:
- apiVersion: v1
  data:
    k8s-pgmon.yaml: "groups:\n- name: k8s.rules\n  rules:\n  - expr: |\n      sum(rate(container_cpu_usage_seconds_total{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\", image!=\"\", container_name!=\"\"}[5m]))\
      \ by (namespace)\n    record: namespace:container_cpu_usage_seconds_total:sum_rate\n\
      \  - expr: |\n      sum by (namespace, pod_name, container_name) (\n       \
      \ rate(container_cpu_usage_seconds_total{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"\
      , image!=\"\", container_name!=\"\"}[5m])\n      )\n    record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate\n\
      \  - expr: |\n      sum(container_memory_usage_bytes{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"\
      , image!=\"\", container_name!=\"\"}) by (namespace)\n    record: namespace:container_memory_usage_bytes:sum\n\
      \  - expr: |\n      sum by (namespace, label_name) ( sum(rate(container_cpu_usage_seconds_total{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\", image!=\"\",container_name!=\"\"}[5m]))\
      \ by (namespace, pod_name) * on (namespace, pod_name) group_left(label_name)\
      \ label_replace(kube_pod_labels{job=\"kube-state-metrics\"}, \"pod_name\", \"\
      $1\", \"pod\", \"(.*)\")\n      )\n    record: namespace_name:container_cpu_usage_seconds_total:sum_rate\n\
      \  - expr: |\n      sum by (namespace, label_name) (\n        sum(container_memory_usage_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\",image!=\"\", container_name!=\"\"}) by\
      \ (pod_name, namespace)\n      * on (namespace, pod_name) group_left(label_name)\n\
      \        label_replace(kube_pod_labels{job=\"kube-state-metrics\"}, \"pod_name\"\
      , \"$1\", \"pod\", \"(.*)\")\n      )\n    record: namespace_name:container_memory_usage_bytes:sum\n\
      \  - expr: |\n      sum by (namespace, label_name) (\n        sum(kube_pod_container_resource_requests_memory_bytes{job=\"\
      kube-state-metrics\"}) by (namespace, pod)\n      * on (namespace, pod) group_left(label_name)\n\
      \        label_replace(kube_pod_labels{job=\"kube-state-metrics\"}, \"pod_name\"\
      , \"$1\", \"pod\", \"(.*)\")\n      )\n    record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum\n\
      \  - expr: |\n      sum by (namespace, label_name) (\n        sum(kube_pod_container_resource_requests_cpu_cores{job=\"\
      kube-state-metrics\"} and on(pod) kube_pod_status_scheduled{condition=\"true\"\
      }) by (namespace, pod)\n      * on (namespace, pod) group_left(label_name)\n\
      \        label_replace(kube_pod_labels{job=\"kube-state-metrics\"}, \"pod_name\"\
      , \"$1\", \"pod\", \"(.*)\")\n      )\n    record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum\n\
      - name: kube-scheduler.rules\n  rules:\n  - expr: |\n      histogram_quantile(0.99,\
      \ sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job=\"kube-scheduler\"\
      }[5m])) without(instance, pod)) / 1e+06\n    labels:\n      quantile: \"0.99\"\
      \n    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.99\"\n    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.99, sum(rate(scheduler_binding_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.99\"\n    record: cluster_quantile:scheduler_binding_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.9\"\n    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.9\"\n    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.9, sum(rate(scheduler_binding_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.9\"\n    record: cluster_quantile:scheduler_binding_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.5\"\n    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.5\"\n    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.5, sum(rate(scheduler_binding_latency_microseconds_bucket{job=\"\
      kube-scheduler\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n     \
      \ quantile: \"0.5\"\n    record: cluster_quantile:scheduler_binding_latency:histogram_quantile\n\
      - name: kube-apiserver.rules\n  rules:\n  - expr: |\n      histogram_quantile(0.99,\
      \ sum(rate(apiserver_request_latencies_bucket{job=\"apiserver\"}[5m])) without(instance,\
      \ pod)) / 1e+06\n    labels:\n      quantile: \"0.99\"\n    record: cluster_quantile:apiserver_request_latencies:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job=\"\
      apiserver\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n      quantile:\
      \ \"0.9\"\n    record: cluster_quantile:apiserver_request_latencies:histogram_quantile\n\
      \  - expr: |\n      histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job=\"\
      apiserver\"}[5m])) without(instance, pod)) / 1e+06\n    labels:\n      quantile:\
      \ \"0.5\"\n    record: cluster_quantile:apiserver_request_latencies:histogram_quantile\n\
      \         \n- name: node.rules\n  rules:\n  - expr: sum(min(kube_pod_info) by\
      \ (node))\n    record: ':kube_pod_info_node_count:'\n  - expr: |\n      max(label_replace(kube_pod_info{job=\"\
      kube-state-metrics\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")) by (node, namespace,\
      \ pod)\n    record: 'node_namespace_pod:kube_pod_info:'\n  - expr: |\n     \
      \ count by (node) (sum by (node, cpu) (\n        node_cpu_seconds_total{job=\"\
      k8s-pgmon/kubernetes-nodes\"}\n      * on (namespace, pod) group_left(node)\n\
      \        node_namespace_pod:kube_pod_info:\n      ))\n    record: node:node_num_cpu:sum\n\
      \  - expr: |\n      1 - avg(rate(node_cpu_seconds_total{job=\"k8s-pgmon/kubernetes-nodes\"\
      ,mode=\"idle\"}[1m]))\n    record: :node_cpu_utilisation:avg1m\n  - expr: |\n\
      \      1 - avg by (node) (\n        rate(node_cpu_seconds_total{job=\"k8s-pgmon/kubernetes-nodes\"\
      ,mode=\"idle\"}[1m])\n      * on (namespace, pod) group_left(node)\n       \
      \ node_namespace_pod:kube_pod_info:)\n    record: node:node_cpu_utilisation:avg1m\n\
      \  - expr: |\n      sum(node_load1{job=\"k8s-pgmon/kubernetes-nodes\"})\n  \
      \    /\n      sum(node:node_num_cpu:sum)\n    record: ':node_cpu_saturation_load1:'\n\
      \  - expr: |\n      sum by (node) (\n        node_load1{job=\"k8s-pgmon/kubernetes-nodes\"\
      }\n      * on (namespace, pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n\
      \      )\n      /\n      node:node_num_cpu:sum\n    record: 'node:node_cpu_saturation_load1:'\n\
      \  - expr: |\n      1 -\n      sum(node_memory_MemFree_bytes{job=\"k8s-pgmon/kubernetes-nodes\"\
      } + node_memory_Cached_bytes{job=\"k8s-pgmon/kubernetes-nodes\"} + node_memory_Buffers_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"})\n      /\n      sum(node_memory_MemTotal_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"})\n    record: ':node_memory_utilisation:'\n  -\
      \ expr: |\n      sum(node_memory_MemFree_bytes{job=\"k8s-pgmon/kubernetes-nodes\"\
      } + node_memory_Cached_bytes{job=\"k8s-pgmon/kubernetes-nodes\"} + node_memory_Buffers_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"})\n    record: :node_memory_MemFreeCachedBuffers_bytes:sum\n\
      \  - expr: |\n      sum(node_memory_MemTotal_bytes{job=\"k8s-pgmon/kubernetes-nodes\"\
      })\n    record: :node_memory_MemTotal_bytes:sum\n  - expr: |\n      sum by (node)\
      \ (\n        (node_memory_MemFree_bytes{job=\"k8s-pgmon/kubernetes-nodes\"}\
      \ + node_memory_Cached_bytes{job=\"k8s-pgmon/kubernetes-nodes\"} + node_memory_Buffers_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"})\n        * on (namespace, pod) group_left(node)\n\
      \          node_namespace_pod:kube_pod_info:\n      )\n    record: node:node_memory_bytes_available:sum\n\
      \  - expr: |\n      sum by (node) (\n        node_memory_MemTotal_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"}\n        * on (namespace, pod) group_left(node)\n\
      \          node_namespace_pod:kube_pod_info:\n      )\n    record: node:node_memory_bytes_total:sum\n\
      \  - expr: |\n      (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)\n\
      \      /\n      scalar(sum(node:node_memory_bytes_total:sum))\n    record: node:node_memory_utilisation:ratio\n\
      \  - expr: |\n      1e3 * sum(\n        (rate(node_vmstat_pgpgin{job=\"k8s-pgmon/kubernetes-nodes\"\
      }[1m])\n       + rate(node_vmstat_pgpgout{job=\"k8s-pgmon/kubernetes-nodes\"\
      }[1m]))\n      )\n    record: :node_memory_swap_io_bytes:sum_rate\n  - expr:\
      \ |\n      1 -\n      sum by (node) (\n        (node_memory_MemFree_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"} + node_memory_Cached_bytes{job=\"k8s-pgmon/kubernetes-nodes\"\
      } + node_memory_Buffers_bytes{job=\"k8s-pgmon/kubernetes-nodes\"})\n      *\
      \ on (namespace, pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n\
      \      )\n      /\n      sum by (node) (\n        node_memory_MemTotal_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"}\n      * on (namespace, pod) group_left(node)\n\
      \        node_namespace_pod:kube_pod_info:\n      )\n    record: 'node:node_memory_utilisation:'\n\
      \  - expr: |\n      1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)\n\
      \    record: 'node:node_memory_utilisation_2:'\n  - expr: |\n      1e3 * sum\
      \ by (node) (\n        (rate(node_vmstat_pgpgin{job=\"k8s-pgmon/kubernetes-nodes\"\
      }[1m])\n       + rate(node_vmstat_pgpgout{job=\"k8s-pgmon/kubernetes-nodes\"\
      }[1m]))\n       * on (namespace, pod) group_left(node)\n         node_namespace_pod:kube_pod_info:\n\
      \      )\n    record: node:node_memory_swap_io_bytes:sum_rate\n  - expr: |\n\
      \      avg(irate(node_disk_io_time_seconds_total{job=\"k8s-pgmon/kubernetes-nodes\"\
      ,device=~\"nvme.+|rbd.+|sd.+|vd.+|xvd.+\"}[1m]))\n    record: :node_disk_utilisation:avg_irate\n\
      \  - expr: |\n      avg by (node) (\n        irate(node_disk_io_time_seconds_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=~\"nvme.+|rbd.+|sd.+|vd.+|xvd.+\"}[1m])\n\
      \      * on (namespace, pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n\
      \      )\n    record: node:node_disk_utilisation:avg_irate\n  - expr: |\n  \
      \    avg(irate(node_disk_io_time_weighted_seconds_total{job=\"k8s-pgmon/kubernetes-nodes\"\
      ,device=~\"nvme.+|rbd.+|sd.+|vd.+|xvd.+\"}[1m]) / 1e3)\n    record: :node_disk_saturation:avg_irate\n\
      \  - expr: |\n      avg by (node) (\n        irate(node_disk_io_time_weighted_seconds_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=~\"nvme.+|rbd.+|sd.+|vd.+|xvd.+\"}[1m])\
      \ / 1e3\n      * on (namespace, pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n\
      \      )\n    record: node:node_disk_saturation:avg_irate\n  - expr: |\n   \
      \   max by (namespace, pod, device) ((node_filesystem_size_bytes{fstype=~\"\
      ext[234]|btrfs|xfs|zfs\"}\n      - node_filesystem_avail_bytes{fstype=~\"ext[234]|btrfs|xfs|zfs\"\
      })\n      / node_filesystem_size_bytes{fstype=~\"ext[234]|btrfs|xfs|zfs\"})\n\
      \    record: 'node:node_filesystem_usage:'\n  - expr: |\n      max by (namespace,\
      \ pod, device) (node_filesystem_avail_bytes{fstype=~\"ext[234]|btrfs|xfs|zfs\"\
      } / node_filesystem_size_bytes{fstype=~\"ext[234]|btrfs|xfs|zfs\"})\n    record:\
      \ 'node:node_filesystem_avail:'\n  - expr: |\n      sum(irate(node_network_receive_bytes_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m])) +\n      sum(irate(node_network_transmit_bytes_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]))\n    record: :node_net_utilisation:sum_irate\n\
      \  - expr: |\n      sum by (node) (\n        (irate(node_network_receive_bytes_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]) +\n        irate(node_network_transmit_bytes_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]))\n      * on (namespace,\
      \ pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n      )\n\
      \    record: node:node_net_utilisation:sum_irate\n  - expr: |\n      sum(irate(node_network_receive_drop_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m])) +\n      sum(irate(node_network_transmit_drop_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]))\n    record: :node_net_saturation:sum_irate\n\
      \  - expr: |\n      sum by (node) (\n        (irate(node_network_receive_drop_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]) +\n        irate(node_network_transmit_drop_total{job=\"\
      k8s-pgmon/kubernetes-nodes\",device=\"eth0\"}[1m]))\n      * on (namespace,\
      \ pod) group_left(node)\n        node_namespace_pod:kube_pod_info:\n      )\n\
      \    record: node:node_net_saturation:sum_irate\n          \n- name: host-status\n\
      \  rules:\n  - alert: high_cpu_load\n    expr: node_load1 > ((count(node_cpu_seconds_total)\
      \ without (cpu)) * node_load1)\n    for: 30s\n    labels:\n      severity: warning\n\
      \    annotations:\n      summary: \"Server under high load\"\n      description:\
      \ \"Docker host is under high load, the avg load 1m is at {{ $value}}. Reported\
      \ by instance {{ $labels.instance }} of job {{ $labels.job }}.\"\n  \n  - alert:\
      \ high_memory_load\n    expr: (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes\
      \ + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum(node_memory_MemTotal_bytes)\
      \ * 100 > 85\n    for: 30s\n    labels:\n      severity: warning\n    annotations:\n\
      \      summary: \"Server memory is almost full\"\n      description: \"Docker\
      \ host memory usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance\
      \ }} of job {{ $labels.job }}.\"\n  \n  - alert: high_storage_load\n    expr:\
      \ (node_filesystem_size_bytes{fstype=\"aufs\"} - node_filesystem_free_bytes{fstype=\"\
      aufs\"}) / node_filesystem_size_bytes{fstype=\"aufs\"}  * 100 > 85\n    for:\
      \ 30s\n    labels:\n      severity: warning\n    annotations:\n      summary:\
      \ \"Server storage is almost full\"\n      description: \"Docker host storage\
      \ usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }}\
      \ of job {{ $labels.job }}.\"\n  \n  - alert: DiskWillFillIn4Hours\n    expr:\
      \ predict_linear(node_filesystem_free{job='node'}[1h], 4*3600) < 0\n    for:\
      \ 5m\n    labels:\n      severity: page\n    annotations:\n      summary: \"\
      Server Disk Will FillIn 4 Hours\"\n      description: \" {{ $labels.job }} of\
      \ {{ $labels.instance }} by {{ humanize $value}}% Server Disk Will FillIn 4\
      \ Hours\"\n   \n\n- name: kube-prometheus-node-recording.rules\n  rules:\n \
      \ - expr: sum(rate(node_cpu{mode!=\"idle\",mode!=\"iowait\"}[3m])) BY (instance)\n\
      \    record: instance:node_cpu:rate:sum\n  - expr: sum((node_filesystem_size{mountpoint=\"\
      /\"} - node_filesystem_free{mountpoint=\"/\"}))\n      BY (instance)\n    record:\
      \ instance:node_filesystem_usage:sum\n  - expr: sum(rate(node_network_receive_bytes[3m]))\
      \ BY (instance)\n    record: instance:node_network_receive_bytes:rate:sum\n\
      \  - expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)\n    record:\
      \ instance:node_network_transmit_bytes:rate:sum\n  - expr: sum(rate(node_cpu{mode!=\"\
      idle\",mode!=\"iowait\"}[5m])) WITHOUT (cpu, mode)\n      / ON(instance) GROUP_LEFT()\
      \ count(sum(node_cpu) BY (instance, cpu)) BY (instance)\n    record: instance:node_cpu:ratio\n\
      \  - expr: sum(rate(node_cpu{mode!=\"idle\",mode!=\"iowait\"}[5m]))\n    record:\
      \ cluster:node_cpu:sum_rate5m\n  - expr: cluster:node_cpu:rate5m / count(sum(node_cpu)\
      \ BY (instance, cpu))\n    record: cluster:node_cpu:ratio\n- name: kubernetes-absent\n\
      \  rules:\n  - alert: AlertmanagerDown\n    annotations:\n      message: Alertmanager\
      \ \u5DF2\u4ECE Prometheus \u76EE\u6807\u53D1\u73B0\u4E2D\u6D88\u5931.\n    \
      \  runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown\n\
      \    expr: |\n      absent(up{job=\"k8s-pgmon/prometheus\"} == 1)\n    for:\
      \ 15m\n    labels:\n      severity: critical\n  - alert: CoreDNSDown\n    annotations:\n\
      \      message: CoreDNS \u5DF2\u7ECF\u4ECE Prometheus \u76EE\u6807\u53D1\u73B0\
      \u4E2D\u6D88\u5931.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-corednsdown\n\
      \    expr: |\n      absent(up{job=\"kube-dns\"} == 1)\n    for: 15m\n    labels:\n\
      \      severity: critical\n  - alert: KubeAPIDown\n    annotations:\n      message:\
      \ KubeAPI has disappeared from Prometheus target discovery.\n      runbook_url:\
      \ https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown\n\
      \    expr: |\n      absent(up{job=\"apiserver\"} == 1)\n    for: 15m\n    labels:\n\
      \      severity: critical\n  - alert: KubeControllerManagerDown\n    annotations:\n\
      \      message: KubeControllerManager has disappeared from Prometheus target\
      \ discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown\n\
      \    expr: |\n      absent(up{job=\"kube-scheduler\"} == 1)\n    for: 15m\n\
      \    labels:\n      severity: critical\n  - alert: KubeSchedulerDown\n    annotations:\n\
      \      message: KubeScheduler has disappeared from Prometheus target discovery.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown\n\
      \    expr: |\n      absent(up{kubernetes_io_name=\"kube-state-metrics\"} ==\
      \ 1)\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStateMetricsDown\n\
      \    annotations:\n      message: KubeStateMetrics has disappeared from Prometheus\
      \ target discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricsdown\n\
      \    expr: |\n      absent(up{job=\"kube-state-metrics\"} == 1)\n    for: 15m\n\
      \    labels:\n      severity: critical\n  - alert: KubeletDown\n    annotations:\n\
      \      message: k8s-pgmon/kubernetes-nodes-kubelet has disappeared from Prometheus\
      \ target discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown\n\
      \    expr: |\n      absent(up{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"} ==\
      \ 1)\n    for: 15m\n    labels:\n      severity: critical\n  - alert: NodeExporterDown\n\
      \    annotations:\n      message: NodeExporter has disappeared from Prometheus\
      \ target discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeexporterdown\n\
      \    expr: |\n      absent(up{job=\"k8s-pgmon/kubernetes-node-exporter\"} ==\
      \ 1)\n    for: 15m\n    labels:\n      severity: critical\n  - alert: PrometheusDown\n\
      \    annotations:\n      message: Prometheus has disappeared from Prometheus\
      \ target discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusdown\n\
      \    expr: |\n      absent(up{job=\"k8s-pgmon/prometheus\"} == 1)\n    for:\
      \ 15m\n    labels:\n      severity: critical\n  - alert: PrometheusOperatorDown\n\
      \    annotations:\n      message: PrometheusOperator has disappeared from Prometheus\
      \ target discovery.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusoperatordown\n\
      \    expr: |\n      absent(up{instance=\"k8s-pgmon-alertmanager:9093\"} == 1)\n\
      \    for: 15m\n    labels:\n      severity: critical\n- name: kubernetes-apps\n\
      \  rules:\n  - alert: KubePodCrashLooping\n    annotations:\n      message:\
      \ Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container\n    \
      \    }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.\n   \
      \   runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping\n\
      \    expr: |\n      rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"\
      }[15m]) * 60 * 5 > 0\n    for: 1h\n    labels:\n      severity: critical\n \
      \ - alert: KubePodNotReady\n    annotations:\n      message: Pod {{ $labels.namespace\
      \ }}/{{ $labels.pod }} has been in a non-ready\n        state for longer than\
      \ an hour.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready\n\
      \    expr: |\n      sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\"\
      , phase=~\"Pending|Unknown\"}) > 0\n    for: 1h\n    labels:\n      severity:\
      \ critical\n  - alert: KubeDeploymentGenerationMismatch\n    annotations:\n\
      \      message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment\n\
      \        }} does not match, this indicates that the Deployment has failed but\
      \ has not\n        been rolled back.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch\n\
      \    expr: |\n      kube_deployment_status_observed_generation{job=\"kube-state-metrics\"\
      }\n        !=\n      kube_deployment_metadata_generation{job=\"kube-state-metrics\"\
      }\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeDeploymentReplicasMismatch\n\
      \    annotations:\n      message: Deployment {{ $labels.namespace }}/{{ $labels.deployment\
      \ }} has not\n        matched the expected number of replicas for longer than\
      \ an hour.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch\n\
      \    expr: |\n      kube_deployment_spec_replicas{job=\"kube-state-metrics\"\
      }\n        !=\n      kube_deployment_status_replicas_available{job=\"kube-state-metrics\"\
      }\n    for: 1h\n    labels:\n      severity: critical\n  - alert: KubeStatefulSetReplicasMismatch\n\
      \    annotations:\n      message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset\
      \ }} has not\n        matched the expected number of replicas for longer than\
      \ 15 minutes.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch\n\
      \    expr: |\n      kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"\
      }\n        !=\n      kube_statefulset_status_replicas{job=\"kube-state-metrics\"\
      }\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStatefulSetGenerationMismatch\n\
      \    annotations:\n      message: StatefulSet generation for {{ $labels.namespace\
      \ }}/{{ $labels.statefulset\n        }} does not match, this indicates that\
      \ the StatefulSet has failed but has\n        not been rolled back.\n      runbook_url:\
      \ https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch\n\
      \    expr: |\n      kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"\
      }\n        !=\n      kube_statefulset_metadata_generation{job=\"kube-state-metrics\"\
      }\n    for: 15m\n    labels:\n      severity: critical\n  - alert: KubeStatefulSetUpdateNotRolledOut\n\
      \    annotations:\n      message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset\
      \ }} update\n        has not been rolled out.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout\n\
      \    expr: |\n      max without (revision) (\n        kube_statefulset_status_current_revision{job=\"\
      kube-state-metrics\"}\n          unless\n        kube_statefulset_status_update_revision{job=\"\
      kube-state-metrics\"}\n      )\n        *\n      (\n        kube_statefulset_replicas{job=\"\
      kube-state-metrics\"}\n          !=\n        kube_statefulset_status_replicas_updated{job=\"\
      kube-state-metrics\"}\n      )\n    for: 15m\n    labels:\n      severity: critical\n\
      \  - alert: KubeDaemonSetRolloutStuck\n    annotations:\n      message: Only\
      \ {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace\n    \
      \    }}/{{ $labels.daemonset }} are scheduled and ready.\n      runbook_url:\
      \ https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck\n\
      \    expr: |\n      kube_daemonset_status_number_ready{job=\"kube-state-metrics\"\
      }\n        /\n      kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"\
      } * 100 < 100\n    for: 15m\n    labels:\n      severity: critical\n  - alert:\
      \ KubeDaemonSetNotScheduled\n    annotations:\n      message: '{{ $value }}\
      \ Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset\n        }}\
      \ are not scheduled.'\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled\n\
      \    expr: |\n      kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"\
      }\n        -\n      kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"\
      } > 0\n    for: 10m\n    labels:\n      severity: warning\n  - alert: KubeDaemonSetMisScheduled\n\
      \    annotations:\n      message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace\
      \ }}/{{ $labels.daemonset\n        }} are running where they are not supposed\
      \ to run.'\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled\n\
      \    expr: |\n      kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"\
      } > 0\n    for: 10m\n    labels:\n      severity: warning\n  - alert: KubeCronJobRunning\n\
      \    annotations:\n      message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob\
      \ }} is taking more\n        than 1h to complete.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning\n\
      \    expr: |\n      time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\"\
      } > 3600\n    for: 1h\n    labels:\n      severity: warning\n  - alert: KubeJobCompletion\n\
      \    annotations:\n      message: Job {{ $labels.namespace }}/{{ $labels.job_name\
      \ }} is taking more than\n        one hour to complete.\n      runbook_url:\
      \ https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion\n\
      \    expr: |\n      kube_job_spec_completions{job=\"kube-state-metrics\"} -\
      \ kube_job_status_succeeded{job=\"kube-state-metrics\"}  > 0\n    for: 1h\n\
      \    labels:\n      severity: warning\n  - alert: KubeJobFailed\n    annotations:\n\
      \      message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to\
      \ complete.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed\n\
      \    expr: |\n      kube_job_status_failed{job=\"kube-state-metrics\"}  > 0\n\
      \    for: 1h\n    labels:\n      severity: warning\n- name: kubernetes-resources\n\
      \  rules:\n  - alert: KubeCPUOvercommit\n    annotations:\n      message: Cluster\
      \ has overcommitted CPU resource requests for Pods and cannot\n        tolerate\
      \ node failure.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit\n\
      \    expr: |\n      sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n\
      \        /\n      sum(node:node_num_cpu:sum)\n        >\n      (count(node:node_num_cpu:sum)-1)\
      \ / count(node:node_num_cpu:sum)\n    for: 5m\n    labels:\n      severity:\
      \ warning\n  - alert: KubeMemOvercommit\n    annotations:\n      message: Cluster\
      \ has overcommitted memory resource requests for Pods and cannot\n        tolerate\
      \ node failure.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit\n\
      \    expr: |\n      sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n\
      \        /\n      sum(node_memory_MemTotal_bytes)\n        >\n      (count(node:node_num_cpu:sum)-1)\n\
      \        /\n      count(node:node_num_cpu:sum)\n    for: 5m\n    labels:\n \
      \     severity: warning\n  - alert: KubeCPUOvercommit\n    annotations:\n  \
      \    message: Cluster has overcommitted CPU resource requests for Namespaces.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit\n\
      \    expr: |\n      sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"\
      hard\", resource=\"requests.cpu\"})\n        /\n      sum(node:node_num_cpu:sum)\n\
      \        > 1.5\n    for: 5m\n    labels:\n      severity: warning\n  - alert:\
      \ KubeMemOvercommit\n    annotations:\n      message: Cluster has overcommitted\
      \ memory resource requests for Namespaces.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit\n\
      \    expr: |\n      sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"\
      hard\", resource=\"requests.memory\"})\n        /\n      sum(node_memory_MemTotal_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes\"})\n        > 1.5\n    for: 5m\n    labels:\n  \
      \    severity: warning\n  - alert: KubeQuotaExceeded\n    annotations:\n   \
      \   message: Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\"\
      \ $value\n        }}% of its {{ $labels.resource }} quota.\n      runbook_url:\
      \ https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded\n\
      \    expr: |\n      100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"\
      used\"}\n        / ignoring(instance, job, type)\n      (kube_resourcequota{job=\"\
      kube-state-metrics\", type=\"hard\"} > 0)\n        > 90\n    for: 15m\n    labels:\n\
      \      severity: warning\n  - alert: CPUThrottlingHigh\n    annotations:\n \
      \     message: '{{ printf \"%0.0f\" $value }}% throttling of CPU in namespace\
      \ {{ $labels.namespace\n        }} for container {{ $labels.container_name }}\
      \ in pod {{ $labels.pod_name }}.'\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh\n\
      \    expr: \"100 * sum(increase(container_cpu_cfs_throttled_periods_total{}[5m]))\
      \ by\n      (container_name, pod_name, namespace) \\n  / \\nsum(increase(container_cpu_cfs_periods_total{}[5m]))\n\
      \      by (container_name, pod_name, namespace)\\n  > 25 \\n\"\n    for: 15m\n\
      \    labels:\n      severity: warning\n- name: kubernetes-storage\n  rules:\n\
      \  - alert: KubePersistentVolumeUsageCritical\n    annotations:\n      message:\
      \ The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }}\n    \
      \    in Namespace {{ $labels.namespace }} is only {{ printf \"%0.2f\" $value\
      \ }}%\n        free.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical\n\
      \    expr: |\n      100 * kubelet_volume_stats_available_bytes{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"\
      }\n        /\n      kubelet_volume_stats_capacity_bytes{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"\
      }\n        < 3\n    for: 1m\n    labels:\n      severity: critical\n  - alert:\
      \ KubePersistentVolumeFullInFourDays\n    annotations:\n      message: Based\
      \ on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim\n\
      \        }} in Namespace {{ $labels.namespace }} is expected to fill up within\
      \ four\n        days. Currently {{ printf \"%0.2f\" $value }}% is available.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays\n\
      \    expr: |\n      100 * (\n        kubelet_volume_stats_available_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\"}\n          /\n        kubelet_volume_stats_capacity_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\"}\n      ) < 15\n      and\n      predict_linear(kubelet_volume_stats_available_bytes{job=\"\
      k8s-pgmon/kubernetes-nodes-kubelet\"}[6h], 4 * 24 * 3600) < 0\n    for: 5m\n\
      \    labels:\n      severity: critical\n  - alert: KubePersistentVolumeErrors\n\
      \    annotations:\n      message: The persistent volume {{ $labels.persistentvolume\
      \ }} has status {{\n        $labels.phase }}.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors\n\
      \    expr: |\n      kube_persistentvolume_status_phase{phase=~\"Failed|Pending\"\
      ,job=\"kube-state-metrics\"} > 0\n    for: 5m\n    labels:\n      severity:\
      \ critical\n- name: kubernetes-system\n  rules:\n  - alert: KubeNodeNotReady\n\
      \    annotations:\n      message: '{{ $labels.node }} has been unready for more\
      \ than an hour.'\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready\n\
      \    expr: |\n      kube_node_status_condition{job=\"kube-state-metrics\",condition=\"\
      Ready\",status=\"true\"} == 0\n    for: 1h\n    labels:\n      severity: warning\n\
      \  - alert: KubeVersionMismatch\n    annotations:\n      message: There are\
      \ {{ $value }} different versions of Kubernetes components\n        running.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch\n\
      \    expr: |\n      count(count(kubernetes_build_info{job!=\"kube-dns\"}) by\
      \ (gitVersion)) > 1\n    for: 1h\n    labels:\n      severity: warning\n  -\
      \ alert: KubeClientErrors\n    annotations:\n      message: Kubernetes API server\
      \ client '{{ $labels.job }}/{{ $labels.instance\n        }}' is experiencing\
      \ {{ printf \"%0.0f\" $value }}% errors.'\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors\n\
      \    expr: |\n      (sum(rate(rest_client_requests_total{code!~\"2..|404\"}[5m]))\
      \ by (instance, job)\n        /\n      sum(rate(rest_client_requests_total[5m]))\
      \ by (instance, job))\n      * 100 > 1\n    for: 15m\n    labels:\n      severity:\
      \ warning\n  - alert: KubeClientErrors\n    annotations:\n      message: Kubernetes\
      \ API server client '{{ $labels.job }}/{{ $labels.instance\n        }}' is experiencing\
      \ {{ printf \"%0.0f\" $value }} errors / second.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors\n\
      \    expr: |\n      sum(rate(ksm_scrape_error_total{job=\"kube-state-metrics\"\
      }[5m])) by (instance, job) > 0.1\n    for: 15m\n    labels:\n      severity:\
      \ warning\n  - alert: KubeletTooManyPods\n    annotations:\n      message: k8s-pgmon/kubernetes-nodes-kubelet\
      \ {{ $labels.instance }} is running {{ $value }} Pods, close\n        to the\
      \ limit of 110.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods\n\
      \    expr: |\n      kubelet_running_pod_count{job=\"k8s-pgmon/kubernetes-nodes-kubelet\"\
      } > 110 * 0.9\n    for: 15m\n    labels:\n      severity: warning\n  - alert:\
      \ KubeAPILatencyHigh\n    annotations:\n      message: The API server has a\
      \ 99th percentile latency of {{ $value }} seconds\n        for {{ $labels.verb\
      \ }} {{ $labels.resource }}.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh\n\
      \    expr: |\n      cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"\
      apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"\
      } > 1\n    for: 10m\n    labels:\n      severity: warning\n  - alert: KubeAPILatencyHigh\n\
      \    annotations:\n      message: The API server has a 99th percentile latency\
      \ of {{ $value }} seconds\n        for {{ $labels.verb }} {{ $labels.resource\
      \ }}.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh\n\
      \    expr: |\n      cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"\
      apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"\
      } > 4\n    for: 10m\n    labels:\n      severity: critical\n  - alert: KubeAPIErrorsHigh\n\
      \    annotations:\n      message: API server is returning errors for {{ $value\
      \ }}% of requests.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh\n\
      \    expr: |\n      sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"\
      ^(?:5..)$\"}[5m])) without(instance, pod)\n        /\n      sum(rate(apiserver_request_count{job=\"\
      apiserver\"}[5m])) without(instance, pod) * 100 > 10\n    for: 10m\n    labels:\n\
      \      severity: critical\n  - alert: KubeAPIErrorsHigh\n    annotations:\n\
      \      message: API server is returning errors for {{ $value }}% of requests.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh\n\
      \    expr: |\n      sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"\
      ^(?:5..)$\"}[5m])) without(instance, pod)\n        /\n      sum(rate(apiserver_request_count{job=\"\
      apiserver\"}[5m])) without(instance, pod) * 100 > 5\n    for: 10m\n    labels:\n\
      \      severity: warning\n  - alert: KubeClientCertificateExpiration\n    annotations:\n\
      \      message: Kubernetes API certificate is expiring in less than 7 days.\n\
      \      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration\n\
      \    expr: |\n      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"\
      apiserver\"}[5m]))) < 604800\n    labels:\n      severity: warning\n  - alert:\
      \ KubeClientCertificateExpiration\n    annotations:\n      message: Kubernetes\
      \ API certificate is expiring in less than 24 hours.\n      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration\n\
      \    expr: |\n      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"\
      apiserver\"}[5m]))) < 86400\n    labels:\n      severity: critical\n- name:\
      \ alertmanager.rules\n  rules:\n  - alert: AlertmanagerConfigInconsistent\n\
      \    annotations:\n      message: The configuration of the instances of the\
      \ Alertmanager cluster `{{$labels.service}}`\n        are out of sync.\n   \
      \ expr: |\n      count_values(\"config_hash\", alertmanager_config_hash{job=\"\
      k8s-pgmon/alertmanager\"}) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_spec_replicas{job=\"\
      prometheus-operator\",controller=\"alertmanager\"}, \"service\", \"alertmanager-$1\"\
      , \"name\", \"(.*)\") != 1\n    for: 5m\n    labels:\n      severity: critical\n\
      \  - alert: AlertmanagerFailedReload\n    annotations:\n      message: Reloading\
      \ Alertmanager's configuration has failed for {{ $labels.namespace\n       \
      \ }}/{{ $labels.pod}}.\n    expr: |\n      alertmanager_config_last_reload_successful{job=\"\
      k8s-pgmon/alertmanager\"} == 0\n    for: 10m\n    labels:\n      severity: warning\n\
      \  - alert: AlertmanagerMembersInconsistent\n    annotations:\n      message:\
      \ Alertmanager has not found all other members of the cluster.\n    expr: |\n\
      \      alertmanager_cluster_members{job=\"k8s-pgmon/alertmanager\"}\n      \
      \  != on (service) GROUP_LEFT()\n      count by (service) (alertmanager_cluster_members{job=\"\
      k8s-pgmon/alertmanager\"})\n    for: 5m\n    labels:\n      severity: critical\n\
      - name: general.rules\n  rules:\n  - alert: TargetDown\n    annotations:\n \
      \     message: '{{ $value }}% of the {{ $labels.job }} targets are down.'\n\
      \    expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10\n    for:\
      \ 10m\n    labels:\n      severity: warning\n- name: kube-prometheus-node-alerting.rules\n\
      \  rules:\n  - alert: NodeDiskRunningFull\n    annotations:\n      message:\
      \ Device {{ $labels.device }} of k8s-pgmon/kubernetes-nodes {{ $labels.namespace\
      \ }}/{{\n        $labels.pod }} will be full within the next 24 hours.\n   \
      \ expr: |\n      (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h],\
      \ 3600 * 24) < 0)\n    for: 30m\n    labels:\n      severity: warning\n  - alert:\
      \ NodeDiskRunningFull\n    annotations:\n      message: Device {{ $labels.device\
      \ }} of k8s-pgmon/kubernetes-nodes {{ $labels.namespace }}/{{\n        $labels.pod\
      \ }} will be full within the next 2 hours.\n    expr: |\n      (node:node_filesystem_usage:\
      \ > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) <\
      \ 0)\n    for: 10m\n    labels:\n      severity: critical\n- name: prometheus.rules\n\
      \  rules:\n  - alert: PrometheusConfigReloadFailed\n    annotations:\n     \
      \ description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}\n\
      \      summary: Reloading Prometheus' configuration failed\n    expr: |\n  \
      \    prometheus_config_last_reload_successful{job=\"k8s-pgmon/prometheus\"}\
      \ == 0\n    for: 10m\n    labels:\n      severity: warning\n  - alert: PrometheusNotificationQueueRunningFull\n\
      \    annotations:\n      description: Prometheus' alert notification queue is\
      \ running full for {{$labels.namespace}}/{{\n        $labels.pod}}\n      summary:\
      \ Prometheus' alert notification queue is running full\n    expr: |\n      predict_linear(prometheus_notifications_queue_length{job=\"\
      k8s-pgmon/prometheus\"}[5m], 60 * 30) > prometheus_notifications_queue_capacity{job=\"\
      k8s-pgmon/prometheus\"}\n    for: 10m\n    labels:\n      severity: warning\n\
      \  - alert: PrometheusErrorSendingAlerts\n    annotations:\n      description:\
      \ Errors while sending alerts from Prometheus {{$labels.namespace}}/{{\n   \
      \     $labels.pod}} to Alertmanager {{$labels.Alertmanager}}\n      summary:\
      \ Errors while sending alert from Prometheus\n    expr: |\n      rate(prometheus_notifications_errors_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) > 0.01\n    for: 10m\n    labels:\n      severity:\
      \ warning\n  - alert: PrometheusErrorSendingAlerts\n    annotations:\n     \
      \ description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{\n\
      \        $labels.pod}} to Alertmanager {{$labels.Alertmanager}}\n      summary:\
      \ Errors while sending alerts from Prometheus\n    expr: |\n      rate(prometheus_notifications_errors_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) > 0.03\n    for: 10m\n    labels:\n      severity:\
      \ critical\n  - alert: PrometheusNotConnectedToAlertmanagers\n    annotations:\n\
      \      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not\
      \ connected\n        to any Alertmanagers\n      summary: Prometheus is not\
      \ connected to any Alertmanagers\n    expr: |\n      prometheus_notifications_alertmanagers_discovered{job=\"\
      k8s-pgmon/prometheus\"} < 1\n    for: 10m\n    labels:\n      severity: warning\n\
      \  - alert: PrometheusTSDBReloadsFailing\n    annotations:\n      description:\
      \ '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}\n     \
      \   reload failures over the last four hours.'\n      summary: Prometheus has\
      \ issues reloading data blocks from disk\n    expr: |\n      increase(prometheus_tsdb_reloads_failures_total{job=\"\
      k8s-pgmon/prometheus\"}[2h]) > 0\n    for: 12h\n    labels:\n      severity:\
      \ warning\n  - alert: PrometheusTSDBCompactionsFailing\n    annotations:\n \
      \     description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}\n\
      \        compaction failures over the last four hours.'\n      summary: Prometheus\
      \ has issues compacting sample blocks\n    expr: |\n      increase(prometheus_tsdb_compactions_failed_total{job=\"\
      k8s-pgmon/prometheus\"}[2h]) > 0\n    for: 12h\n    labels:\n      severity:\
      \ warning\n  - alert: PrometheusTSDBWALCorruptions\n    annotations:\n     \
      \ description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead\n\
      \        log (WAL).'\n      summary: Prometheus write-ahead log is corrupted\n\
      \    expr: |\n      tsdb_wal_corruptions_total{job=\"k8s-pgmon/prometheus\"\
      } > 0\n    for: 4h\n    labels:\n      severity: warning\n  - alert: PrometheusNotIngestingSamples\n\
      \    annotations:\n      description: Prometheus {{ $labels.namespace }}/{{\
      \ $labels.pod}} isn't ingesting\n        samples.\n      summary: Prometheus\
      \ isn't ingesting samples\n    expr: |\n      rate(prometheus_tsdb_head_samples_appended_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) <= 0\n    for: 10m\n    labels:\n      severity:\
      \ warning\n  - alert: PrometheusTargetScrapesDuplicate\n    annotations:\n \
      \     description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected\n\
      \        due to duplicate timestamps but different values'\n      summary: Prometheus\
      \ has many samples rejected\n    expr: |\n      increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"\
      k8s-pgmon/prometheus\"}[5m]) > 0\n    for: 10m\n    labels:\n      severity:\
      \ warning"
  kind: ConfigMap
  metadata:
    name: k8s-pgmon-rules
    namespace: pgmon
kind: ConfigMapList
