apiVersion: v1
kind: ConfigMap
metadata:
  name: workflow-controller-configmap
data:
  config: "# instanceID is a label selector to limit the controller's watch to a specific\
    \ instance. It\n# contains an arbitrary value that is carried forward into its\
    \ pod labels, under the key\n# workflows.argoproj.io/controller-instanceid, for\
    \ the purposes of workflow segregation. This\n# enables a controller to only receive\
    \ workflow and pod events that it is interested about,\n# in order to support\
    \ multiple controllers in a single cluster, and ultimately allows the\n# controller\
    \ itself to be bundled as part of a higher level application. If omitted, the\n\
    # controller watches workflows and pods that *are not* labeled with an instance\
    \ id.\ninstanceID: my-ci-controller\n\n# Parallelism limits the max total parallel\
    \ workflows that can execute at the same time\n# (available since Argo v2.3)\n\
    parallelism: 10\n\n# Whether or not to emit events on node completion. These can\
    \ take a up a lot of space in\n# k8s (typically etcd) resulting in errors when\
    \ trying to create new events:\n# \"Unable to create audit event: etcdserver:\
    \ mvcc: database space exceeded\"\n# This config item allows you to disable this.\n\
    # (since v2.9)\nnodeEvents:\n  enabled: true\n\n# uncomment flowing lines if workflow\
    \ controller runs in a different k8s cluster with the \n# workflow workloads,\
    \ or needs to communicate with the k8s apiserver using an out-of-cluster\n# kubeconfig\
    \ secret\n# kubeConfig:\n#   # name of the kubeconfig secret, may not be empty\
    \ when kubeConfig specified\n#   secretName: kubeconfig-secret\n#   # key of the\
    \ kubeconfig secret, may not be empty when kubeConfig specified\n#   secretKey:\
    \ kubeconfig\n#   # mounting path of the kubeconfig secret, default to /kube/config\n\
    #   mountPath: /kubeconfig/mount/path\n#   # volume name when mounting the secret,\
    \ default to kubeconfig\n#   volumeName: kube-config-volume\n\nlinks:\n  - name:\
    \ Example Workflow Link\n    scope: workflow\n    url: http://logging-facility?namespace=${metadata.namespace}&workflowName=${metadata.name}\n\
    \  - name: Example Pod Link\n    scope: pod\n    url: http://logging-facility?namespace=${metadata.namespace}&podName=${metadata.name}\n\
    \n# artifactRepository defines the default location to be used as the artifact\
    \ repository for\n# container artifacts.\nartifactRepository:\n  # archiveLogs\
    \ will archive the main container logs as an artifact\n  archiveLogs: true\n\n\
    \  s3:\n    # Use the corresponding endpoint depending on your S3 provider:\n\
    \    #   AWS: s3.amazonaws.com\n    #   GCS: storage.googleapis.com\n    #   Minio:\
    \ my-minio-endpoint.default:9000\n    endpoint: s3.amazonaws.com\n    bucket:\
    \ my-bucket\n    region: us-west-2\n    # insecure will disable TLS. Primarily\
    \ used for minio installs not configured with TLS\n    insecure: false\n    #\
    \ keyFormat is a format pattern to define how artifacts will be organized in a\
    \ bucket.\n    # It can reference workflow metadata variables such as workflow.namespace,\
    \ workflow.name,\n    # pod.name. Can also use strftime formating of workflow.creationTimestamp\
    \ so that workflow\n    # artifacts can be organized by date. If omitted, will\
    \ use `{{workflow.name}}/{{pod.name}}`,\n    # which has potential for have collisions.\n\
    \    # The following example pattern organizes workflow artifacts under a \"my-artifacts\"\
    \ sub dir,\n    # then sub dirs for year, month, date and finally workflow name\
    \ and pod.\n    # e.g.: my-artifacts/2018/08/23/my-workflow-abc123/my-workflow-abc123-1234567890\n\
    \    keyFormat: \"my-artifacts\\\n      /{{workflow.creationTimestamp.Y}}\\\n\
    \      /{{workflow.creationTimestamp.m}}\\\n      /{{workflow.creationTimestamp.d}}\\\
    \n      /{{workflow.name}}\\\n      /{{pod.name}}\"\n    # The actual secret object\
    \ (in this example my-s3-credentials), should be created in every\n    # namespace\
    \ where a workflow needs to store its artifacts to S3. If omitted,\n    # attempts\
    \ to use IAM role to access the bucket (instead of accessKey/secretKey).\n   \
    \ accessKeySecret:\n      name: my-s3-credentials\n      key: accessKey\n    secretKeySecret:\n\
    \      name: my-s3-credentials\n      key: secretKey\n\n# Specifies the container\
    \ runtime interface to use (default: docker)\n# must be one of: docker, kubelet,\
    \ k8sapi, pns\ncontainerRuntimeExecutor: docker\n\n# Specifies the location of\
    \ docker.sock on the host for docker executor (default: /var/run/docker.sock)\n\
    # (available since Argo v2.4)\ndockerSockPath: /var/someplace/else/docker.sock\n\
    \n# kubelet port when using kubelet executor (default: 10250)\nkubeletPort: 10250\n\
    \n# disable the TLS verification of the kubelet executor (default: false)\nkubeletInsecure:\
    \ false\n\n# executor controls how the init and wait container should be customized\n\
    # (available since Argo v2.3)\nexecutor:\n  imagePullPolicy: IfNotPresent\n  resources:\n\
    \    requests:\n      cpu: 0.1\n      memory: 64Mi\n    limits:\n      cpu: 0.5\n\
    \      memory: 512Mi\n  # args & env allows command line arguments and environment\
    \ variables to be appended to the\n  # executor container and is mainly used for\
    \ development/debugging purposes.\n  args:\n  - --loglevel\n  - debug\n  - --gloglevel\n\
    \  - \"6\"\n  env:\n  # ARGO_TRACE enables some tracing information for debugging\
    \ purposes. Currently it enables\n  # logging of S3 request/response payloads\
    \ (including auth headers)\n  - name: ARGO_TRACE\n    value: \"1\"\n\n# metricsConfig\
    \ controls the path and port for prometheus metrics. Metrics are enabled and emitted\
    \ on localhost:9090/metrics\n# by default.\nmetricsConfig:\n  # Enabled controls\
    \ metric emission. Default is true, set \"enabled: false\" to turn off\n  enabled:\
    \ true\n  # Path is the path where metrics are emitted. Must start with a \"/\"\
    . Default is \"/metrics\"\n  path: /metrics\n  # Port is the port where metrics\
    \ are emitted. Default is \"9090\"\n  port: 8080\n  # MetricsTTL sets how often\
    \ custom metrics are cleared from memory. Default is \"0\", metrics are never\
    \ cleared\n  metricsTTL: \"10m\"\n  # IgnoreErrors is a flag that instructs prometheus\
    \ to ignore metric emission errors. Default is \"false\"\n  ignoreErrors: false\n\
    \n  # DEPRECATED: Legacy metrics are now removed, this field is ignored\n  disableLegacy:\
    \ false\n\n# telemetryConfig controls the path and port for prometheus telemetry.\
    \ Telemetry is enabled and emitted in the same endpoint\n# as metrics by default,\
    \ but can be overridden using this config.\ntelemetryConfig:\n  enabled: true\n\
    \  path: /telemetry\n  port: 8080\n\n# enable persistence using postgres\npersistence:\n\
    \  connectionPool:\n    maxIdleConns: 100\n    maxOpenConns: 0\n    connMaxLifetime:\
    \ 0s # 0 means connections don't have a max lifetime\n  #  if true node status\
    \ is only saved to the persistence DB to avoid the 1MB limit in etcd\n  nodeStatusOffLoad:\
    \ false\n  # save completed workloads to the workflow archive\n  archive: false\n\
    \  # the number of days to keep archived workflows (the default is forever)\n\
    \  archiveTTL: 180d\n\n  # LabelSelector determines the workflow that matches\
    \ with the matchlabels or matchrequirements, will be archived.\n  # https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\n\
    \  archiveLabelSelector:\n    matchLabels:\n      workflows.argoproj.io/archive-strategy:\
    \ \"always\"\n\n  # Optional name of the cluster I'm running in. This must be\
    \ unique for your cluster.\n  clusterName: default\n  postgresql:\n    host: localhost\n\
    \    port: 5432\n    database: postgres\n    tableName: argo_workflows\n    #\
    \ the database secrets must be in the same namespace of the controller\n    userNameSecret:\n\
    \      name: argo-postgres-config\n      key: username\n    passwordSecret:\n\
    \      name: argo-postgres-config\n      key: password\n\n  # Optional config\
    \ for mysql:\n  # mysql:\n  #   host: localhost\n  #   port: 3306\n  #   database:\
    \ argo\n  #   tableName: argo_workflows\n  #   userNameSecret:\n  #     name:\
    \ argo-mysql-config\n  #     key: username\n  #   passwordSecret:\n  #     name:\
    \ argo-mysql-config\n  #     key: password\n\n# Default values that will apply\
    \ to all Workflows from this controller, unless overridden on the Workflow-level\n\
    # See more: docs/default-workflow-specs.yaml\nworkflowDefaults:\n  metadata:\n\
    \    annotations:\n      argo: workflows\n    labels:\n      foo: bar\n  spec:\n\
    \    ttlStrategy:\n      secondsAfterSuccess: 5\n    parallelism: 3\n\n# SSO Configuration\
    \ for the Argo server.\nsso:\n  # This is the root URL of the OIDC provider (required).\n\
    \  issuer: https://issuer.root.url/\n  # This is name of the secret and the key\
    \ in it that contain OIDC client\n  # ID issued to the application by the provider\
    \ (required).\n  clientId:\n    name: client-id-secret\n    key: client-id-key\n\
    \  # This is name of the secret and the key in it that contain OIDC client\n \
    \ # secret issued to the application by the provider (required).\n  clientSecret:\n\
    \    name: client-secret-secret\n    key: client-secret-key\n  # This is the redirect\
    \ URL supplied to the provider (required). It must\n  # be in the form <argo-server-root-url>/oauth2/callback.\
    \ It must be\n  # browser-accessible.\n  redirectUrl: https://argo-server/oauth2/callback\n\
    \n# workflowRequirements restricts the Workflows that the controller will process.\n\
    # Current options:\n#   referenceOnly: Only Workflows using \"workflowTemplateRef\"\
    \ will be processed. This allows the administrator of the controller\n#     to\
    \ set a \"library\" of templates that may be run by its opeartor, limiting arbitrary\
    \ Workflow execution.\n#   strictReferenceOnly: Only Workflows using \"workflowTemplateRef\"\
    \ will be processed and the controller will enforce\n#     that the WorkflowTemplate\
    \ that is referenced hasn't changed between operations. If you want to make sure\
    \ the operator of the\n#     Workflow cannot run an arbitrary Workflow, use this\
    \ option.\nworkflowRequirements:\n  referenceOnly: true"
