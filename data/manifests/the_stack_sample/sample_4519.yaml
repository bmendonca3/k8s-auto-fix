apiVersion: v1
kind: Pod
metadata:
  name: fluentd-cloud-logging
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: ''
spec:
  dnsPolicy: Default
  containers:
  - name: fluentd-cloud-logging
    image: gcr.io/google_containers/fluentd-gcp:2.0.1
    command:
    - /bin/sh
    - -c
    - "mkdir /etc/fluent/config.d &&\n  echo \"$FLUENTD_CONFIG\" > /etc/fluent/config.d/main.conf\
      \ &&\n  /run.sh $FLUENTD_ARGS 2>&1 >>/var/log/fluentd.log"
    env:
    - name: FLUENTD_ARGS
      value: --no-supervisor
    - name: FLUENTD_CONFIG
      value: "# This configuration file for Fluentd is used\n# to watch changes to\
        \ Docker log files that live in the\n# directory /var/lib/docker/containers/\
        \ and are symbolically\n# linked to from the /var/log/containers directory\
        \ using names that capture the\n# pod name and container name. These logs\
        \ are then submitted to\n# Google Cloud Logging which assumes the installation\
        \ of the cloud-logging plug-in.\n#\n# Example\n# =======\n# A line in the\
        \ Docker log file might look like this JSON:\n#\n# {\"log\":\"2014/09/25 21:15:03\
        \ Got request with path wombat\\\\n\",\n#  \"stream\":\"stderr\",\n#   \"\
        time\":\"2014-09-25T21:15:03.499185026Z\"}\n#\n# The record reformer is used\
        \ to write the tag to focus on the pod name\n# and the Kubernetes container\
        \ name. For example a Docker container's logs\n# might be in the directory:\n\
        #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\n\
        # and in the file:\n#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n\
        # where 997599971ee6... is the Docker ID of the running container.\n# The\
        \ Kubernetes kubelet makes a symbolic link to this file on the host machine\n\
        # in the /var/log/containers directory which includes the pod name and the\
        \ Kubernetes\n# container name:\n#    synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n\
        #    ->\n#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n\
        # The /var/log directory on the host is mapped to the /var/log directory in\
        \ the container\n# running this instance of Fluentd and we end up collecting\
        \ the file:\n#   /var/log/containers/synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n\
        # This results in the tag:\n#  var.log.containers.synthetic-logger-0.25lps-pod_default-synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n\
        # The record reformer is used is discard the var.log.containers prefix and\n\
        # the Docker container ID suffix and \"kubernetes.\" is pre-pended giving\
        \ the tag:\n#   kubernetes.synthetic-logger-0.25lps-pod_default-synth-lgr\n\
        # Tag is then parsed by google_cloud plugin and translated to the metadata,\n\
        # visible in the log viewer\n\n# Example:\n# {\"log\":\"[info:2016-02-16T16:04:05.930-08:00]\
        \ Some log text here\\\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"\
        }\n<source>\n  type tail\n  format json\n  time_key time\n  path /var/log/containers/*.log\n\
        \  pos_file /var/log/gcp-containers.log.pos\n  time_format %Y-%m-%dT%H:%M:%S.%N%Z\n\
        \  tag reform.*\n  read_from_head true\n</source>\n\n<filter reform.**>\n\
        \  type parser\n  format /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\
        \\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<log>.*)/\n  reserve_data\
        \ true\n  suppress_parse_error_log true\n  key_name log\n</filter>\n\n<match\
        \ reform.**>\n  type record_reformer\n  enable_ruby true\n  tag raw.kubernetes.${tag_suffix[4].split('-')[0..-2].join('-')}\n\
        </match>\n\n# Detect exceptions in the log output and forward them as one\
        \ log entry.\n<match raw.kubernetes.**>\n  @type copy\n\n  <store>\n    @type\
        \ prometheus\n\n    <metric>\n      type counter\n      name logging_line_count\n\
        \      desc Total number of lines generated by application containers\n  \
        \    <labels>\n        tag ${tag}\n      </labels>\n    </metric>\n  </store>\n\
        \  <store>\n    @type detect_exceptions\n\n    remove_tag_prefix raw\n   \
        \ message log\n    stream stream\n    multiline_flush_interval 5\n    max_bytes\
        \ 500000\n    max_lines 1000\n  </store>\n</match>\n\n# Example:\n# 2015-12-21\
        \ 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward]\
        \ at time 23:17:22.066081\n<source>\n  type tail\n  format /^(?<time>[^ ]*\
        \ [^ ,]*)[^\\\\[]*\\\\[[^\\\\]]*\\\\]\\\\[(?<severity>[^ \\\\]]*) *\\\\] (?<message>.*)$/\n\
        \  time_format %Y-%m-%d %H:%M:%S\n  path /var/log/salt/minion\n  pos_file\
        \ /var/log/gcp-salt.pos\n  tag salt\n</source>\n\n# Example:\n# Dec 21 23:17:22\
        \ gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script\
        \ /var/run/google.startup.script\n<source>\n  type tail\n  format syslog\n\
        \  path /var/log/startupscript.log\n  pos_file /var/log/gcp-startupscript.log.pos\n\
        \  tag startupscript\n</source>\n\n# Examples:\n# time=\"2016-02-04T06:51:03.053580605Z\"\
        \ level=info msg=\"GET /containers/json\"\n# time=\"2016-02-04T07:53:57.505612354Z\"\
        \ level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\n\
        <source>\n  type tail\n  format /^time=\"(?<time>[^)]*)\" level=(?<severity>[^\
        \ ]*) msg=\"(?<message>[^\"]*)\"( err=\"(?<error>[^\"]*)\")?( statusCode=($<status_code>\\\
        \\d+))?/\n  path /var/log/docker.log\n  pos_file /var/log/gcp-docker.log.pos\n\
        \  tag docker\n</source>\n\n# Example:\n# 2016/02/04 06:52:38 filePurge: successfully\
        \ removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\n\
        <source>\n  type tail\n  # Not parsing this, because it doesn't have anything\
        \ particularly useful to\n  # parse out of it (like severities).\n  format\
        \ none\n  path /var/log/etcd.log\n  pos_file /var/log/gcp-etcd.log.pos\n \
        \ tag etcd\n</source>\n\n# Multi-line parsing is required for all the kube\
        \ logs because very large log\n# statements, such as those that include entire\
        \ object bodies, get split into\n# multiple lines by glog.\n\n# Example:\n\
        # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms)\
        \ 200 [[Go-http-client/1.1] 10.244.1.3:40537]\n<source>\n  type tail\n  format\
        \ multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\\\w\\\\\
        d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\
        \\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n  time_format %m%d\
        \ %H:%M:%S.%N\n  path /var/log/kubelet.log\n  pos_file /var/log/gcp-kubelet.log.pos\n\
        \  tag kubelet\n</source>\n\n# Example:\n# I1118 21:26:53.975789       6 proxier.go:1096]\
        \ Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp)\
        \ was open before and is still needed\n<source>\n  type tail\n  format multiline\n\
        \  multiline_flush_interval 5s\n  format_firstline /^\\\\w\\\\d{4}/\n  format1\
        \ /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\\
        s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n  time_format %m%d %H:%M:%S.%N\n\
        \  path /var/log/kube-proxy.log\n  pos_file /var/log/gcp-kube-proxy.log.pos\n\
        \  tag kube-proxy\n</source>\n\n# Example:\n# I0204 07:00:19.604280      \
        \ 5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3\
        \ (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\n<source>\n  type tail\n\
        \  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\\
        \\w\\\\d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\\
        s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n  time_format\
        \ %m%d %H:%M:%S.%N\n  path /var/log/kube-apiserver.log\n  pos_file /var/log/gcp-kube-apiserver.log.pos\n\
        \  tag kube-apiserver\n</source>\n\n# Example:\n# 2017-02-09T00:15:57.992775796Z\
        \ AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" ip=\"104.132.1.72\" method=\"\
        GET\" user=\"kubecfg\" as=\"<self>\" asgroups=\"<lookup>\" namespace=\"default\"\
        \ uri=\"/api/v1/namespaces/default/pods\"\n# 2017-02-09T00:15:57.993528822Z\
        \ AUDIT: id=\"90c73c7c-97d6-4b65-9461-f94606ff825f\" response=\"200\"\n<source>\n\
        \  type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline\
        \ /^\\\\S+\\\\s+AUDIT:/\n  # Fields must be explicitly captured by name to\
        \ be parsed into the record.\n  # Fields may not always be present, and order\
        \ may change, so this just looks\n  # for a list of key=\"\\\\\"quoted\\\\\
        \" value\" pairs separated by spaces.\n  # Unknown fields are ignored.\n \
        \ # Note: We can't separate query/response lines as format1/format2 because\n\
        \  #       they don't always come one after the other for a given query.\n\
        \  # TODO: Maybe add a JSON output mode to audit log so we can get rid of\
        \ this?\n  format1 /^(?<time>\\\\S+) AUDIT:(?: (?:id=\"(?<id>(?:[^\"\\\\\\\
        \\]|\\\\\\\\.)*)\"|ip=\"(?<ip>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|method=\"(?<method>(?:[^\"\
        \\\\\\\\]|\\\\\\\\.)*)\"|user=\"(?<user>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|groups=\"\
        (?<groups>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|as=\"(?<as>(?:[^\"\\\\\\\\]|\\\\\
        \\\\.)*)\"|asgroups=\"(?<asgroups>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|namespace=\"\
        (?<namespace>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|uri=\"(?<uri>(?:[^\"\\\\\\\\\
        ]|\\\\\\\\.)*)\"|response=\"(?<response>(?:[^\"\\\\\\\\]|\\\\\\\\.)*)\"|\\\
        \\w+=\"(?:[^\"\\\\\\\\]|\\\\\\\\.)*\"))*/\n  time_format %FT%T.%L%Z\n  path\
        \ /var/log/kube-apiserver-audit.log\n  pos_file /var/log/gcp-kube-apiserver-audit.log.pos\n\
        \  tag kube-apiserver-audit\n</source>\n\n# Example:\n# I0204 06:55:31.872680\
        \       5 servicecontroller.go:277] LB already exists and doesn't need update\
        \ for service kube-system/kube-ui\n<source>\n  type tail\n  format multiline\n\
        \  multiline_flush_interval 5s\n  format_firstline /^\\\\w\\\\d{4}/\n  format1\
        \ /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\\
        s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n  time_format %m%d %H:%M:%S.%N\n\
        \  path /var/log/kube-controller-manager.log\n  pos_file /var/log/gcp-kube-controller-manager.log.pos\n\
        \  tag kube-controller-manager\n</source>\n\n# Example:\n# W0204 06:49:18.239674\
        \       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of\
        \ *api.Service ended with: 401: The event in requested index is outdated and\
        \ cleared (the requested history has been cleared [2578313/2577886]) [2579312]\n\
        <source>\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n\
        \  format_firstline /^\\\\w\\\\d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\
        \\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n\
        \  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-scheduler.log\n  pos_file\
        \ /var/log/gcp-kube-scheduler.log.pos\n  tag kube-scheduler\n</source>\n\n\
        # Example:\n# I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler\n\
        <source>\n  type tail\n  format multiline\n  multiline_flush_interval 5s\n\
        \  format_firstline /^\\\\w\\\\d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\
        \\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n\
        \  time_format %m%d %H:%M:%S.%N\n  path /var/log/rescheduler.log\n  pos_file\
        \ /var/log/gcp-rescheduler.log.pos\n  tag rescheduler\n</source>\n\n# Example:\n\
        # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from\
        \ path /etc/gce.conf\n<source>\n  type tail\n  format multiline\n  multiline_flush_interval\
        \ 5s\n  format_firstline /^\\\\w\\\\d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\
        \\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n\
        \  time_format %m%d %H:%M:%S.%N\n  path /var/log/glbc.log\n  pos_file /var/log/gcp-glbc.log.pos\n\
        \  tag glbc\n</source>\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230]\
        \ Reading config from path /etc/gce.conf\n<source>\n  type tail\n  format\
        \ multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\\\w\\\\\
        d{4}/\n  format1 /^(?<severity>\\\\w)(?<time>\\\\d{4} [^\\\\s]*)\\\\s+(?<pid>\\\
        \\d+)\\\\s+(?<source>[^ \\\\]]+)\\\\] (?<message>.*)/\n  time_format %m%d\
        \ %H:%M:%S.%N\n  path /var/log/cluster-autoscaler.log\n  pos_file /var/log/gcp-cluster-autoscaler.log.pos\n\
        \  tag cluster-autoscaler\n</source>\n\n# Logs from systemd-journal for interesting\
        \ services.\n<source>\n  type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"\
        docker.service\" }]\n  pos_file /var/log/gcp-journald-docker.pos\n  read_from_head\
        \ true\n  tag docker\n</source>\n\n<source>\n  type systemd\n  filters [{\
        \ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\n  pos_file /var/log/gcp-journald-kubelet.pos\n\
        \  read_from_head true\n  tag kubelet\n</source>\n\n# Prometheus monitoring\n\
        <source>\n  @type prometheus\n  port 80\n</source>\n\n<source>\n  @type prometheus_monitor\n\
        </source>\n\n# We use 2 output stanzas - one to handle the container logs\
        \ and one to handle\n# the node daemon logs, the latter of which explicitly\
        \ sends its logs to the\n# compute.googleapis.com service rather than container.googleapis.com\
        \ to keep\n# them separate since most users don't care about the node logs.\n\
        <match kubernetes.**>\n  @type copy\n\n  <store>\n    @type google_cloud\n\
        \n    # Set the buffer type to file to improve the reliability and reduce\
        \ the memory consumption\n    buffer_type file\n    buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer\n\
        \    # Set queue_full action to block because we want to pause gracefully\n\
        \    # in case of the off-the-limits load instead of throwing an exception\n\
        \    buffer_queue_full_action block\n    # Set the chunk limit conservatively\
        \ to avoid exceeding the GCL limit\n    # of 10MiB per write request.\n  \
        \  buffer_chunk_limit 2M\n    # Cap the combined memory usage of this buffer\
        \ and the one below to\n    # 2MiB/chunk * (6 + 2) chunks = 16 MiB\n    buffer_queue_limit\
        \ 6\n    # Never wait more than 5 seconds before flushing logs in the non-error\
        \ case.\n    flush_interval 5s\n    # Never wait longer than 30 seconds between\
        \ retries.\n    max_retry_wait 30\n    # Disable the limit on the number of\
        \ retries (retry forever).\n    disable_retry_limit\n    # Use multiple threads\
        \ for processing.\n    num_threads 2\n  </store>\n  <store>\n    @type prometheus\n\
        \n    <metric>\n      type counter\n      name logging_entry_count\n     \
        \ desc Total number of log entries generated by either an application container\
        \ or a system component\n      <labels>\n        tag ${tag}\n        component\
        \ container\n      </labels>\n    </metric>\n  </store>\n</match>\n\n# Keep\
        \ a smaller buffer here since these logs are less important than the user's\n\
        # container logs.\n<match **>\n  @type copy\n\n  <store>\n    @type google_cloud\n\
        \n    detect_subservice false\n    buffer_type file\n    buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer\n\
        \    buffer_queue_full_action block\n    buffer_chunk_limit 2M\n    buffer_queue_limit\
        \ 2\n    flush_interval 5s\n    max_retry_wait 30\n    disable_retry_limit\n\
        \    num_threads 2\n  </store>\n  <store>\n    @type prometheus\n\n    <metric>\n\
        \      type counter\n      name logging_entry_count\n      desc Total number\
        \ of log entries generated by either an application container or a system\
        \ component\n      <labels>\n        tag ${tag}\n        component system\n\
        \      </labels>\n    </metric>\n  </store>\n</match>"
    resources:
      limits:
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 200Mi
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: libsystemddir
      mountPath: /host/lib
      readOnly: true
    livenessProbe:
      initialDelaySeconds: 600
      periodSeconds: 60
      exec:
        command:
        - /bin/sh
        - -c
        - "LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900};\
          \ if [ ! -e /var/log/fluentd-buffers ]; then\n  exit 1;\nfi; LAST_MODIFIED_DATE=`stat\
          \ /var/log/fluentd-buffers | grep Modify | sed -r \"s/Modify: (.*)/\\1/\"\
          `; LAST_MODIFIED_TIMESTAMP=`date -d \"$LAST_MODIFIED_DATE\" +%s`; if [ `date\
          \ +%s` -gt `expr $LAST_MODIFIED_TIMESTAMP + $STUCK_THRESHOLD_SECONDS` ];\
          \ then\n  rm -rf /var/log/fluentd-buffers;\n  exit 1;\nfi; if [ `date +%s`\
          \ -gt `expr $LAST_MODIFIED_TIMESTAMP + $LIVENESS_THRESHOLD_SECONDS` ]; then\n\
          \  exit 1;\nfi;\n"
  terminationGracePeriodSeconds: 30
  volumes:
  - name: varlog
    hostPath:
      path: /var/log
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/docker/containers
  - name: libsystemddir
    hostPath:
      path: /usr/lib64
